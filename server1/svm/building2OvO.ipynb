{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.C * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.C * self.w - np.dot(x_i, y[idx]))\n",
    "                    self.b -= self.lr * y[idx]\n",
    "\n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.w) + self.b\n",
    "        return np.sign(linear_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneVsOneSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVsOneSVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.classifiers = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        for i in range(len(self.classes)):\n",
    "            for j in range(i + 1, len(self.classes)):\n",
    "                class_i = self.classes[i]\n",
    "                class_j = self.classes[j]\n",
    "\n",
    "                # Filter the dataset to include only class_i and class_j\n",
    "                X_ij = X[(y == class_i) | (y == class_j)]\n",
    "                y_ij = y[(y == class_i) | (y == class_j)]\n",
    "                y_ij = np.where(y_ij == class_i, 1, -1)  # Assign labels +1 and -1\n",
    "\n",
    "                # Train a binary SVM on this subset\n",
    "                svm = BinarySVM(C=self.C, learning_rate=self.learning_rate, n_iters=self.n_iters)\n",
    "                svm.fit(X_ij, y_ij)\n",
    "\n",
    "                # Store the classifier\n",
    "                self.classifiers[(class_i, class_j)] = svm\n",
    "\n",
    "    def predict(self, X):\n",
    "        votes = np.zeros((X.shape[0], len(self.classes)))\n",
    "\n",
    "        for (class_i, class_j), svm in self.classifiers.items():\n",
    "            predictions = svm.predict(X)\n",
    "            for idx, prediction in enumerate(predictions):\n",
    "                if prediction == 1:\n",
    "                    votes[idx, np.where(self.classes == class_i)] += 1\n",
    "                else:\n",
    "                    votes[idx, np.where(self.classes == class_j)] += 1\n",
    "\n",
    "        # Determine the final prediction by majority vote\n",
    "        return self.classes[np.argmax(votes, axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2 2 2 2 2]\n",
      "Actual Labels: [0 0 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Example dataset\n",
    "data = {\n",
    "    'feature1': [2, 4, 4, 6, 6, 8],\n",
    "    'feature2': [3, 3, 6, 5, 8, 7],\n",
    "    'label': [0, 0, 1, 1, 2, 2]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df[['feature1', 'feature2']].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Initialize and train the One-vs-One SVM\n",
    "ovo_svm = OneVsOneSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "ovo_svm.fit(X, y)\n",
    "\n",
    "# Test the classifier\n",
    "predictions = ovo_svm.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual Labels:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[0.         0.04402281 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.03521825 0.        ]\n",
      " [0.         0.         0.         0.04402281 0.         0.04402281\n",
      "  0.         0.        ]]\n",
      "Vocabulary:\n",
      " ['sample', 'a', 'this', 'text', 'is', 'of', 'another', 'example']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def compute_tf(text):\n",
    "    tf_text = Counter(text)\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i] / float(len(text))\n",
    "    return tf_text\n",
    "\n",
    "def compute_idf(word, corpus):\n",
    "    return math.log10(len(corpus) / (1 + sum([1 for text in corpus if word in text])))\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "    corpus = [doc.split() for doc in corpus]\n",
    "    vocabulary = list(set(word for doc in corpus for word in doc))\n",
    "    idf = {word: compute_idf(word, corpus) for word in vocabulary}\n",
    "    tfidf = []\n",
    "\n",
    "    for text in corpus:\n",
    "        tf = compute_tf(text)\n",
    "        tfidf.append([tf.get(word, 0) * idf[word] for word in vocabulary])\n",
    "    \n",
    "    return np.array(tfidf), vocabulary\n",
    "\n",
    "# Example usage:\n",
    "corpus = [\"this is a sample\", \"this is another example example\", \"sample example of text\"]\n",
    "tfidf_matrix, vocabulary = compute_tfidf(corpus)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix)\n",
    "print(\"Vocabulary:\\n\", vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 3 3 3 3]\n",
      "Actual Labels: [1 2 3 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Your dataset (replace this with actual data)\n",
    "news_articles = [\n",
    "    \"government passes new law\",\n",
    "    \"football match ends in draw\",\n",
    "    \"new technology in smartphones\",\n",
    "    \"politician gives a speech\",\n",
    "    \"sports event attracts large crowd\"\n",
    "]\n",
    "labels = [1, 2, 3, 1, 2]  # Example labels: 1=Politics, 2=Sports, 3=Technology\n",
    "\n",
    "# Convert news articles to TF-IDF features\n",
    "X, vocabulary = compute_tfidf(news_articles)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize and train the One-vs-One SVM\n",
    "ovo_svm = OneVsOneSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "ovo_svm.fit(X, y)\n",
    "\n",
    "# Test the classifier on the training data\n",
    "predictions = ovo_svm.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual Labels:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"../dataset/BBC News Train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Category_class=sorted(df_train[\"Category\"].unique())\n",
    "Category_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={'business':0, 'entertainment':1, 'politics':2, 'sport':3, 'tech':4}\n",
    "df_train['CategoryId']=df_train['Category'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, vocabulary = compute_tfidf(df_train['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['CategoryId'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the One-vs-One SVM\n",
    "ovo_svm = OneVsOneSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "ovo_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [4 4 4 ... 4 4 4]\n",
      "Actual Labels: [0 0 0 ... 0 4 4]\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier on the training data\n",
    "predictions = ovo_svm.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual Labels:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17516778523489934\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Weights: [ 0.00000000e+00  9.16875453e-04  7.69604046e-04  0.00000000e+00\n",
      "  9.31678376e-04  1.22740532e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -7.34970303e-04  0.00000000e+00\n",
      " -9.37290881e-04 -2.06040819e-03  1.25724960e-03  0.00000000e+00\n",
      "  9.31678376e-04 -9.37290881e-04  0.00000000e+00 -1.80394368e-03\n",
      "  7.48333039e-04  7.48333039e-04 -1.27499275e-03  1.27754785e-03\n",
      " -9.67800253e-04  0.00000000e+00 -1.23975336e-03  0.00000000e+00\n",
      " -1.24472729e-03  6.24260874e-04  1.27754785e-03  9.24247279e-04\n",
      " -7.34970303e-04 -9.41051322e-04  1.81973400e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.25724960e-03\n",
      " -7.32033362e-04  9.54332074e-04  0.00000000e+00  2.04391287e-03\n",
      "  0.00000000e+00 -7.32033362e-04  0.00000000e+00 -1.65849034e-03\n",
      " -9.52423410e-04  0.00000000e+00 -7.32033362e-04  0.00000000e+00\n",
      "  0.00000000e+00 -2.04791039e-03  9.16875453e-04  1.32132668e-03\n",
      "  0.00000000e+00  0.00000000e+00 -2.06460759e-03 -9.52423410e-04\n",
      "  0.00000000e+00  0.00000000e+00 -2.78598024e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.25222563e-03  0.00000000e+00\n",
      " -7.34970303e-04  0.00000000e+00  9.13211619e-04  7.60414850e-04\n",
      " -1.41752581e-03  1.25724960e-03  9.24247279e-04  0.00000000e+00\n",
      " -1.38285103e-03 -9.41051322e-04  1.26229372e-03  0.00000000e+00\n",
      "  1.80755879e-03 -1.28524390e-03 -7.58894021e-04  0.00000000e+00\n",
      "  1.29386249e-03  0.00000000e+00  7.69604046e-04  1.58104003e-03\n",
      "  9.54332074e-04  9.13211619e-04  9.27955389e-04  1.28781954e-03\n",
      "  9.16875453e-04  9.27955389e-04  7.60414850e-04  9.27955389e-04\n",
      " -9.41051322e-04  0.00000000e+00  1.28781954e-03  0.00000000e+00\n",
      "  1.25222563e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.64155866e-03  9.31678376e-04 -1.25976913e-03 -9.52423410e-04\n",
      " -7.58894021e-04  0.00000000e+00  7.48333039e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.38025358e-03  0.00000000e+00\n",
      "  1.36704920e-03  1.28781954e-03 -1.21953947e-03 -7.58894021e-04\n",
      "  7.69604046e-04  1.25222563e-03 -1.23975336e-03 -1.52670808e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.28010806e-03  7.69604046e-04  0.00000000e+00  0.00000000e+00\n",
      " -2.07275795e-03 -9.37290881e-04  0.00000000e+00  7.69604046e-04\n",
      " -3.57547164e-05  0.00000000e+00 -1.24472729e-03 -9.22398785e-04\n",
      " -9.41051322e-04  0.00000000e+00], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.          0.0156925   0.01317192  0.          0.01594586  0.02100728\n",
      "  0.          0.          0.          0.         -0.01257916  0.\n",
      " -0.01604192 -0.03526429  0.02151807  0.          0.01594586 -0.01604192\n",
      "  0.         -0.03087485  0.01280787  0.01280787 -0.02182175  0.02186548\n",
      " -0.01656409  0.         -0.02121862  0.         -0.02130375  0.01068435\n",
      "  0.02186548  0.01581867 -0.01257916 -0.01610628  0.0311451   0.\n",
      "  0.          0.          0.          0.02151807 -0.01252889  0.01633358\n",
      "  0.          0.03498197  0.         -0.01252889  0.         -0.02838539\n",
      " -0.01630091  0.         -0.01252889  0.          0.         -0.03505039\n",
      "  0.0156925   0.02261476  0.          0.         -0.03533616 -0.01630091\n",
      "  0.          0.         -0.00476826  0.          0.          0.\n",
      "  0.02143209  0.         -0.01257916  0.          0.01562979  0.01301465\n",
      " -0.02426123  0.02151807  0.01581867  0.         -0.02366776 -0.01610628\n",
      "  0.0216044   0.          0.03093672 -0.0219972  -0.01298862  0.\n",
      "  0.02214471  0.          0.01317192  0.02705981  0.01633358  0.01562979\n",
      "  0.01588214  0.02204128  0.0156925   0.01588214  0.01301465  0.01588214\n",
      " -0.01610628  0.          0.02204128  0.          0.02143209  0.\n",
      "  0.          0.         -0.0280956   0.01594586 -0.02156119 -0.01630091\n",
      " -0.01298862  0.          0.01280787  0.          0.          0.\n",
      " -0.02362331  0.          0.02339731  0.02204128 -0.02087266 -0.01298862\n",
      "  0.01317192  0.02143209 -0.02121862 -0.02612991  0.          0.\n",
      "  0.          0.         -0.0219093   0.01317192  0.          0.\n",
      " -0.03547566 -0.01604192  0.          0.01317192 -0.00061195  0.\n",
      " -0.02130375 -0.01578703 -0.01610628  0.        ], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.          0.01572891  0.01320248  0.          0.01598285  0.02105602\n",
      "  0.          0.          0.          0.         -0.01260834  0.\n",
      " -0.01607913 -0.0353461   0.02156799  0.          0.01598285 -0.01607913\n",
      "  0.         -0.03094648  0.01283758  0.01283758 -0.02187237  0.02191621\n",
      " -0.01660252  0.         -0.02126785  0.         -0.02135317  0.01070913\n",
      "  0.02191621  0.01585537 -0.01260834 -0.01614364  0.03121736  0.\n",
      "  0.          0.          0.          0.02156799 -0.01255796  0.01637147\n",
      "  0.          0.03506312  0.         -0.01255796  0.         -0.02845124\n",
      " -0.01633873  0.         -0.01255796  0.          0.         -0.0351317\n",
      "  0.01572891  0.02266723  0.          0.         -0.03541814 -0.01633873\n",
      "  0.          0.         -0.00477932  0.          0.          0.\n",
      "  0.02148181  0.         -0.01260834  0.          0.01566605  0.01304484\n",
      " -0.02431752  0.02156799  0.01585537  0.         -0.02372267 -0.01614364\n",
      "  0.02165452  0.          0.03100849 -0.02204823 -0.01301875  0.\n",
      "  0.02219608  0.          0.01320248  0.02712259  0.01637147  0.01566605\n",
      "  0.01591898  0.02209242  0.01572891  0.01591898  0.01304484  0.01591898\n",
      " -0.01614364  0.          0.02209242  0.          0.02148181  0.\n",
      "  0.          0.         -0.02816078  0.01598285 -0.02161121 -0.01633873\n",
      " -0.01301875  0.          0.01283758  0.          0.          0.\n",
      " -0.02367811  0.          0.02345159  0.02209242 -0.02092108 -0.01301875\n",
      "  0.01320248  0.02148181 -0.02126785 -0.02619053  0.          0.\n",
      "  0.          0.         -0.02196013  0.01320248  0.          0.\n",
      " -0.03555796 -0.01607913  0.          0.01320248 -0.00061337  0.\n",
      " -0.02135317 -0.01582366 -0.01614364  0.        ], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.          0.015729    0.01320256  0.          0.01598294  0.02105614\n",
      "  0.          0.          0.          0.         -0.01260841  0.\n",
      " -0.01607922 -0.0353463   0.02156812  0.          0.01598294 -0.01607922\n",
      "  0.         -0.03094665  0.01283765  0.01283765 -0.0218725   0.02191633\n",
      " -0.01660261  0.         -0.02126797  0.         -0.0213533   0.01070919\n",
      "  0.02191633  0.01585546 -0.01260841 -0.01614373  0.03121753  0.\n",
      "  0.          0.          0.          0.02156812 -0.01255803  0.01637157\n",
      "  0.          0.03506332  0.         -0.01255803  0.         -0.0284514\n",
      " -0.01633882  0.         -0.01255803  0.          0.         -0.0351319\n",
      "  0.015729    0.02266736  0.          0.         -0.03541834 -0.01633882\n",
      "  0.          0.         -0.00477935  0.          0.          0.\n",
      "  0.02148193  0.         -0.01260841  0.          0.01566614  0.01304492\n",
      " -0.02431765  0.02156812  0.01585546  0.         -0.02372281 -0.01614373\n",
      "  0.02165465  0.          0.03100867 -0.02204836 -0.01301883  0.\n",
      "  0.02219621  0.          0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      " -0.01614373  0.          0.02209254  0.          0.02148193  0.\n",
      "  0.          0.         -0.02816094  0.01598294 -0.02161134 -0.01633882\n",
      " -0.01301883  0.          0.01283765  0.          0.          0.\n",
      " -0.02367825  0.          0.02345173  0.02209254 -0.0209212  -0.01301883\n",
      "  0.01320256  0.02148193 -0.02126797 -0.02619068  0.          0.\n",
      "  0.          0.         -0.02196025  0.01320256  0.          0.\n",
      " -0.03555816 -0.01607922  0.          0.01320256 -0.00061337  0.\n",
      " -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.          0.015729    0.01320256  0.          0.01598294  0.02105614\n",
      "  0.          0.          0.          0.         -0.01260841  0.\n",
      " -0.01607922 -0.0353463   0.02156812  0.          0.01598294 -0.01607922\n",
      "  0.         -0.03094665  0.01283765  0.01283765 -0.0218725   0.02191633\n",
      " -0.01660261  0.         -0.02126797  0.         -0.0213533   0.01070919\n",
      "  0.02191633  0.01585546 -0.01260841 -0.01614373  0.03121754  0.\n",
      "  0.          0.          0.          0.02156812 -0.01255803  0.01637157\n",
      "  0.          0.03506332  0.         -0.01255803  0.         -0.0284514\n",
      " -0.01633882  0.         -0.01255803  0.          0.         -0.0351319\n",
      "  0.015729    0.02266736  0.          0.         -0.03541834 -0.01633882\n",
      "  0.          0.         -0.00477935  0.          0.          0.\n",
      "  0.02148193  0.         -0.01260841  0.          0.01566614  0.01304492\n",
      " -0.02431765  0.02156812  0.01585546  0.         -0.02372281 -0.01614373\n",
      "  0.02165465  0.          0.03100867 -0.02204836 -0.01301883  0.\n",
      "  0.02219621  0.          0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      " -0.01614373  0.          0.02209254  0.          0.02148193  0.\n",
      "  0.          0.         -0.02816094  0.01598294 -0.02161134 -0.01633882\n",
      " -0.01301883  0.          0.01283765  0.          0.          0.\n",
      " -0.02367825  0.          0.02345173  0.02209254 -0.0209212  -0.01301883\n",
      "  0.01320256  0.02148193 -0.02126797 -0.02619068  0.          0.\n",
      "  0.          0.         -0.02196025  0.01320256  0.          0.\n",
      " -0.03555816 -0.01607922  0.          0.01320256 -0.00061337  0.\n",
      " -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.          0.015729    0.01320256  0.          0.01598294  0.02105614\n",
      "  0.          0.          0.          0.         -0.01260841  0.\n",
      " -0.01607922 -0.0353463   0.02156812  0.          0.01598294 -0.01607922\n",
      "  0.         -0.03094665  0.01283765  0.01283765 -0.0218725   0.02191633\n",
      " -0.01660261  0.         -0.02126797  0.         -0.0213533   0.01070919\n",
      "  0.02191633  0.01585546 -0.01260841 -0.01614373  0.03121754  0.\n",
      "  0.          0.          0.          0.02156812 -0.01255803  0.01637157\n",
      "  0.          0.03506332  0.         -0.01255803  0.         -0.0284514\n",
      " -0.01633882  0.         -0.01255803  0.          0.         -0.0351319\n",
      "  0.015729    0.02266736  0.          0.         -0.03541834 -0.01633882\n",
      "  0.          0.         -0.00477935  0.          0.          0.\n",
      "  0.02148193  0.         -0.01260841  0.          0.01566614  0.01304492\n",
      " -0.02431765  0.02156812  0.01585546  0.         -0.02372281 -0.01614373\n",
      "  0.02165465  0.          0.03100867 -0.02204836 -0.01301883  0.\n",
      "  0.02219621  0.          0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      " -0.01614373  0.          0.02209254  0.          0.02148193  0.\n",
      "  0.          0.         -0.02816094  0.01598294 -0.02161134 -0.01633882\n",
      " -0.01301883  0.          0.01283765  0.          0.          0.\n",
      " -0.02367825  0.          0.02345173  0.02209254 -0.0209212  -0.01301883\n",
      "  0.01320256  0.02148193 -0.02126797 -0.02619068  0.          0.\n",
      "  0.          0.         -0.02196025  0.01320256  0.          0.\n",
      " -0.03555816 -0.01607922  0.          0.01320256 -0.00061337  0.\n",
      " -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.          0.015729    0.01320256  0.          0.01598294  0.02105614\n",
      "  0.          0.          0.          0.         -0.01260841  0.\n",
      " -0.01607922 -0.0353463   0.02156812  0.          0.01598294 -0.01607922\n",
      "  0.         -0.03094665  0.01283765  0.01283765 -0.0218725   0.02191633\n",
      " -0.01660261  0.         -0.02126797  0.         -0.0213533   0.01070919\n",
      "  0.02191633  0.01585546 -0.01260841 -0.01614373  0.03121754  0.\n",
      "  0.          0.          0.          0.02156812 -0.01255803  0.01637157\n",
      "  0.          0.03506332  0.         -0.01255803  0.         -0.0284514\n",
      " -0.01633882  0.         -0.01255803  0.          0.         -0.0351319\n",
      "  0.015729    0.02266736  0.          0.         -0.03541834 -0.01633882\n",
      "  0.          0.         -0.00477935  0.          0.          0.\n",
      "  0.02148193  0.         -0.01260841  0.          0.01566614  0.01304492\n",
      " -0.02431765  0.02156812  0.01585546  0.         -0.02372281 -0.01614373\n",
      "  0.02165465  0.          0.03100867 -0.02204836 -0.01301883  0.\n",
      "  0.02219621  0.          0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      " -0.01614373  0.          0.02209254  0.          0.02148193  0.\n",
      "  0.          0.         -0.02816094  0.01598294 -0.02161134 -0.01633882\n",
      " -0.01301883  0.          0.01283765  0.          0.          0.\n",
      " -0.02367825  0.          0.02345173  0.02209254 -0.0209212  -0.01301883\n",
      "  0.01320256  0.02148193 -0.02126797 -0.02619068  0.          0.\n",
      "  0.          0.         -0.02196025  0.01320256  0.          0.\n",
      " -0.03555816 -0.01607922  0.          0.01320256 -0.00061337  0.\n",
      " -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.          0.015729    0.01320256  0.          0.01598294  0.02105614\n",
      "  0.          0.          0.          0.         -0.01260841  0.\n",
      " -0.01607922 -0.0353463   0.02156812  0.          0.01598294 -0.01607922\n",
      "  0.         -0.03094665  0.01283765  0.01283765 -0.0218725   0.02191633\n",
      " -0.01660261  0.         -0.02126797  0.         -0.0213533   0.01070919\n",
      "  0.02191633  0.01585546 -0.01260841 -0.01614373  0.03121754  0.\n",
      "  0.          0.          0.          0.02156812 -0.01255803  0.01637157\n",
      "  0.          0.03506332  0.         -0.01255803  0.         -0.0284514\n",
      " -0.01633882  0.         -0.01255803  0.          0.         -0.0351319\n",
      "  0.015729    0.02266736  0.          0.         -0.03541834 -0.01633882\n",
      "  0.          0.         -0.00477935  0.          0.          0.\n",
      "  0.02148193  0.         -0.01260841  0.          0.01566614  0.01304492\n",
      " -0.02431765  0.02156812  0.01585546  0.         -0.02372281 -0.01614373\n",
      "  0.02165465  0.          0.03100867 -0.02204836 -0.01301883  0.\n",
      "  0.02219621  0.          0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      " -0.01614373  0.          0.02209254  0.          0.02148193  0.\n",
      "  0.          0.         -0.02816094  0.01598294 -0.02161134 -0.01633882\n",
      " -0.01301883  0.          0.01283765  0.          0.          0.\n",
      " -0.02367825  0.          0.02345173  0.02209254 -0.0209212  -0.01301883\n",
      "  0.01320256  0.02148193 -0.02126797 -0.02619068  0.          0.\n",
      "  0.          0.         -0.02196025  0.01320256  0.          0.\n",
      " -0.03555816 -0.01607922  0.          0.01320256 -0.00061337  0.\n",
      " -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.          0.015729    0.01320256  0.          0.01598294  0.02105614\n",
      "  0.          0.          0.          0.         -0.01260841  0.\n",
      " -0.01607922 -0.0353463   0.02156812  0.          0.01598294 -0.01607922\n",
      "  0.         -0.03094665  0.01283765  0.01283765 -0.0218725   0.02191633\n",
      " -0.01660261  0.         -0.02126797  0.         -0.0213533   0.01070919\n",
      "  0.02191633  0.01585546 -0.01260841 -0.01614373  0.03121754  0.\n",
      "  0.          0.          0.          0.02156812 -0.01255803  0.01637157\n",
      "  0.          0.03506332  0.         -0.01255803  0.         -0.0284514\n",
      " -0.01633882  0.         -0.01255803  0.          0.         -0.0351319\n",
      "  0.015729    0.02266736  0.          0.         -0.03541834 -0.01633882\n",
      "  0.          0.         -0.00477935  0.          0.          0.\n",
      "  0.02148193  0.         -0.01260841  0.          0.01566614  0.01304492\n",
      " -0.02431765  0.02156812  0.01585546  0.         -0.02372281 -0.01614373\n",
      "  0.02165465  0.          0.03100867 -0.02204836 -0.01301883  0.\n",
      "  0.02219621  0.          0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      " -0.01614373  0.          0.02209254  0.          0.02148193  0.\n",
      "  0.          0.         -0.02816094  0.01598294 -0.02161134 -0.01633882\n",
      " -0.01301883  0.          0.01283765  0.          0.          0.\n",
      " -0.02367825  0.          0.02345173  0.02209254 -0.0209212  -0.01301883\n",
      "  0.01320256  0.02148193 -0.02126797 -0.02619068  0.          0.\n",
      "  0.          0.         -0.02196025  0.01320256  0.          0.\n",
      " -0.03555816 -0.01607922  0.          0.01320256 -0.00061337  0.\n",
      " -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.          0.015729    0.01320256  0.          0.01598294  0.02105614\n",
      "  0.          0.          0.          0.         -0.01260841  0.\n",
      " -0.01607922 -0.0353463   0.02156812  0.          0.01598294 -0.01607922\n",
      "  0.         -0.03094665  0.01283765  0.01283765 -0.0218725   0.02191633\n",
      " -0.01660261  0.         -0.02126797  0.         -0.0213533   0.01070919\n",
      "  0.02191633  0.01585546 -0.01260841 -0.01614373  0.03121754  0.\n",
      "  0.          0.          0.          0.02156812 -0.01255803  0.01637157\n",
      "  0.          0.03506332  0.         -0.01255803  0.         -0.0284514\n",
      " -0.01633882  0.         -0.01255803  0.          0.         -0.0351319\n",
      "  0.015729    0.02266736  0.          0.         -0.03541834 -0.01633882\n",
      "  0.          0.         -0.00477935  0.          0.          0.\n",
      "  0.02148193  0.         -0.01260841  0.          0.01566614  0.01304492\n",
      " -0.02431765  0.02156812  0.01585546  0.         -0.02372281 -0.01614373\n",
      "  0.02165465  0.          0.03100867 -0.02204836 -0.01301883  0.\n",
      "  0.02219621  0.          0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      " -0.01614373  0.          0.02209254  0.          0.02148193  0.\n",
      "  0.          0.         -0.02816094  0.01598294 -0.02161134 -0.01633882\n",
      " -0.01301883  0.          0.01283765  0.          0.          0.\n",
      " -0.02367825  0.          0.02345173  0.02209254 -0.0209212  -0.01301883\n",
      "  0.01320256  0.02148193 -0.02126797 -0.02619068  0.          0.\n",
      "  0.          0.         -0.02196025  0.01320256  0.          0.\n",
      " -0.03555816 -0.01607922  0.          0.01320256 -0.00061337  0.\n",
      " -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 0: Weights: [-0.0009224   0.00091688  0.0007696  -0.00153601  0.00093168  0.00122741\n",
      " -0.00074983  0.         -0.0012699  -0.0009261   0.          0.\n",
      "  0.          0.          0.00125725 -0.00093355  0.00093168  0.\n",
      "  0.          0.          0.00074833  0.00074833  0.          0.00127755\n",
      "  0.         -0.00074983  0.         -0.00094862  0.         -0.00017025\n",
      "  0.00127755  0.00092425  0.          0.          0.00181973 -0.00074385\n",
      " -0.000765   -0.00075284  0.          0.00125725  0.          0.00095433\n",
      " -0.00094483  0.00204391 -0.00096008  0.         -0.00093355  0.\n",
      "  0.         -0.00096008  0.         -0.000765   -0.00061248  0.\n",
      "  0.00091688  0.00132133 -0.00075284 -0.0009224   0.          0.\n",
      " -0.00075284  0.         -0.00336076 -0.0009678  -0.00139335 -0.0009261\n",
      "  0.00125223  0.          0.         -0.00123869  0.00091321  0.00076041\n",
      "  0.          0.00125725  0.00092425 -0.00061804  0.          0.\n",
      "  0.00126229 -0.00077115  0.00180756  0.          0.          0.\n",
      " -0.00022227 -0.0009678   0.0007696   0.00158104  0.00095433  0.00091321\n",
      "  0.00092796  0.00128782  0.00091688  0.00092796  0.00076041  0.00092796\n",
      "  0.         -0.00094862  0.00128782 -0.0012699   0.00125223 -0.00061248\n",
      " -0.00093355  0.          0.00078787  0.00093168  0.          0.\n",
      "  0.         -0.00094483  0.00074833 -0.00075284 -0.00074385 -0.00091504\n",
      "  0.         -0.00074983  0.00136705  0.00128782  0.          0.\n",
      "  0.0007696   0.00125223  0.          0.         -0.00199068  0.\n",
      " -0.00096008 -0.00152685  0.          0.0007696  -0.00061248  0.\n",
      "  0.          0.         -0.00123869  0.0007696   0.00075876 -0.0012699\n",
      "  0.          0.          0.         -0.00061248], Bias: 0.0\n",
      "Iteration 100: Weights: [-0.01578703  0.0156925   0.01317192 -0.02628905  0.01594586  0.02100728\n",
      " -0.01283353  0.         -0.02173455 -0.01585037  0.          0.\n",
      "  0.          0.          0.02151807 -0.01597781  0.01594586  0.\n",
      "  0.          0.          0.01280787  0.01280787  0.          0.02186548\n",
      "  0.         -0.01283353  0.         -0.01623577  0.         -0.00291391\n",
      "  0.02186548  0.01581867  0.          0.          0.0311451  -0.01273117\n",
      " -0.01309305 -0.01288502  0.          0.02151807  0.          0.01633358\n",
      " -0.0161709   0.03498197 -0.01643197  0.         -0.01597781  0.\n",
      "  0.         -0.01643197  0.         -0.01309305 -0.01048263  0.\n",
      "  0.0156925   0.02261476 -0.01288502 -0.01578703  0.          0.\n",
      " -0.01288502  0.         -0.05752008 -0.01656409 -0.02384746 -0.01585037\n",
      "  0.02143209  0.          0.         -0.02120034  0.01562979  0.01301465\n",
      "  0.          0.02151807  0.01581867 -0.01057793  0.          0.\n",
      "  0.0216044  -0.01319832  0.03093672  0.          0.          0.\n",
      " -0.00380414 -0.01656409  0.01317192  0.02705981  0.01633358  0.01562979\n",
      "  0.01588214  0.02204128  0.0156925   0.01588214  0.01301465  0.01588214\n",
      "  0.         -0.01623577  0.02204128 -0.02173455  0.02143209 -0.01048263\n",
      " -0.01597781  0.          0.01348448  0.01594586  0.          0.\n",
      "  0.         -0.0161709   0.01280787 -0.01288502 -0.01273117 -0.01566112\n",
      "  0.         -0.01283353  0.02339731  0.02204128  0.          0.\n",
      "  0.01317192  0.02143209  0.          0.         -0.0340709   0.\n",
      " -0.01643197 -0.02613242  0.          0.01317192 -0.01048263  0.\n",
      "  0.          0.         -0.02120034  0.01317192  0.0129863  -0.02173455\n",
      "  0.          0.          0.         -0.01048263], Bias: 0.0\n",
      "Iteration 200: Weights: [-0.01582366  0.01572891  0.01320248 -0.02635004  0.01598285  0.02105602\n",
      " -0.01286331  0.         -0.02178497 -0.01588714  0.          0.\n",
      "  0.          0.          0.02156799 -0.01601488  0.01598285  0.\n",
      "  0.          0.          0.01283758  0.01283758  0.          0.02191621\n",
      "  0.         -0.01286331  0.         -0.01627344  0.         -0.00292067\n",
      "  0.02191621  0.01585537  0.          0.          0.03121736 -0.01276071\n",
      " -0.01312342 -0.01291491  0.          0.02156799  0.          0.01637147\n",
      " -0.01620841  0.03506312 -0.0164701   0.         -0.01601488  0.\n",
      "  0.         -0.0164701   0.         -0.01312342 -0.01050695  0.\n",
      "  0.01572891  0.02266723 -0.01291491 -0.01582366  0.          0.\n",
      " -0.01291491  0.         -0.05765352 -0.01660252 -0.02390278 -0.01588714\n",
      "  0.02148181  0.          0.         -0.02124952  0.01566605  0.01304484\n",
      "  0.          0.02156799  0.01585537 -0.01060247  0.          0.\n",
      "  0.02165452 -0.01322894  0.03100849  0.          0.          0.\n",
      " -0.00381297 -0.01660252  0.01320248  0.02712259  0.01637147  0.01566605\n",
      "  0.01591898  0.02209242  0.01572891  0.01591898  0.01304484  0.01591898\n",
      "  0.         -0.01627344  0.02209242 -0.02178497  0.02148181 -0.01050695\n",
      " -0.01601488  0.          0.01351576  0.01598285  0.          0.\n",
      "  0.         -0.01620841  0.01283758 -0.01291491 -0.01276071 -0.01569745\n",
      "  0.         -0.01286331  0.02345159  0.02209242  0.          0.\n",
      "  0.01320248  0.02148181  0.          0.         -0.03414994  0.\n",
      " -0.0164701  -0.02619305  0.          0.01320248 -0.01050695  0.\n",
      "  0.          0.         -0.02124952  0.01320248  0.01301643 -0.02178497\n",
      "  0.          0.          0.         -0.01050695], Bias: 0.0\n",
      "Iteration 300: Weights: [-0.01582375  0.015729    0.01320256 -0.02635019  0.01598294  0.02105614\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.          0.\n",
      "  0.          0.          0.02156812 -0.01601497  0.01598294  0.\n",
      "  0.          0.          0.01283765  0.01283765  0.          0.02191633\n",
      "  0.         -0.01286338  0.         -0.01627353  0.         -0.00292068\n",
      "  0.02191633  0.01585546  0.          0.          0.03121753 -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.02156812  0.          0.01637157\n",
      " -0.0162085   0.03506332 -0.01647019  0.         -0.01601497  0.\n",
      "  0.         -0.01647019  0.         -0.0131235  -0.01050701  0.\n",
      "  0.015729    0.02266736 -0.01291499 -0.01582375  0.          0.\n",
      " -0.01291499  0.         -0.05765385 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.02148193  0.          0.         -0.02124964  0.01566614  0.01304492\n",
      "  0.          0.02156812  0.01585546 -0.01060253  0.          0.\n",
      "  0.02165465 -0.01322901  0.03100867  0.          0.          0.\n",
      " -0.00381299 -0.01660261  0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      "  0.         -0.01627353  0.02209254 -0.0217851   0.02148193 -0.01050701\n",
      " -0.01601497  0.          0.01351584  0.01598294  0.          0.\n",
      "  0.         -0.0162085   0.01283765 -0.01291499 -0.01276078 -0.01569754\n",
      "  0.         -0.01286338  0.02345173  0.02209254  0.          0.\n",
      "  0.01320256  0.02148193  0.          0.         -0.03415014  0.\n",
      " -0.01647019 -0.02619319  0.          0.01320256 -0.01050701  0.\n",
      "  0.          0.         -0.02124964  0.01320256  0.01301651 -0.0217851\n",
      "  0.          0.          0.         -0.01050701], Bias: 0.0\n",
      "Iteration 400: Weights: [-0.01582375  0.015729    0.01320256 -0.02635019  0.01598294  0.02105614\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.          0.\n",
      "  0.          0.          0.02156812 -0.01601497  0.01598294  0.\n",
      "  0.          0.          0.01283765  0.01283765  0.          0.02191633\n",
      "  0.         -0.01286338  0.         -0.01627353  0.         -0.00292068\n",
      "  0.02191633  0.01585546  0.          0.          0.03121754 -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.02156812  0.          0.01637157\n",
      " -0.0162085   0.03506332 -0.01647019  0.         -0.01601497  0.\n",
      "  0.         -0.01647019  0.         -0.0131235  -0.01050701  0.\n",
      "  0.015729    0.02266736 -0.01291499 -0.01582375  0.          0.\n",
      " -0.01291499  0.         -0.05765385 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.02148193  0.          0.         -0.02124964  0.01566614  0.01304492\n",
      "  0.          0.02156812  0.01585546 -0.01060253  0.          0.\n",
      "  0.02165465 -0.01322901  0.03100867  0.          0.          0.\n",
      " -0.00381299 -0.01660261  0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      "  0.         -0.01627353  0.02209254 -0.0217851   0.02148193 -0.01050701\n",
      " -0.01601497  0.          0.01351584  0.01598294  0.          0.\n",
      "  0.         -0.0162085   0.01283765 -0.01291499 -0.01276078 -0.01569754\n",
      "  0.         -0.01286338  0.02345173  0.02209254  0.          0.\n",
      "  0.01320256  0.02148193  0.          0.         -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.          0.01320256 -0.01050701  0.\n",
      "  0.          0.         -0.02124964  0.01320256  0.01301651 -0.0217851\n",
      "  0.          0.          0.         -0.01050701], Bias: 0.0\n",
      "Iteration 500: Weights: [-0.01582375  0.015729    0.01320256 -0.02635019  0.01598294  0.02105614\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.          0.\n",
      "  0.          0.          0.02156812 -0.01601497  0.01598294  0.\n",
      "  0.          0.          0.01283765  0.01283765  0.          0.02191633\n",
      "  0.         -0.01286338  0.         -0.01627353  0.         -0.00292068\n",
      "  0.02191633  0.01585546  0.          0.          0.03121754 -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.02156812  0.          0.01637157\n",
      " -0.0162085   0.03506332 -0.01647019  0.         -0.01601497  0.\n",
      "  0.         -0.01647019  0.         -0.0131235  -0.01050701  0.\n",
      "  0.015729    0.02266736 -0.01291499 -0.01582375  0.          0.\n",
      " -0.01291499  0.         -0.05765385 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.02148193  0.          0.         -0.02124964  0.01566614  0.01304492\n",
      "  0.          0.02156812  0.01585546 -0.01060253  0.          0.\n",
      "  0.02165465 -0.01322901  0.03100867  0.          0.          0.\n",
      " -0.00381299 -0.01660261  0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      "  0.         -0.01627353  0.02209254 -0.0217851   0.02148193 -0.01050701\n",
      " -0.01601497  0.          0.01351584  0.01598294  0.          0.\n",
      "  0.         -0.0162085   0.01283765 -0.01291499 -0.01276078 -0.01569754\n",
      "  0.         -0.01286338  0.02345173  0.02209254  0.          0.\n",
      "  0.01320256  0.02148193  0.          0.         -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.          0.01320256 -0.01050701  0.\n",
      "  0.          0.         -0.02124964  0.01320256  0.01301651 -0.0217851\n",
      "  0.          0.          0.         -0.01050701], Bias: 0.0\n",
      "Iteration 600: Weights: [-0.01582375  0.015729    0.01320256 -0.02635019  0.01598294  0.02105614\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.          0.\n",
      "  0.          0.          0.02156812 -0.01601497  0.01598294  0.\n",
      "  0.          0.          0.01283765  0.01283765  0.          0.02191633\n",
      "  0.         -0.01286338  0.         -0.01627353  0.         -0.00292068\n",
      "  0.02191633  0.01585546  0.          0.          0.03121754 -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.02156812  0.          0.01637157\n",
      " -0.0162085   0.03506332 -0.01647019  0.         -0.01601497  0.\n",
      "  0.         -0.01647019  0.         -0.0131235  -0.01050701  0.\n",
      "  0.015729    0.02266736 -0.01291499 -0.01582375  0.          0.\n",
      " -0.01291499  0.         -0.05765385 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.02148193  0.          0.         -0.02124964  0.01566614  0.01304492\n",
      "  0.          0.02156812  0.01585546 -0.01060253  0.          0.\n",
      "  0.02165465 -0.01322901  0.03100867  0.          0.          0.\n",
      " -0.00381299 -0.01660261  0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      "  0.         -0.01627353  0.02209254 -0.0217851   0.02148193 -0.01050701\n",
      " -0.01601497  0.          0.01351584  0.01598294  0.          0.\n",
      "  0.         -0.0162085   0.01283765 -0.01291499 -0.01276078 -0.01569754\n",
      "  0.         -0.01286338  0.02345173  0.02209254  0.          0.\n",
      "  0.01320256  0.02148193  0.          0.         -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.          0.01320256 -0.01050701  0.\n",
      "  0.          0.         -0.02124964  0.01320256  0.01301651 -0.0217851\n",
      "  0.          0.          0.         -0.01050701], Bias: 0.0\n",
      "Iteration 700: Weights: [-0.01582375  0.015729    0.01320256 -0.02635019  0.01598294  0.02105614\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.          0.\n",
      "  0.          0.          0.02156812 -0.01601497  0.01598294  0.\n",
      "  0.          0.          0.01283765  0.01283765  0.          0.02191633\n",
      "  0.         -0.01286338  0.         -0.01627353  0.         -0.00292068\n",
      "  0.02191633  0.01585546  0.          0.          0.03121754 -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.02156812  0.          0.01637157\n",
      " -0.0162085   0.03506332 -0.01647019  0.         -0.01601497  0.\n",
      "  0.         -0.01647019  0.         -0.0131235  -0.01050701  0.\n",
      "  0.015729    0.02266736 -0.01291499 -0.01582375  0.          0.\n",
      " -0.01291499  0.         -0.05765385 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.02148193  0.          0.         -0.02124964  0.01566614  0.01304492\n",
      "  0.          0.02156812  0.01585546 -0.01060253  0.          0.\n",
      "  0.02165465 -0.01322901  0.03100867  0.          0.          0.\n",
      " -0.00381299 -0.01660261  0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      "  0.         -0.01627353  0.02209254 -0.0217851   0.02148193 -0.01050701\n",
      " -0.01601497  0.          0.01351584  0.01598294  0.          0.\n",
      "  0.         -0.0162085   0.01283765 -0.01291499 -0.01276078 -0.01569754\n",
      "  0.         -0.01286338  0.02345173  0.02209254  0.          0.\n",
      "  0.01320256  0.02148193  0.          0.         -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.          0.01320256 -0.01050701  0.\n",
      "  0.          0.         -0.02124964  0.01320256  0.01301651 -0.0217851\n",
      "  0.          0.          0.         -0.01050701], Bias: 0.0\n",
      "Iteration 800: Weights: [-0.01582375  0.015729    0.01320256 -0.02635019  0.01598294  0.02105614\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.          0.\n",
      "  0.          0.          0.02156812 -0.01601497  0.01598294  0.\n",
      "  0.          0.          0.01283765  0.01283765  0.          0.02191633\n",
      "  0.         -0.01286338  0.         -0.01627353  0.         -0.00292068\n",
      "  0.02191633  0.01585546  0.          0.          0.03121754 -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.02156812  0.          0.01637157\n",
      " -0.0162085   0.03506332 -0.01647019  0.         -0.01601497  0.\n",
      "  0.         -0.01647019  0.         -0.0131235  -0.01050701  0.\n",
      "  0.015729    0.02266736 -0.01291499 -0.01582375  0.          0.\n",
      " -0.01291499  0.         -0.05765385 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.02148193  0.          0.         -0.02124964  0.01566614  0.01304492\n",
      "  0.          0.02156812  0.01585546 -0.01060253  0.          0.\n",
      "  0.02165465 -0.01322901  0.03100867  0.          0.          0.\n",
      " -0.00381299 -0.01660261  0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      "  0.         -0.01627353  0.02209254 -0.0217851   0.02148193 -0.01050701\n",
      " -0.01601497  0.          0.01351584  0.01598294  0.          0.\n",
      "  0.         -0.0162085   0.01283765 -0.01291499 -0.01276078 -0.01569754\n",
      "  0.         -0.01286338  0.02345173  0.02209254  0.          0.\n",
      "  0.01320256  0.02148193  0.          0.         -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.          0.01320256 -0.01050701  0.\n",
      "  0.          0.         -0.02124964  0.01320256  0.01301651 -0.0217851\n",
      "  0.          0.          0.         -0.01050701], Bias: 0.0\n",
      "Iteration 900: Weights: [-0.01582375  0.015729    0.01320256 -0.02635019  0.01598294  0.02105614\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.          0.\n",
      "  0.          0.          0.02156812 -0.01601497  0.01598294  0.\n",
      "  0.          0.          0.01283765  0.01283765  0.          0.02191633\n",
      "  0.         -0.01286338  0.         -0.01627353  0.         -0.00292068\n",
      "  0.02191633  0.01585546  0.          0.          0.03121754 -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.02156812  0.          0.01637157\n",
      " -0.0162085   0.03506332 -0.01647019  0.         -0.01601497  0.\n",
      "  0.         -0.01647019  0.         -0.0131235  -0.01050701  0.\n",
      "  0.015729    0.02266736 -0.01291499 -0.01582375  0.          0.\n",
      " -0.01291499  0.         -0.05765385 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.02148193  0.          0.         -0.02124964  0.01566614  0.01304492\n",
      "  0.          0.02156812  0.01585546 -0.01060253  0.          0.\n",
      "  0.02165465 -0.01322901  0.03100867  0.          0.          0.\n",
      " -0.00381299 -0.01660261  0.01320256  0.02712274  0.01637157  0.01566614\n",
      "  0.01591907  0.02209254  0.015729    0.01591907  0.01304492  0.01591907\n",
      "  0.         -0.01627353  0.02209254 -0.0217851   0.02148193 -0.01050701\n",
      " -0.01601497  0.          0.01351584  0.01598294  0.          0.\n",
      "  0.         -0.0162085   0.01283765 -0.01291499 -0.01276078 -0.01569754\n",
      "  0.         -0.01286338  0.02345173  0.02209254  0.          0.\n",
      "  0.01320256  0.02148193  0.          0.         -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.          0.01320256 -0.01050701  0.\n",
      "  0.          0.         -0.02124964  0.01320256  0.01301651 -0.0217851\n",
      "  0.          0.          0.         -0.01050701], Bias: 0.0\n",
      "Iteration 0: Weights: [-0.0009224   0.          0.         -0.00153601  0.          0.\n",
      " -0.00074983  0.         -0.0012699  -0.0009261   0.0007335   0.\n",
      "  0.00093542  0.00205629  0.         -0.00093355  0.          0.00093542\n",
      "  0.          0.00180034  0.          0.          0.00127244  0.\n",
      "  0.00096586 -0.00074983  0.00123727 -0.00094862  0.00124224 -0.00079451\n",
      "  0.          0.          0.0007335   0.00093917  0.         -0.00074385\n",
      " -0.000765   -0.00075284  0.          0.          0.00073057  0.\n",
      " -0.00094483  0.         -0.00096008  0.00073057 -0.00093355  0.00165517\n",
      "  0.00095052 -0.00096008  0.00073057 -0.000765   -0.00061248  0.00204381\n",
      "  0.          0.         -0.00075284 -0.0009224   0.00206048  0.00095052\n",
      " -0.00075284  0.         -0.00308272 -0.0009678  -0.00139335 -0.0009261\n",
      "  0.          0.          0.0007335  -0.00123869  0.          0.\n",
      "  0.00141469  0.          0.         -0.00061804  0.00138009  0.00093917\n",
      "  0.         -0.00077115  0.          0.00128267  0.00075738  0.\n",
      " -0.00151613 -0.0009678   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.00093917 -0.00094862  0.         -0.0012699   0.         -0.00061248\n",
      " -0.00093355  0.          0.00242457  0.          0.00125725  0.00095052\n",
      "  0.00075738 -0.00094483  0.         -0.00075284 -0.00074385 -0.00091504\n",
      "  0.00137749 -0.00074983  0.          0.          0.0012171   0.00075738\n",
      "  0.          0.          0.00123727  0.00152365 -0.00199068  0.\n",
      " -0.00096008 -0.00152685  0.00127755  0.         -0.00061248  0.\n",
      "  0.00206861  0.00093542 -0.00123869  0.          0.00079292 -0.0012699\n",
      "  0.00124224  0.00092055  0.00093917 -0.00061248], Bias: 0.0\n",
      "Iteration 100: Weights: [-0.01578703  0.          0.         -0.02628905  0.          0.\n",
      " -0.01283353  0.         -0.02173455 -0.01585037  0.012554    0.\n",
      "  0.01600983  0.03519376  0.         -0.01597781  0.          0.01600983\n",
      "  0.          0.0308131   0.          0.          0.02177811  0.\n",
      "  0.01653096 -0.01283353  0.02117618 -0.01623577  0.02126114 -0.01359825\n",
      "  0.          0.          0.012554    0.01607406  0.         -0.01273117\n",
      " -0.01309305 -0.01288502  0.          0.          0.01250384  0.\n",
      " -0.0161709   0.         -0.01643197  0.01250384 -0.01597781  0.02832861\n",
      "  0.01626831 -0.01643197  0.01250384 -0.01309305 -0.01048263  0.03498028\n",
      "  0.          0.         -0.01288502 -0.01578703  0.03526549  0.01626831\n",
      " -0.01288502  0.         -0.05276135 -0.01656409 -0.02384746 -0.01585037\n",
      "  0.          0.          0.012554   -0.02120034  0.          0.\n",
      "  0.02421271  0.          0.         -0.01057793  0.02362043  0.01607406\n",
      "  0.         -0.01319832  0.          0.02195321  0.01296264  0.\n",
      " -0.02594885 -0.01656409  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01607406 -0.01623577  0.         -0.02173455  0.         -0.01048263\n",
      " -0.01597781  0.          0.04149692  0.          0.02151807  0.01626831\n",
      "  0.01296264 -0.0161709   0.         -0.01288502 -0.01273117 -0.01566112\n",
      "  0.02357606 -0.01283353  0.          0.          0.02083091  0.01296264\n",
      "  0.          0.          0.02117618  0.02607765 -0.0340709   0.\n",
      " -0.01643197 -0.02613242  0.02186548  0.         -0.01048263  0.\n",
      "  0.0354047   0.01600983 -0.02120034  0.          0.01357106 -0.02173455\n",
      "  0.02126114  0.01575546  0.01607406 -0.01048263], Bias: 0.0\n",
      "Iteration 200: Weights: [-0.01582366  0.          0.         -0.02635004  0.          0.\n",
      " -0.01286331  0.         -0.02178497 -0.01588714  0.01258313  0.\n",
      "  0.01604697  0.03527541  0.         -0.01601488  0.          0.01604697\n",
      "  0.          0.03088458  0.          0.          0.02182863  0.\n",
      "  0.01656931 -0.01286331  0.02122531 -0.01627344  0.02131047 -0.0136298\n",
      "  0.          0.          0.01258313  0.01611136  0.         -0.01276071\n",
      " -0.01312342 -0.01291491  0.          0.          0.01253284  0.\n",
      " -0.01620841  0.         -0.0164701   0.01253284 -0.01601488  0.02839434\n",
      "  0.01630605 -0.0164701   0.01253284 -0.01312342 -0.01050695  0.03506144\n",
      "  0.          0.         -0.01291491 -0.01582366  0.0353473   0.01630605\n",
      " -0.01291491  0.         -0.05288376 -0.01660252 -0.02390278 -0.01588714\n",
      "  0.          0.          0.01258313 -0.02124952  0.          0.\n",
      "  0.02426888  0.          0.         -0.01060247  0.02367523  0.01611136\n",
      "  0.         -0.01322894  0.          0.02200414  0.01299271  0.\n",
      " -0.02600905 -0.01660252  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01611136 -0.01627344  0.         -0.02178497  0.         -0.01050695\n",
      " -0.01601488  0.          0.04159319  0.          0.02156799  0.01630605\n",
      "  0.01299271 -0.01620841  0.         -0.01291491 -0.01276071 -0.01569745\n",
      "  0.02363076 -0.01286331  0.          0.          0.02087924  0.01299271\n",
      "  0.          0.          0.02122531  0.02613815 -0.03414994  0.\n",
      " -0.0164701  -0.02619305  0.02191621  0.         -0.01050695  0.\n",
      "  0.03548684  0.01604697 -0.02124952  0.          0.01360254 -0.02178497\n",
      "  0.02131047  0.01579201  0.01611136 -0.01050695], Bias: 0.0\n",
      "Iteration 300: Weights: [-0.01582375  0.          0.         -0.02635019  0.          0.\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.0125832   0.\n",
      "  0.01604707  0.03527561  0.         -0.01601497  0.          0.01604707\n",
      "  0.          0.03088476  0.          0.          0.02182875  0.\n",
      "  0.01656941 -0.01286338  0.02122543 -0.01627353  0.02131059 -0.01362988\n",
      "  0.          0.          0.0125832   0.01611145  0.         -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.          0.01253292  0.\n",
      " -0.0162085   0.         -0.01647019  0.01253292 -0.01601497  0.0283945\n",
      "  0.01630614 -0.01647019  0.01253292 -0.0131235  -0.01050701  0.03506164\n",
      "  0.          0.         -0.01291499 -0.01582375  0.0353475   0.01630614\n",
      " -0.01291499  0.         -0.05288406 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.          0.          0.0125832  -0.02124964  0.          0.\n",
      "  0.02426902  0.          0.         -0.01060253  0.02367536  0.01611145\n",
      "  0.         -0.01322901  0.          0.02200426  0.01299279  0.\n",
      " -0.0260092  -0.01660261  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01611145 -0.01627353  0.         -0.0217851   0.         -0.01050701\n",
      " -0.01601497  0.          0.04159342  0.          0.02156812  0.01630614\n",
      "  0.01299279 -0.0162085   0.         -0.01291499 -0.01276078 -0.01569754\n",
      "  0.02363089 -0.01286338  0.          0.          0.02087936  0.01299279\n",
      "  0.          0.          0.02122543  0.02613829 -0.03415014  0.\n",
      " -0.01647019 -0.02619319  0.02191633  0.         -0.01050701  0.\n",
      "  0.03548704  0.01604707 -0.02124964  0.          0.01360262 -0.0217851\n",
      "  0.02131059  0.0157921   0.01611145 -0.01050701], Bias: 0.0\n",
      "Iteration 400: Weights: [-0.01582375  0.          0.         -0.02635019  0.          0.\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.0125832   0.\n",
      "  0.01604707  0.03527561  0.         -0.01601497  0.          0.01604707\n",
      "  0.          0.03088476  0.          0.          0.02182875  0.\n",
      "  0.01656941 -0.01286338  0.02122543 -0.01627353  0.02131059 -0.01362988\n",
      "  0.          0.          0.0125832   0.01611145  0.         -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.          0.01253292  0.\n",
      " -0.0162085   0.         -0.01647019  0.01253292 -0.01601497  0.0283945\n",
      "  0.01630615 -0.01647019  0.01253292 -0.0131235  -0.01050701  0.03506164\n",
      "  0.          0.         -0.01291499 -0.01582375  0.0353475   0.01630615\n",
      " -0.01291499  0.         -0.05288406 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.          0.          0.0125832  -0.02124964  0.          0.\n",
      "  0.02426902  0.          0.         -0.01060253  0.02367536  0.01611145\n",
      "  0.         -0.01322901  0.          0.02200426  0.01299279  0.\n",
      " -0.0260092  -0.01660261  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01611145 -0.01627353  0.         -0.0217851   0.         -0.01050701\n",
      " -0.01601497  0.          0.04159342  0.          0.02156812  0.01630615\n",
      "  0.01299279 -0.0162085   0.         -0.01291499 -0.01276078 -0.01569754\n",
      "  0.02363089 -0.01286338  0.          0.          0.02087936  0.01299279\n",
      "  0.          0.          0.02122543  0.02613829 -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.02191633  0.         -0.01050701  0.\n",
      "  0.03548704  0.01604707 -0.02124964  0.          0.01360262 -0.0217851\n",
      "  0.02131059  0.0157921   0.01611145 -0.01050701], Bias: 0.0\n",
      "Iteration 500: Weights: [-0.01582375  0.          0.         -0.02635019  0.          0.\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.0125832   0.\n",
      "  0.01604707  0.03527561  0.         -0.01601497  0.          0.01604707\n",
      "  0.          0.03088476  0.          0.          0.02182875  0.\n",
      "  0.01656941 -0.01286338  0.02122543 -0.01627353  0.02131059 -0.01362988\n",
      "  0.          0.          0.0125832   0.01611145  0.         -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.          0.01253292  0.\n",
      " -0.0162085   0.         -0.01647019  0.01253292 -0.01601497  0.0283945\n",
      "  0.01630615 -0.01647019  0.01253292 -0.0131235  -0.01050701  0.03506164\n",
      "  0.          0.         -0.01291499 -0.01582375  0.0353475   0.01630615\n",
      " -0.01291499  0.         -0.05288406 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.          0.          0.0125832  -0.02124964  0.          0.\n",
      "  0.02426902  0.          0.         -0.01060253  0.02367536  0.01611145\n",
      "  0.         -0.01322901  0.          0.02200426  0.01299279  0.\n",
      " -0.0260092  -0.01660261  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01611145 -0.01627353  0.         -0.0217851   0.         -0.01050701\n",
      " -0.01601497  0.          0.04159342  0.          0.02156812  0.01630615\n",
      "  0.01299279 -0.0162085   0.         -0.01291499 -0.01276078 -0.01569754\n",
      "  0.02363089 -0.01286338  0.          0.          0.02087936  0.01299279\n",
      "  0.          0.          0.02122543  0.02613829 -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.02191633  0.         -0.01050701  0.\n",
      "  0.03548704  0.01604707 -0.02124964  0.          0.01360262 -0.0217851\n",
      "  0.02131059  0.0157921   0.01611145 -0.01050701], Bias: 0.0\n",
      "Iteration 600: Weights: [-0.01582375  0.          0.         -0.02635019  0.          0.\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.0125832   0.\n",
      "  0.01604707  0.03527561  0.         -0.01601497  0.          0.01604707\n",
      "  0.          0.03088476  0.          0.          0.02182875  0.\n",
      "  0.01656941 -0.01286338  0.02122543 -0.01627353  0.02131059 -0.01362988\n",
      "  0.          0.          0.0125832   0.01611145  0.         -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.          0.01253292  0.\n",
      " -0.0162085   0.         -0.01647019  0.01253292 -0.01601497  0.0283945\n",
      "  0.01630615 -0.01647019  0.01253292 -0.0131235  -0.01050701  0.03506164\n",
      "  0.          0.         -0.01291499 -0.01582375  0.0353475   0.01630615\n",
      " -0.01291499  0.         -0.05288406 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.          0.          0.0125832  -0.02124964  0.          0.\n",
      "  0.02426902  0.          0.         -0.01060253  0.02367536  0.01611145\n",
      "  0.         -0.01322901  0.          0.02200426  0.01299279  0.\n",
      " -0.0260092  -0.01660261  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01611145 -0.01627353  0.         -0.0217851   0.         -0.01050701\n",
      " -0.01601497  0.          0.04159342  0.          0.02156812  0.01630615\n",
      "  0.01299279 -0.0162085   0.         -0.01291499 -0.01276078 -0.01569754\n",
      "  0.02363089 -0.01286338  0.          0.          0.02087936  0.01299279\n",
      "  0.          0.          0.02122543  0.02613829 -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.02191633  0.         -0.01050701  0.\n",
      "  0.03548704  0.01604707 -0.02124964  0.          0.01360262 -0.0217851\n",
      "  0.02131059  0.0157921   0.01611145 -0.01050701], Bias: 0.0\n",
      "Iteration 700: Weights: [-0.01582375  0.          0.         -0.02635019  0.          0.\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.0125832   0.\n",
      "  0.01604707  0.03527561  0.         -0.01601497  0.          0.01604707\n",
      "  0.          0.03088476  0.          0.          0.02182875  0.\n",
      "  0.01656941 -0.01286338  0.02122543 -0.01627353  0.02131059 -0.01362988\n",
      "  0.          0.          0.0125832   0.01611145  0.         -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.          0.01253292  0.\n",
      " -0.0162085   0.         -0.01647019  0.01253292 -0.01601497  0.0283945\n",
      "  0.01630615 -0.01647019  0.01253292 -0.0131235  -0.01050701  0.03506164\n",
      "  0.          0.         -0.01291499 -0.01582375  0.0353475   0.01630615\n",
      " -0.01291499  0.         -0.05288406 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.          0.          0.0125832  -0.02124964  0.          0.\n",
      "  0.02426902  0.          0.         -0.01060253  0.02367536  0.01611145\n",
      "  0.         -0.01322901  0.          0.02200426  0.01299279  0.\n",
      " -0.0260092  -0.01660261  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01611145 -0.01627353  0.         -0.0217851   0.         -0.01050701\n",
      " -0.01601497  0.          0.04159342  0.          0.02156812  0.01630615\n",
      "  0.01299279 -0.0162085   0.         -0.01291499 -0.01276078 -0.01569754\n",
      "  0.02363089 -0.01286338  0.          0.          0.02087936  0.01299279\n",
      "  0.          0.          0.02122543  0.02613829 -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.02191633  0.         -0.01050701  0.\n",
      "  0.03548704  0.01604707 -0.02124964  0.          0.01360262 -0.0217851\n",
      "  0.02131059  0.0157921   0.01611145 -0.01050701], Bias: 0.0\n",
      "Iteration 800: Weights: [-0.01582375  0.          0.         -0.02635019  0.          0.\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.0125832   0.\n",
      "  0.01604707  0.03527561  0.         -0.01601497  0.          0.01604707\n",
      "  0.          0.03088476  0.          0.          0.02182875  0.\n",
      "  0.01656941 -0.01286338  0.02122543 -0.01627353  0.02131059 -0.01362988\n",
      "  0.          0.          0.0125832   0.01611145  0.         -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.          0.01253292  0.\n",
      " -0.0162085   0.         -0.01647019  0.01253292 -0.01601497  0.0283945\n",
      "  0.01630615 -0.01647019  0.01253292 -0.0131235  -0.01050701  0.03506164\n",
      "  0.          0.         -0.01291499 -0.01582375  0.0353475   0.01630615\n",
      " -0.01291499  0.         -0.05288406 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.          0.          0.0125832  -0.02124964  0.          0.\n",
      "  0.02426902  0.          0.         -0.01060253  0.02367536  0.01611145\n",
      "  0.         -0.01322901  0.          0.02200426  0.01299279  0.\n",
      " -0.0260092  -0.01660261  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01611145 -0.01627353  0.         -0.0217851   0.         -0.01050701\n",
      " -0.01601497  0.          0.04159342  0.          0.02156812  0.01630615\n",
      "  0.01299279 -0.0162085   0.         -0.01291499 -0.01276078 -0.01569754\n",
      "  0.02363089 -0.01286338  0.          0.          0.02087936  0.01299279\n",
      "  0.          0.          0.02122543  0.02613829 -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.02191633  0.         -0.01050701  0.\n",
      "  0.03548704  0.01604707 -0.02124964  0.          0.01360262 -0.0217851\n",
      "  0.02131059  0.0157921   0.01611145 -0.01050701], Bias: 0.0\n",
      "Iteration 900: Weights: [-0.01582375  0.          0.         -0.02635019  0.          0.\n",
      " -0.01286338  0.         -0.0217851  -0.01588724  0.0125832   0.\n",
      "  0.01604707  0.03527561  0.         -0.01601497  0.          0.01604707\n",
      "  0.          0.03088476  0.          0.          0.02182875  0.\n",
      "  0.01656941 -0.01286338  0.02122543 -0.01627353  0.02131059 -0.01362988\n",
      "  0.          0.          0.0125832   0.01611145  0.         -0.01276078\n",
      " -0.0131235  -0.01291499  0.          0.          0.01253292  0.\n",
      " -0.0162085   0.         -0.01647019  0.01253292 -0.01601497  0.0283945\n",
      "  0.01630615 -0.01647019  0.01253292 -0.0131235  -0.01050701  0.03506164\n",
      "  0.          0.         -0.01291499 -0.01582375  0.0353475   0.01630615\n",
      " -0.01291499  0.         -0.05288406 -0.01660261 -0.02390292 -0.01588724\n",
      "  0.          0.          0.0125832  -0.02124964  0.          0.\n",
      "  0.02426902  0.          0.         -0.01060253  0.02367536  0.01611145\n",
      "  0.         -0.01322901  0.          0.02200426  0.01299279  0.\n",
      " -0.0260092  -0.01660261  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01611145 -0.01627353  0.         -0.0217851   0.         -0.01050701\n",
      " -0.01601497  0.          0.04159342  0.          0.02156812  0.01630615\n",
      "  0.01299279 -0.0162085   0.         -0.01291499 -0.01276078 -0.01569754\n",
      "  0.02363089 -0.01286338  0.          0.          0.02087936  0.01299279\n",
      "  0.          0.          0.02122543  0.02613829 -0.03415014  0.\n",
      " -0.01647019 -0.0261932   0.02191633  0.         -0.01050701  0.\n",
      "  0.03548704  0.01604707 -0.02124964  0.          0.01360262 -0.0217851\n",
      "  0.02131059  0.0157921   0.01611145 -0.01050701], Bias: 0.0\n",
      "Classifier (1, 2) predictions: [ 1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1.\n",
      "  1. -1.  1.  1. -1.  0.  1. -1. -1.  1. -1. -1.  1. -1.  0.  1. -1. -1.\n",
      "  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.]\n",
      "Classifier (1, 3) predictions: [ 1. -1. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  1. -1.  1.  1. -1.  1.  0.  1.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 210\u001b[0m\n\u001b[0;32m    207\u001b[0m ovo_svm\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Test the classifier on the training data\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43movo_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual Labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y)\n",
      "Cell \u001b[1;32mIn[17], line 90\u001b[0m, in \u001b[0;36mOneVsOneSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     88\u001b[0m             votes[idx, class_labels[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m             \u001b[43mvotes\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Return the class with the most votes\u001b[39;00m\n\u001b[0;32m     93\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(votes, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function to compute the TF-IDF (simplified)\n",
    "def compute_tfidf(corpus):\n",
    "    from collections import Counter\n",
    "    from math import log\n",
    "\n",
    "    vocabulary = list(set(word for document in corpus for word in document.split()))\n",
    "    vocabulary.sort()\n",
    "    tfidf_matrix = np.zeros((len(corpus), len(vocabulary)))\n",
    "\n",
    "    for i, document in enumerate(corpus):\n",
    "        word_counts = Counter(document.split())\n",
    "        for word, count in word_counts.items():\n",
    "            tf = count / len(document.split())\n",
    "            idf = log(len(corpus) / sum(1 for doc in corpus if word in doc.split()))\n",
    "            tfidf_matrix[i, vocabulary.index(word)] = tf * idf\n",
    "\n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Binary SVM Classifier\n",
    "class BinarySVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for iteration in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights - np.dot(x_i, y[idx]))\n",
    "                    self.bias -= self.learning_rate * y[idx]\n",
    "\n",
    "            # Debugging: print weights and bias at every 100 iterations\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}: Weights: {self.weights}, Bias: {self.bias}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) - self.bias)\n",
    "\n",
    "# One-vs-One SVM Classifier\n",
    "class OneVsOneSVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        for i in range(len(unique_classes)):\n",
    "            for j in range(i + 1, len(unique_classes)):\n",
    "                class_i = unique_classes[i]\n",
    "                class_j = unique_classes[j]\n",
    "\n",
    "                # Filter the data for the two classes\n",
    "                idx = np.where((y == class_i) | (y == class_j))\n",
    "                X_filtered = X[idx]\n",
    "                y_filtered = y[idx]\n",
    "\n",
    "                # Convert class labels to +1 and -1\n",
    "                y_filtered = np.where(y_filtered == class_i, 1, -1)\n",
    "\n",
    "                # Train the binary classifier\n",
    "                clf = BinarySVM(C=self.C, learning_rate=self.learning_rate, n_iters=self.n_iters)\n",
    "                clf.fit(X_filtered, y_filtered)\n",
    "                self.classifiers.append((clf, (class_i, class_j)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        votes = np.zeros((X.shape[0], len(np.unique([c for pair in [x[1] for x in self.classifiers] for c in pair]))))\n",
    "\n",
    "        for clf, class_labels in self.classifiers:\n",
    "            predictions = clf.predict(X)\n",
    "            print(f\"Classifier {class_labels} predictions: {predictions}\")  # Debugging line\n",
    "            for idx, pred in enumerate(predictions):\n",
    "                if pred == 1:\n",
    "                    votes[idx, class_labels[0]] += 1\n",
    "                else:\n",
    "                    votes[idx, class_labels[1]] += 1\n",
    "\n",
    "        # Return the class with the most votes\n",
    "        final_predictions = np.argmax(votes, axis=1)\n",
    "        print(f\"Votes: {votes}\")  # Debugging line\n",
    "        print(f\"Final predictions: {final_predictions}\")  # Debugging line\n",
    "        return final_predictions\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Sample dataset\n",
    "news_articles = [\n",
    "    \"government passes new law\",                     # Politics\n",
    "    \"football match ends in draw\",                    # Sports\n",
    "    \"new technology in smartphones\",                  # Technology\n",
    "    \"politician gives a speech\",                      # Politics\n",
    "    \"sports event attracts large crowd\",              # Sports\n",
    "    \"new tech gadgets released this year\",            # Technology\n",
    "    \"election results announced\",                     # Politics\n",
    "    \"soccer team wins championship\",                  # Sports\n",
    "    \"AI advancements in healthcare\",                  # Technology\n",
    "    \"local council meeting updates\",                  # Politics\n",
    "    \"basketball game highlights\",                     # Sports\n",
    "    \"innovations in artificial intelligence\",         # Technology\n",
    "    \"politician's new policy proposal\",               # Politics\n",
    "    \"swimming competition results\",                   # Sports\n",
    "    \"latest trends in smartphone design\",             # Technology\n",
    "    \"government budget allocation review\",            # Politics\n",
    "    \"volleyball tournament concludes\",                # Sports\n",
    "    \"breakthrough in renewable energy\",               # Technology\n",
    "    \"senator's speech on climate change\",             # Politics\n",
    "    \"baseball team training camp\",                    # Sports\n",
    "    \"tech company announces new software\",            # Technology\n",
    "    \"international relations summit\",                 # Politics\n",
    "    \"world cup qualifying matches\",                   # Sports\n",
    "    \"smart home devices market growth\",               # Technology\n",
    "    \"legislative bill discussion\",                    # Politics\n",
    "    \"rugby game results\",                            # Sports\n",
    "    \"technology in education sector\",                # Technology\n",
    "    \"mayoral election debate\",                        # Politics\n",
    "    \"national soccer league season start\",            # Sports\n",
    "    \"advancements in quantum computing\",              # Technology\n",
    "    \"press conference on new laws\",                   # Politics\n",
    "    \"hockey match final scores\",                      # Sports\n",
    "    \"virtual reality applications\",                   # Technology\n",
    "    \"parliamentary debate on economy\",                # Politics\n",
    "    \"college basketball championship\",                # Sports\n",
    "    \"latest trends in gadget development\",            # Technology\n",
    "    \"congressional committee meeting\",                # Politics\n",
    "    \"tennis tournament highlights\",                   # Sports\n",
    "    \"emerging technologies in fintech\",               # Technology\n",
    "    \"state of the union address\",                     # Politics\n",
    "    \"motorsports event results\",                      # Sports\n",
    "    \"new innovations in medical tech\",                # Technology\n",
    "    \"political rally speeches\",                       # Politics\n",
    "    \"community sports league updates\",                # Sports\n",
    "    \"tech industry conference news\",                  # Technology\n",
    "    \"government response to natural disaster\",        # Politics\n",
    "    \"annual sports awards ceremony\",                  # Sports\n",
    "    \"technological impacts on job market\"             # Technology\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3   # Technology\n",
    "]\n",
    "\n",
    "\n",
    "# Convert news articles to TF-IDF features\n",
    "X, vocabulary = compute_tfidf(news_articles)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize and train the One-vs-One SVM\n",
    "ovo_svm = OneVsOneSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "ovo_svm.fit(X, y)\n",
    "\n",
    "# Test the classifier on the training data\n",
    "predictions = ovo_svm.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual Labels:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [1 2 3]\n",
      "Iteration 0: Weights: [ 0.00000000e+00  9.13211619e-04  7.66528708e-04  0.00000000e+00\n",
      "  9.27955389e-04  1.22250060e-03  0.00000000e+00 -9.67800253e-04\n",
      "  0.00000000e+00  0.00000000e+00 -7.32033362e-04 -9.67800253e-04\n",
      " -9.33545467e-04 -2.05217480e-03  1.25222563e-03  0.00000000e+00\n",
      "  9.27955389e-04 -9.33545467e-04 -9.67800253e-04 -1.79673512e-03\n",
      "  7.45342701e-04  7.45342701e-04 -1.26989788e-03  1.27244277e-03\n",
      " -9.63932923e-04  0.00000000e+00 -1.23479930e-03  0.00000000e+00\n",
      " -1.23975336e-03  6.21766328e-04  1.27244277e-03  9.20553987e-04\n",
      " -7.32033362e-04 -9.37290881e-04  1.81246234e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  7.72691722e-04  1.25222563e-03\n",
      " -7.29108157e-04  9.50518563e-04  0.00000000e+00  2.03574539e-03\n",
      "  0.00000000e+00 -7.29108157e-04  0.00000000e+00 -1.65186301e-03\n",
      " -9.48617526e-04  0.00000000e+00 -7.29108157e-04  0.00000000e+00\n",
      "  0.00000000e+00 -2.03972694e-03  9.13211619e-04  1.86945537e-03\n",
      "  0.00000000e+00  0.00000000e+00 -2.05635742e-03 -9.48617526e-04\n",
      "  0.00000000e+00  0.00000000e+00 -2.77484746e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.24722173e-03  0.00000000e+00\n",
      " -7.32033362e-04  0.00000000e+00  9.09562425e-04  7.57376233e-04\n",
      " -1.41186138e-03  1.25222563e-03  9.20553987e-04  0.00000000e+00\n",
      " -1.37732515e-03 -9.37290881e-04  1.25724960e-03  0.00000000e+00\n",
      "  1.80033579e-03 -1.28010806e-03 -7.55861480e-04  7.72691722e-04\n",
      "  1.28869222e-03  0.00000000e+00  7.66528708e-04  1.57472220e-03\n",
      "  9.50518563e-04  9.09562425e-04  9.24247279e-04  1.28267341e-03\n",
      "  9.13211619e-04  9.24247279e-04  7.57376233e-04  9.24247279e-04\n",
      " -9.37290881e-04  0.00000000e+00  1.28267341e-03  0.00000000e+00\n",
      "  1.24722173e-03  0.00000000e+00  0.00000000e+00  7.72691722e-04\n",
      " -1.63499899e-03  9.27955389e-04 -1.25473510e-03 -9.48617526e-04\n",
      " -7.55861480e-04  0.00000000e+00  7.45342701e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.37473808e-03  0.00000000e+00\n",
      "  1.36158647e-03  1.28267341e-03 -1.90781337e-03 -7.55861480e-04\n",
      "  7.66528708e-04  1.24722173e-03 -1.23479930e-03 -1.52060736e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.27499275e-03  7.66528708e-04  0.00000000e+00  7.72691722e-04\n",
      " -2.06447521e-03 -9.33545467e-04  0.00000000e+00  7.66528708e-04\n",
      " -3.56118405e-05  0.00000000e+00 -1.23975336e-03 -9.18712879e-04\n",
      " -9.37290881e-04  0.00000000e+00], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.          0.01469335  0.01233326  0.          0.01493057  0.01966973\n",
      "  0.         -0.01557167  0.          0.         -0.01177824 -0.01557167\n",
      " -0.01502052 -0.03301899  0.020148    0.          0.01493057 -0.01502052\n",
      " -0.01557167 -0.02890903  0.01199238  0.01199238 -0.02043235  0.02047329\n",
      " -0.01550944  0.         -0.01986762  0.         -0.01994733  0.01000407\n",
      "  0.02047329  0.01481149 -0.01177824 -0.01508078  0.02916207  0.\n",
      "  0.          0.          0.01243242  0.020148   -0.01173117  0.01529361\n",
      "  0.          0.03275464  0.         -0.01173117  0.         -0.02657807\n",
      " -0.01526302  0.         -0.01173117  0.          0.         -0.03281871\n",
      "  0.01469335  0.03007908  0.          0.         -0.03308629 -0.01526302\n",
      "  0.          0.         -0.00446466  0.          0.          0.\n",
      "  0.02006749  0.         -0.01177824  0.          0.01463464  0.012186\n",
      " -0.0227165   0.020148    0.01481149  0.         -0.02216082 -0.01508078\n",
      "  0.02022884  0.          0.02896696 -0.02059662 -0.01216163  0.01243242\n",
      "  0.02073474  0.          0.01233326  0.02533689  0.01529361  0.01463464\n",
      "  0.01487091  0.0206379   0.01469335  0.01487091  0.012186    0.01487091\n",
      " -0.01508078  0.          0.0206379   0.          0.02006749  0.\n",
      "  0.          0.01243242 -0.02630673  0.01493057 -0.02018838 -0.01526302\n",
      " -0.01216163  0.          0.01199238  0.          0.          0.\n",
      " -0.0221192   0.          0.02190759  0.0206379  -0.03069625 -0.01216163\n",
      "  0.01233326  0.02006749 -0.01986762 -0.0244662   0.          0.\n",
      "  0.          0.         -0.02051432  0.01233326  0.          0.01243242\n",
      " -0.0332169  -0.01502052  0.          0.01233326 -0.00057299  0.\n",
      " -0.01994733 -0.01478186 -0.01508078  0.        ], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.          0.0147161   0.01235235  0.          0.01495369  0.01970019\n",
      "  0.         -0.01559578  0.          0.         -0.01179647 -0.01559578\n",
      " -0.01504377 -0.03307011  0.0201792   0.          0.01495369 -0.01504377\n",
      " -0.01559578 -0.02895379  0.01201095  0.01201095 -0.02046398  0.02050499\n",
      " -0.01553346  0.         -0.01989838  0.         -0.01997821  0.01001956\n",
      "  0.02050499  0.01483442 -0.01179647 -0.01510413  0.02920723  0.\n",
      "  0.          0.          0.01245167  0.0201792  -0.01174934  0.01531729\n",
      "  0.          0.03280536  0.         -0.01174934  0.         -0.02661922\n",
      " -0.01528666  0.         -0.01174934  0.          0.         -0.03286952\n",
      "  0.0147161   0.03012565  0.          0.         -0.03313751 -0.01528666\n",
      "  0.          0.         -0.00447157  0.          0.          0.\n",
      "  0.02009856  0.         -0.01179647  0.          0.0146573   0.01220487\n",
      " -0.02275168  0.0201792   0.01483442  0.         -0.02219514 -0.01510413\n",
      "  0.02026016  0.          0.02901181 -0.02062852 -0.01218046  0.01245167\n",
      "  0.02076685  0.          0.01235235  0.02537612  0.01531729  0.0146573\n",
      "  0.01489394  0.02066985  0.0147161   0.01489394  0.01220487  0.01489394\n",
      " -0.01510413  0.          0.02066985  0.          0.02009856  0.\n",
      "  0.          0.01245167 -0.02634746  0.01495369 -0.02021964 -0.01528666\n",
      " -0.01218046  0.          0.01201095  0.          0.          0.\n",
      " -0.02215345  0.          0.02194151  0.02066985 -0.03074378 -0.01218046\n",
      "  0.01235235  0.02009856 -0.01989838 -0.02450408  0.          0.\n",
      "  0.          0.         -0.02054608  0.01235235  0.          0.01245167\n",
      " -0.03326833 -0.01504377  0.          0.01235235 -0.00057387  0.\n",
      " -0.01997821 -0.01480475 -0.01510413  0.        ], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.          0.01471614  0.01235239  0.          0.01495373  0.01970024\n",
      "  0.         -0.01559582  0.          0.         -0.0117965  -0.01559582\n",
      " -0.01504381 -0.0330702   0.02017925  0.          0.01495373 -0.01504381\n",
      " -0.01559582 -0.02895386  0.01201098  0.01201098 -0.02046403  0.02050504\n",
      " -0.0155335   0.         -0.01989843  0.         -0.01997826  0.01001958\n",
      "  0.02050504  0.01483446 -0.0117965  -0.01510417  0.0292073   0.\n",
      "  0.          0.          0.0124517   0.02017925 -0.01174937  0.01531733\n",
      "  0.          0.03280544  0.         -0.01174937  0.         -0.02661929\n",
      " -0.01528669  0.         -0.01174937  0.          0.         -0.0328696\n",
      "  0.01471614  0.03012573  0.          0.         -0.0331376  -0.01528669\n",
      "  0.          0.         -0.00447159  0.          0.          0.\n",
      "  0.02009861  0.         -0.0117965   0.          0.01465733  0.0122049\n",
      " -0.02275173  0.02017925  0.01483446  0.         -0.02219519 -0.01510417\n",
      "  0.02026021  0.          0.02901189 -0.02062857 -0.01218049  0.0124517\n",
      "  0.0207669   0.          0.01235239  0.02537619  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      " -0.01510417  0.          0.02066991  0.          0.02009861  0.\n",
      "  0.          0.0124517  -0.02634753  0.01495373 -0.02021969 -0.01528669\n",
      " -0.01218049  0.          0.01201098  0.          0.          0.\n",
      " -0.0221535   0.          0.02194157  0.02066991 -0.03074386 -0.01218049\n",
      "  0.01235239  0.02009861 -0.01989843 -0.02450414  0.          0.\n",
      "  0.          0.         -0.02054614  0.01235239  0.          0.0124517\n",
      " -0.03326842 -0.01504381  0.          0.01235239 -0.00057387  0.\n",
      " -0.01997826 -0.01480479 -0.01510417  0.        ], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.          0.01471614  0.01235239  0.          0.01495373  0.01970024\n",
      "  0.         -0.01559582  0.          0.         -0.0117965  -0.01559582\n",
      " -0.01504381 -0.0330702   0.02017925  0.          0.01495373 -0.01504381\n",
      " -0.01559582 -0.02895386  0.01201098  0.01201098 -0.02046403  0.02050504\n",
      " -0.0155335   0.         -0.01989843  0.         -0.01997826  0.01001958\n",
      "  0.02050504  0.01483446 -0.0117965  -0.01510417  0.0292073   0.\n",
      "  0.          0.          0.0124517   0.02017925 -0.01174937  0.01531733\n",
      "  0.          0.03280544  0.         -0.01174937  0.         -0.02661929\n",
      " -0.01528669  0.         -0.01174937  0.          0.         -0.0328696\n",
      "  0.01471614  0.03012573  0.          0.         -0.0331376  -0.01528669\n",
      "  0.          0.         -0.00447159  0.          0.          0.\n",
      "  0.02009861  0.         -0.0117965   0.          0.01465733  0.0122049\n",
      " -0.02275173  0.02017925  0.01483446  0.         -0.02219519 -0.01510417\n",
      "  0.02026021  0.          0.02901189 -0.02062857 -0.01218049  0.0124517\n",
      "  0.0207669   0.          0.01235239  0.02537619  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      " -0.01510417  0.          0.02066991  0.          0.02009861  0.\n",
      "  0.          0.0124517  -0.02634753  0.01495373 -0.02021969 -0.01528669\n",
      " -0.01218049  0.          0.01201098  0.          0.          0.\n",
      " -0.0221535   0.          0.02194157  0.02066991 -0.03074386 -0.01218049\n",
      "  0.01235239  0.02009861 -0.01989843 -0.02450414  0.          0.\n",
      "  0.          0.         -0.02054614  0.01235239  0.          0.0124517\n",
      " -0.03326842 -0.01504381  0.          0.01235239 -0.00057387  0.\n",
      " -0.01997826 -0.01480479 -0.01510417  0.        ], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.          0.01471614  0.01235239  0.          0.01495373  0.01970024\n",
      "  0.         -0.01559582  0.          0.         -0.0117965  -0.01559582\n",
      " -0.01504381 -0.0330702   0.02017925  0.          0.01495373 -0.01504381\n",
      " -0.01559582 -0.02895386  0.01201098  0.01201098 -0.02046403  0.02050504\n",
      " -0.0155335   0.         -0.01989843  0.         -0.01997826  0.01001958\n",
      "  0.02050504  0.01483446 -0.0117965  -0.01510417  0.0292073   0.\n",
      "  0.          0.          0.0124517   0.02017925 -0.01174937  0.01531733\n",
      "  0.          0.03280544  0.         -0.01174937  0.         -0.02661929\n",
      " -0.01528669  0.         -0.01174937  0.          0.         -0.0328696\n",
      "  0.01471614  0.03012573  0.          0.         -0.0331376  -0.01528669\n",
      "  0.          0.         -0.00447159  0.          0.          0.\n",
      "  0.02009861  0.         -0.0117965   0.          0.01465733  0.0122049\n",
      " -0.02275173  0.02017925  0.01483446  0.         -0.02219519 -0.01510417\n",
      "  0.02026021  0.          0.02901189 -0.02062857 -0.01218049  0.0124517\n",
      "  0.0207669   0.          0.01235239  0.02537619  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      " -0.01510417  0.          0.02066991  0.          0.02009861  0.\n",
      "  0.          0.0124517  -0.02634753  0.01495373 -0.02021969 -0.01528669\n",
      " -0.01218049  0.          0.01201098  0.          0.          0.\n",
      " -0.0221535   0.          0.02194157  0.02066991 -0.03074386 -0.01218049\n",
      "  0.01235239  0.02009861 -0.01989843 -0.02450414  0.          0.\n",
      "  0.          0.         -0.02054614  0.01235239  0.          0.0124517\n",
      " -0.03326842 -0.01504381  0.          0.01235239 -0.00057387  0.\n",
      " -0.01997826 -0.01480479 -0.01510417  0.        ], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.          0.01471614  0.01235239  0.          0.01495373  0.01970024\n",
      "  0.         -0.01559582  0.          0.         -0.0117965  -0.01559582\n",
      " -0.01504381 -0.0330702   0.02017925  0.          0.01495373 -0.01504381\n",
      " -0.01559582 -0.02895386  0.01201098  0.01201098 -0.02046403  0.02050504\n",
      " -0.0155335   0.         -0.01989843  0.         -0.01997826  0.01001958\n",
      "  0.02050504  0.01483446 -0.0117965  -0.01510417  0.0292073   0.\n",
      "  0.          0.          0.0124517   0.02017925 -0.01174937  0.01531733\n",
      "  0.          0.03280544  0.         -0.01174937  0.         -0.02661929\n",
      " -0.01528669  0.         -0.01174937  0.          0.         -0.0328696\n",
      "  0.01471614  0.03012573  0.          0.         -0.0331376  -0.01528669\n",
      "  0.          0.         -0.00447159  0.          0.          0.\n",
      "  0.02009861  0.         -0.0117965   0.          0.01465733  0.0122049\n",
      " -0.02275173  0.02017925  0.01483446  0.         -0.02219519 -0.01510417\n",
      "  0.02026021  0.          0.02901189 -0.02062857 -0.01218049  0.0124517\n",
      "  0.0207669   0.          0.01235239  0.02537619  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      " -0.01510417  0.          0.02066991  0.          0.02009861  0.\n",
      "  0.          0.0124517  -0.02634753  0.01495373 -0.02021969 -0.01528669\n",
      " -0.01218049  0.          0.01201098  0.          0.          0.\n",
      " -0.0221535   0.          0.02194157  0.02066991 -0.03074386 -0.01218049\n",
      "  0.01235239  0.02009861 -0.01989843 -0.02450414  0.          0.\n",
      "  0.          0.         -0.02054614  0.01235239  0.          0.0124517\n",
      " -0.03326842 -0.01504381  0.          0.01235239 -0.00057387  0.\n",
      " -0.01997826 -0.01480479 -0.01510417  0.        ], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.          0.01471614  0.01235239  0.          0.01495373  0.01970024\n",
      "  0.         -0.01559582  0.          0.         -0.0117965  -0.01559582\n",
      " -0.01504381 -0.0330702   0.02017925  0.          0.01495373 -0.01504381\n",
      " -0.01559582 -0.02895386  0.01201098  0.01201098 -0.02046403  0.02050504\n",
      " -0.0155335   0.         -0.01989843  0.         -0.01997826  0.01001958\n",
      "  0.02050504  0.01483446 -0.0117965  -0.01510417  0.0292073   0.\n",
      "  0.          0.          0.0124517   0.02017925 -0.01174937  0.01531733\n",
      "  0.          0.03280544  0.         -0.01174937  0.         -0.02661929\n",
      " -0.01528669  0.         -0.01174937  0.          0.         -0.0328696\n",
      "  0.01471614  0.03012573  0.          0.         -0.0331376  -0.01528669\n",
      "  0.          0.         -0.00447159  0.          0.          0.\n",
      "  0.02009861  0.         -0.0117965   0.          0.01465733  0.0122049\n",
      " -0.02275173  0.02017925  0.01483446  0.         -0.02219519 -0.01510417\n",
      "  0.02026021  0.          0.02901189 -0.02062857 -0.01218049  0.0124517\n",
      "  0.0207669   0.          0.01235239  0.02537619  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      " -0.01510417  0.          0.02066991  0.          0.02009861  0.\n",
      "  0.          0.0124517  -0.02634753  0.01495373 -0.02021969 -0.01528669\n",
      " -0.01218049  0.          0.01201098  0.          0.          0.\n",
      " -0.0221535   0.          0.02194157  0.02066991 -0.03074386 -0.01218049\n",
      "  0.01235239  0.02009861 -0.01989843 -0.02450414  0.          0.\n",
      "  0.          0.         -0.02054614  0.01235239  0.          0.0124517\n",
      " -0.03326842 -0.01504381  0.          0.01235239 -0.00057387  0.\n",
      " -0.01997826 -0.01480479 -0.01510417  0.        ], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.          0.01471614  0.01235239  0.          0.01495373  0.01970024\n",
      "  0.         -0.01559582  0.          0.         -0.0117965  -0.01559582\n",
      " -0.01504381 -0.0330702   0.02017925  0.          0.01495373 -0.01504381\n",
      " -0.01559582 -0.02895386  0.01201098  0.01201098 -0.02046403  0.02050504\n",
      " -0.0155335   0.         -0.01989843  0.         -0.01997826  0.01001958\n",
      "  0.02050504  0.01483446 -0.0117965  -0.01510417  0.0292073   0.\n",
      "  0.          0.          0.0124517   0.02017925 -0.01174937  0.01531733\n",
      "  0.          0.03280544  0.         -0.01174937  0.         -0.02661929\n",
      " -0.01528669  0.         -0.01174937  0.          0.         -0.0328696\n",
      "  0.01471614  0.03012573  0.          0.         -0.0331376  -0.01528669\n",
      "  0.          0.         -0.00447159  0.          0.          0.\n",
      "  0.02009861  0.         -0.0117965   0.          0.01465733  0.0122049\n",
      " -0.02275173  0.02017925  0.01483446  0.         -0.02219519 -0.01510417\n",
      "  0.02026021  0.          0.02901189 -0.02062857 -0.01218049  0.0124517\n",
      "  0.0207669   0.          0.01235239  0.02537619  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      " -0.01510417  0.          0.02066991  0.          0.02009861  0.\n",
      "  0.          0.0124517  -0.02634753  0.01495373 -0.02021969 -0.01528669\n",
      " -0.01218049  0.          0.01201098  0.          0.          0.\n",
      " -0.0221535   0.          0.02194157  0.02066991 -0.03074386 -0.01218049\n",
      "  0.01235239  0.02009861 -0.01989843 -0.02450414  0.          0.\n",
      "  0.          0.         -0.02054614  0.01235239  0.          0.0124517\n",
      " -0.03326842 -0.01504381  0.          0.01235239 -0.00057387  0.\n",
      " -0.01997826 -0.01480479 -0.01510417  0.        ], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.          0.01471614  0.01235239  0.          0.01495373  0.01970024\n",
      "  0.         -0.01559582  0.          0.         -0.0117965  -0.01559582\n",
      " -0.01504381 -0.0330702   0.02017925  0.          0.01495373 -0.01504381\n",
      " -0.01559582 -0.02895386  0.01201098  0.01201098 -0.02046403  0.02050504\n",
      " -0.0155335   0.         -0.01989843  0.         -0.01997826  0.01001958\n",
      "  0.02050504  0.01483446 -0.0117965  -0.01510417  0.0292073   0.\n",
      "  0.          0.          0.0124517   0.02017925 -0.01174937  0.01531733\n",
      "  0.          0.03280544  0.         -0.01174937  0.         -0.02661929\n",
      " -0.01528669  0.         -0.01174937  0.          0.         -0.0328696\n",
      "  0.01471614  0.03012573  0.          0.         -0.0331376  -0.01528669\n",
      "  0.          0.         -0.00447159  0.          0.          0.\n",
      "  0.02009861  0.         -0.0117965   0.          0.01465733  0.0122049\n",
      " -0.02275173  0.02017925  0.01483446  0.         -0.02219519 -0.01510417\n",
      "  0.02026021  0.          0.02901189 -0.02062857 -0.01218049  0.0124517\n",
      "  0.0207669   0.          0.01235239  0.02537619  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      " -0.01510417  0.          0.02066991  0.          0.02009861  0.\n",
      "  0.          0.0124517  -0.02634753  0.01495373 -0.02021969 -0.01528669\n",
      " -0.01218049  0.          0.01201098  0.          0.          0.\n",
      " -0.0221535   0.          0.02194157  0.02066991 -0.03074386 -0.01218049\n",
      "  0.01235239  0.02009861 -0.01989843 -0.02450414  0.          0.\n",
      "  0.          0.         -0.02054614  0.01235239  0.          0.0124517\n",
      " -0.03326842 -0.01504381  0.          0.01235239 -0.00057387  0.\n",
      " -0.01997826 -0.01480479 -0.01510417  0.        ], Bias: 0.0\n",
      "Iteration 0: Weights: [-0.00091871  0.00091321  0.00076653 -0.00152987  0.00092796  0.0012225\n",
      " -0.00074684  0.         -0.00126482 -0.0009224   0.          0.\n",
      "  0.          0.          0.00125223 -0.00092982  0.00092796  0.\n",
      "  0.          0.          0.00074534  0.00074534  0.          0.00127244\n",
      "  0.         -0.00074684  0.         -0.00094483  0.         -0.00016957\n",
      "  0.00127244  0.00092055  0.          0.          0.00181246 -0.00074088\n",
      " -0.00076194 -0.00074983  0.00077269  0.00125223  0.          0.00095052\n",
      " -0.00094105  0.00203575 -0.00095624  0.         -0.00092982  0.\n",
      "  0.         -0.00095624  0.         -0.00076194 -0.00061003  0.\n",
      "  0.00091321  0.00186946 -0.00074983 -0.00091871  0.          0.\n",
      " -0.00074983 -0.00077424 -0.00334733 -0.00096393 -0.00138778 -0.0009224\n",
      "  0.00124722 -0.00077424  0.         -0.00123374  0.00090956  0.00075738\n",
      "  0.          0.00125223  0.00092055 -0.00125118  0.          0.\n",
      "  0.00125725 -0.00076806  0.00180034  0.          0.          0.00077269\n",
      " -0.00022138 -0.00096393  0.00076653  0.00107774  0.00095052  0.00090956\n",
      "  0.00092425  0.00128267  0.00091321  0.00092425  0.00075738  0.00092425\n",
      "  0.         -0.00094483  0.00128267 -0.00126482  0.00124722 -0.00061003\n",
      " -0.00092982  0.00077269  0.00078472  0.00092796  0.          0.\n",
      "  0.         -0.00094105  0.00074534 -0.00074983 -0.00074088 -0.00091139\n",
      "  0.         -0.00074684  0.00136159  0.00128267  0.          0.\n",
      "  0.00076653  0.00124722  0.          0.         -0.00198273 -0.00077424\n",
      " -0.00095624 -0.00152075  0.          0.00076653 -0.00061003  0.00077269\n",
      "  0.          0.         -0.00123374  0.00076653  0.00075573 -0.00126482\n",
      "  0.          0.          0.         -0.00061003], Bias: 0.0\n",
      "Iteration 100: Weights: [-0.01478186  0.01469335  0.01233326 -0.02461521  0.01493057  0.01966973\n",
      " -0.01201641  0.         -0.0203507  -0.01484117  0.          0.\n",
      "  0.          0.          0.020148   -0.0149605   0.01493057  0.\n",
      "  0.          0.          0.01199238  0.01199238  0.          0.02047329\n",
      "  0.         -0.01201641  0.         -0.01520203  0.         -0.00272838\n",
      "  0.02047329  0.01481149  0.          0.          0.02916207 -0.01192057\n",
      " -0.01225941 -0.01206462  0.01243242  0.020148    0.          0.01529361\n",
      " -0.01514128  0.03275464 -0.01538574  0.         -0.0149605   0.\n",
      "  0.         -0.01538574  0.         -0.01225941 -0.0098152   0.\n",
      "  0.01469335  0.03007908 -0.01206462 -0.01478186  0.          0.\n",
      " -0.01206462 -0.01245733 -0.05385774 -0.01550944 -0.02232908 -0.01484117\n",
      "  0.02006749 -0.01245733  0.         -0.0198505   0.01463464  0.012186\n",
      "  0.          0.020148    0.01481149 -0.02013125  0.          0.\n",
      "  0.02022884 -0.01235797  0.02896696  0.          0.          0.01243242\n",
      " -0.00356193 -0.01550944  0.01233326  0.01734059  0.01529361  0.01463464\n",
      "  0.01487091  0.0206379   0.01469335  0.01487091  0.012186    0.01487091\n",
      "  0.         -0.01520203  0.0206379  -0.0203507   0.02006749 -0.0098152\n",
      " -0.0149605   0.01243242  0.01262591  0.01493057  0.          0.\n",
      "  0.         -0.01514128  0.01199238 -0.01206462 -0.01192057 -0.01466396\n",
      "  0.         -0.01201641  0.02190759  0.0206379   0.          0.\n",
      "  0.01233326  0.02006749  0.          0.         -0.03190159 -0.01245733\n",
      " -0.01538574 -0.02446855  0.          0.01233326 -0.0098152   0.01243242\n",
      "  0.          0.         -0.0198505   0.01233326  0.01215946 -0.0203507\n",
      "  0.          0.          0.         -0.0098152 ], Bias: 0.0\n",
      "Iteration 200: Weights: [-0.01480475  0.0147161   0.01235235 -0.02465333  0.01495369  0.01970019\n",
      " -0.01203502  0.         -0.02038221 -0.01486415  0.          0.\n",
      "  0.          0.          0.0201792  -0.01498366  0.01495369  0.\n",
      "  0.          0.          0.01201095  0.01201095  0.          0.02050499\n",
      "  0.         -0.01203502  0.         -0.01522557  0.         -0.0027326\n",
      "  0.02050499  0.01483442  0.          0.          0.02920723 -0.01193903\n",
      " -0.01227839 -0.0120833   0.01245167  0.0201792   0.          0.01531729\n",
      " -0.01516473  0.03280536 -0.01540956  0.         -0.01498366  0.\n",
      "  0.         -0.01540956  0.         -0.01227839 -0.00983039  0.\n",
      "  0.0147161   0.03012565 -0.0120833  -0.01480475  0.          0.\n",
      " -0.0120833  -0.01247662 -0.05394113 -0.01553346 -0.02236365 -0.01486415\n",
      "  0.02009856 -0.01247662  0.         -0.01988123  0.0146573   0.01220487\n",
      "  0.          0.0201792   0.01483442 -0.02016242  0.          0.\n",
      "  0.02026016 -0.01237711  0.02901181  0.          0.          0.01245167\n",
      " -0.00356744 -0.01553346  0.01235235  0.01736744  0.01531729  0.0146573\n",
      "  0.01489394  0.02066985  0.0147161   0.01489394  0.01220487  0.01489394\n",
      "  0.         -0.01522557  0.02066985 -0.02038221  0.02009856 -0.00983039\n",
      " -0.01498366  0.01245167  0.01264546  0.01495369  0.          0.\n",
      "  0.         -0.01516473  0.01201095 -0.0120833  -0.01193903 -0.01468667\n",
      "  0.         -0.01203502  0.02194151  0.02066985  0.          0.\n",
      "  0.01235235  0.02009856  0.          0.         -0.03195098 -0.01247662\n",
      " -0.01540956 -0.02450644  0.          0.01235235 -0.00983039  0.01245167\n",
      "  0.          0.         -0.01988123  0.01235235  0.01217828 -0.02038221\n",
      "  0.          0.          0.         -0.00983039], Bias: 0.0\n",
      "Iteration 300: Weights: [-0.01480479  0.01471614  0.01235239 -0.02465339  0.01495373  0.01970024\n",
      " -0.01203505  0.         -0.02038226 -0.01486419  0.          0.\n",
      "  0.          0.          0.02017925 -0.0149837   0.01495373  0.\n",
      "  0.          0.          0.01201098  0.01201098  0.          0.02050504\n",
      "  0.         -0.01203505  0.         -0.01522561  0.         -0.00273261\n",
      "  0.02050504  0.01483446  0.          0.          0.0292073  -0.01193906\n",
      " -0.01227842 -0.01208334  0.0124517   0.02017925  0.          0.01531733\n",
      " -0.01516477  0.03280544 -0.0154096   0.         -0.0149837   0.\n",
      "  0.         -0.0154096   0.         -0.01227842 -0.00983042  0.\n",
      "  0.01471614  0.03012573 -0.01208334 -0.01480479  0.          0.\n",
      " -0.01208334 -0.01247665 -0.05394127 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.02009861 -0.01247665  0.         -0.01988128  0.01465733  0.0122049\n",
      "  0.          0.02017925  0.01483446 -0.02016247  0.          0.\n",
      "  0.02026021 -0.01237714  0.02901189  0.          0.          0.0124517\n",
      " -0.00356745 -0.0155335   0.01235239  0.01736748  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      "  0.         -0.01522561  0.02066991 -0.02038226  0.02009861 -0.00983042\n",
      " -0.0149837   0.0124517   0.0126455   0.01495373  0.          0.\n",
      "  0.         -0.01516477  0.01201098 -0.01208334 -0.01193906 -0.01468671\n",
      "  0.         -0.01203505  0.02194157  0.02066991  0.          0.\n",
      "  0.01235239  0.02009861  0.          0.         -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.          0.01235239 -0.00983042  0.0124517\n",
      "  0.          0.         -0.01988128  0.01235239  0.01217832 -0.02038226\n",
      "  0.          0.          0.         -0.00983042], Bias: 0.0\n",
      "Iteration 400: Weights: [-0.01480479  0.01471614  0.01235239 -0.02465339  0.01495373  0.01970024\n",
      " -0.01203505  0.         -0.02038226 -0.01486419  0.          0.\n",
      "  0.          0.          0.02017925 -0.0149837   0.01495373  0.\n",
      "  0.          0.          0.01201098  0.01201098  0.          0.02050504\n",
      "  0.         -0.01203505  0.         -0.01522561  0.         -0.00273261\n",
      "  0.02050504  0.01483446  0.          0.          0.0292073  -0.01193906\n",
      " -0.01227842 -0.01208334  0.0124517   0.02017925  0.          0.01531733\n",
      " -0.01516477  0.03280544 -0.0154096   0.         -0.0149837   0.\n",
      "  0.         -0.0154096   0.         -0.01227842 -0.00983042  0.\n",
      "  0.01471614  0.03012573 -0.01208334 -0.01480479  0.          0.\n",
      " -0.01208334 -0.01247665 -0.05394127 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.02009861 -0.01247665  0.         -0.01988128  0.01465733  0.0122049\n",
      "  0.          0.02017925  0.01483446 -0.02016247  0.          0.\n",
      "  0.02026021 -0.01237714  0.02901189  0.          0.          0.0124517\n",
      " -0.00356745 -0.0155335   0.01235239  0.01736748  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      "  0.         -0.01522561  0.02066991 -0.02038226  0.02009861 -0.00983042\n",
      " -0.0149837   0.0124517   0.0126455   0.01495373  0.          0.\n",
      "  0.         -0.01516477  0.01201098 -0.01208334 -0.01193906 -0.01468671\n",
      "  0.         -0.01203505  0.02194157  0.02066991  0.          0.\n",
      "  0.01235239  0.02009861  0.          0.         -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.          0.01235239 -0.00983042  0.0124517\n",
      "  0.          0.         -0.01988128  0.01235239  0.01217832 -0.02038226\n",
      "  0.          0.          0.         -0.00983042], Bias: 0.0\n",
      "Iteration 500: Weights: [-0.01480479  0.01471614  0.01235239 -0.02465339  0.01495373  0.01970024\n",
      " -0.01203505  0.         -0.02038226 -0.01486419  0.          0.\n",
      "  0.          0.          0.02017925 -0.0149837   0.01495373  0.\n",
      "  0.          0.          0.01201098  0.01201098  0.          0.02050504\n",
      "  0.         -0.01203505  0.         -0.01522561  0.         -0.00273261\n",
      "  0.02050504  0.01483446  0.          0.          0.0292073  -0.01193906\n",
      " -0.01227842 -0.01208334  0.0124517   0.02017925  0.          0.01531733\n",
      " -0.01516477  0.03280544 -0.0154096   0.         -0.0149837   0.\n",
      "  0.         -0.0154096   0.         -0.01227842 -0.00983042  0.\n",
      "  0.01471614  0.03012573 -0.01208334 -0.01480479  0.          0.\n",
      " -0.01208334 -0.01247665 -0.05394127 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.02009861 -0.01247665  0.         -0.01988128  0.01465733  0.0122049\n",
      "  0.          0.02017925  0.01483446 -0.02016247  0.          0.\n",
      "  0.02026021 -0.01237714  0.02901189  0.          0.          0.0124517\n",
      " -0.00356745 -0.0155335   0.01235239  0.01736748  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      "  0.         -0.01522561  0.02066991 -0.02038226  0.02009861 -0.00983042\n",
      " -0.0149837   0.0124517   0.0126455   0.01495373  0.          0.\n",
      "  0.         -0.01516477  0.01201098 -0.01208334 -0.01193906 -0.01468671\n",
      "  0.         -0.01203505  0.02194157  0.02066991  0.          0.\n",
      "  0.01235239  0.02009861  0.          0.         -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.          0.01235239 -0.00983042  0.0124517\n",
      "  0.          0.         -0.01988128  0.01235239  0.01217832 -0.02038226\n",
      "  0.          0.          0.         -0.00983042], Bias: 0.0\n",
      "Iteration 600: Weights: [-0.01480479  0.01471614  0.01235239 -0.02465339  0.01495373  0.01970024\n",
      " -0.01203505  0.         -0.02038226 -0.01486419  0.          0.\n",
      "  0.          0.          0.02017925 -0.0149837   0.01495373  0.\n",
      "  0.          0.          0.01201098  0.01201098  0.          0.02050504\n",
      "  0.         -0.01203505  0.         -0.01522561  0.         -0.00273261\n",
      "  0.02050504  0.01483446  0.          0.          0.0292073  -0.01193906\n",
      " -0.01227842 -0.01208334  0.0124517   0.02017925  0.          0.01531733\n",
      " -0.01516477  0.03280544 -0.0154096   0.         -0.0149837   0.\n",
      "  0.         -0.0154096   0.         -0.01227842 -0.00983042  0.\n",
      "  0.01471614  0.03012573 -0.01208334 -0.01480479  0.          0.\n",
      " -0.01208334 -0.01247665 -0.05394127 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.02009861 -0.01247665  0.         -0.01988128  0.01465733  0.0122049\n",
      "  0.          0.02017925  0.01483446 -0.02016247  0.          0.\n",
      "  0.02026021 -0.01237714  0.02901189  0.          0.          0.0124517\n",
      " -0.00356745 -0.0155335   0.01235239  0.01736748  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      "  0.         -0.01522561  0.02066991 -0.02038226  0.02009861 -0.00983042\n",
      " -0.0149837   0.0124517   0.0126455   0.01495373  0.          0.\n",
      "  0.         -0.01516477  0.01201098 -0.01208334 -0.01193906 -0.01468671\n",
      "  0.         -0.01203505  0.02194157  0.02066991  0.          0.\n",
      "  0.01235239  0.02009861  0.          0.         -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.          0.01235239 -0.00983042  0.0124517\n",
      "  0.          0.         -0.01988128  0.01235239  0.01217832 -0.02038226\n",
      "  0.          0.          0.         -0.00983042], Bias: 0.0\n",
      "Iteration 700: Weights: [-0.01480479  0.01471614  0.01235239 -0.02465339  0.01495373  0.01970024\n",
      " -0.01203505  0.         -0.02038226 -0.01486419  0.          0.\n",
      "  0.          0.          0.02017925 -0.0149837   0.01495373  0.\n",
      "  0.          0.          0.01201098  0.01201098  0.          0.02050504\n",
      "  0.         -0.01203505  0.         -0.01522561  0.         -0.00273261\n",
      "  0.02050504  0.01483446  0.          0.          0.0292073  -0.01193906\n",
      " -0.01227842 -0.01208334  0.0124517   0.02017925  0.          0.01531733\n",
      " -0.01516477  0.03280544 -0.0154096   0.         -0.0149837   0.\n",
      "  0.         -0.0154096   0.         -0.01227842 -0.00983042  0.\n",
      "  0.01471614  0.03012573 -0.01208334 -0.01480479  0.          0.\n",
      " -0.01208334 -0.01247665 -0.05394127 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.02009861 -0.01247665  0.         -0.01988128  0.01465733  0.0122049\n",
      "  0.          0.02017925  0.01483446 -0.02016247  0.          0.\n",
      "  0.02026021 -0.01237714  0.02901189  0.          0.          0.0124517\n",
      " -0.00356745 -0.0155335   0.01235239  0.01736748  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      "  0.         -0.01522561  0.02066991 -0.02038226  0.02009861 -0.00983042\n",
      " -0.0149837   0.0124517   0.0126455   0.01495373  0.          0.\n",
      "  0.         -0.01516477  0.01201098 -0.01208334 -0.01193906 -0.01468671\n",
      "  0.         -0.01203505  0.02194157  0.02066991  0.          0.\n",
      "  0.01235239  0.02009861  0.          0.         -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.          0.01235239 -0.00983042  0.0124517\n",
      "  0.          0.         -0.01988128  0.01235239  0.01217832 -0.02038226\n",
      "  0.          0.          0.         -0.00983042], Bias: 0.0\n",
      "Iteration 800: Weights: [-0.01480479  0.01471614  0.01235239 -0.02465339  0.01495373  0.01970024\n",
      " -0.01203505  0.         -0.02038226 -0.01486419  0.          0.\n",
      "  0.          0.          0.02017925 -0.0149837   0.01495373  0.\n",
      "  0.          0.          0.01201098  0.01201098  0.          0.02050504\n",
      "  0.         -0.01203505  0.         -0.01522561  0.         -0.00273261\n",
      "  0.02050504  0.01483446  0.          0.          0.0292073  -0.01193906\n",
      " -0.01227842 -0.01208334  0.0124517   0.02017925  0.          0.01531733\n",
      " -0.01516477  0.03280544 -0.0154096   0.         -0.0149837   0.\n",
      "  0.         -0.0154096   0.         -0.01227842 -0.00983042  0.\n",
      "  0.01471614  0.03012573 -0.01208334 -0.01480479  0.          0.\n",
      " -0.01208334 -0.01247665 -0.05394127 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.02009861 -0.01247665  0.         -0.01988128  0.01465733  0.0122049\n",
      "  0.          0.02017925  0.01483446 -0.02016247  0.          0.\n",
      "  0.02026021 -0.01237714  0.02901189  0.          0.          0.0124517\n",
      " -0.00356745 -0.0155335   0.01235239  0.01736748  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      "  0.         -0.01522561  0.02066991 -0.02038226  0.02009861 -0.00983042\n",
      " -0.0149837   0.0124517   0.0126455   0.01495373  0.          0.\n",
      "  0.         -0.01516477  0.01201098 -0.01208334 -0.01193906 -0.01468671\n",
      "  0.         -0.01203505  0.02194157  0.02066991  0.          0.\n",
      "  0.01235239  0.02009861  0.          0.         -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.          0.01235239 -0.00983042  0.0124517\n",
      "  0.          0.         -0.01988128  0.01235239  0.01217832 -0.02038226\n",
      "  0.          0.          0.         -0.00983042], Bias: 0.0\n",
      "Iteration 900: Weights: [-0.01480479  0.01471614  0.01235239 -0.02465339  0.01495373  0.01970024\n",
      " -0.01203505  0.         -0.02038226 -0.01486419  0.          0.\n",
      "  0.          0.          0.02017925 -0.0149837   0.01495373  0.\n",
      "  0.          0.          0.01201098  0.01201098  0.          0.02050504\n",
      "  0.         -0.01203505  0.         -0.01522561  0.         -0.00273261\n",
      "  0.02050504  0.01483446  0.          0.          0.0292073  -0.01193906\n",
      " -0.01227842 -0.01208334  0.0124517   0.02017925  0.          0.01531733\n",
      " -0.01516477  0.03280544 -0.0154096   0.         -0.0149837   0.\n",
      "  0.         -0.0154096   0.         -0.01227842 -0.00983042  0.\n",
      "  0.01471614  0.03012573 -0.01208334 -0.01480479  0.          0.\n",
      " -0.01208334 -0.01247665 -0.05394127 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.02009861 -0.01247665  0.         -0.01988128  0.01465733  0.0122049\n",
      "  0.          0.02017925  0.01483446 -0.02016247  0.          0.\n",
      "  0.02026021 -0.01237714  0.02901189  0.          0.          0.0124517\n",
      " -0.00356745 -0.0155335   0.01235239  0.01736748  0.01531733  0.01465733\n",
      "  0.01489398  0.02066991  0.01471614  0.01489398  0.0122049   0.01489398\n",
      "  0.         -0.01522561  0.02066991 -0.02038226  0.02009861 -0.00983042\n",
      " -0.0149837   0.0124517   0.0126455   0.01495373  0.          0.\n",
      "  0.         -0.01516477  0.01201098 -0.01208334 -0.01193906 -0.01468671\n",
      "  0.         -0.01203505  0.02194157  0.02066991  0.          0.\n",
      "  0.01235239  0.02009861  0.          0.         -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.          0.01235239 -0.00983042  0.0124517\n",
      "  0.          0.         -0.01988128  0.01235239  0.01217832 -0.02038226\n",
      "  0.          0.          0.         -0.00983042], Bias: 0.0\n",
      "Iteration 0: Weights: [-0.00091871  0.          0.         -0.00152987  0.          0.\n",
      " -0.00074684  0.00096586 -0.00126482 -0.0009224   0.00073057  0.00096586\n",
      "  0.00093168  0.00204807  0.         -0.00092982  0.          0.00093168\n",
      "  0.00096586  0.00179314  0.          0.          0.00126736  0.\n",
      "  0.00096201 -0.00074684  0.00123233 -0.00094483  0.00123727 -0.00079134\n",
      "  0.          0.          0.00073057  0.00093542  0.         -0.00074088\n",
      " -0.00076194 -0.00074983  0.          0.          0.00072765  0.\n",
      " -0.00094105  0.         -0.00095624  0.00072765 -0.00092982  0.00164856\n",
      "  0.00094672 -0.00095624  0.00072765 -0.00076194 -0.00061003  0.00203565\n",
      "  0.          0.         -0.00074983 -0.00091871  0.00205224  0.00094672\n",
      " -0.00074983 -0.00077424 -0.0030704  -0.00096393 -0.00138778 -0.0009224\n",
      "  0.         -0.00077424  0.00073057 -0.00123374  0.          0.\n",
      "  0.00140904  0.          0.         -0.00125118  0.00137457  0.00093542\n",
      "  0.         -0.00076806  0.          0.00127755  0.00075435  0.\n",
      " -0.00151007 -0.00096393  0.         -0.00049698  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.00093542 -0.00094483  0.         -0.00126482  0.         -0.00061003\n",
      " -0.00092982  0.          0.00241488  0.          0.00125223  0.00094672\n",
      "  0.00075435 -0.00094105  0.         -0.00074983 -0.00074088 -0.00091139\n",
      "  0.00137199 -0.00074684  0.          0.          0.001904    0.00075435\n",
      "  0.          0.          0.00123233  0.00151757 -0.00198273 -0.00077424\n",
      " -0.00095624 -0.00152075  0.00127244  0.         -0.00061003  0.\n",
      "  0.00206035  0.00093168 -0.00123374  0.          0.00078976 -0.00126482\n",
      "  0.00123727  0.00091688  0.00093542 -0.00061003], Bias: 0.0\n",
      "Iteration 100: Weights: [-0.01478186  0.          0.         -0.02461521  0.          0.\n",
      " -0.01201641  0.01554053 -0.0203507  -0.01484117  0.01175468  0.01554053\n",
      "  0.01499048  0.03295295  0.         -0.0149605   0.          0.01499048\n",
      "  0.01554053  0.02885121  0.          0.          0.02039148  0.\n",
      "  0.01547843 -0.01201641  0.01982788 -0.01520203  0.01990743 -0.01273244\n",
      "  0.          0.          0.01175468  0.01505062  0.         -0.01192057\n",
      " -0.01225941 -0.01206462  0.          0.          0.01170771  0.\n",
      " -0.01514128  0.         -0.01538574  0.01170771 -0.0149605   0.02652491\n",
      "  0.0152325  -0.01538574  0.01170771 -0.01225941 -0.0098152   0.03275307\n",
      "  0.          0.         -0.01206462 -0.01478186  0.03302011  0.0152325\n",
      " -0.01206462 -0.01245733 -0.04940201 -0.01550944 -0.02232908 -0.01484117\n",
      "  0.         -0.01245733  0.01175468 -0.0198505   0.          0.\n",
      "  0.02267107  0.          0.         -0.02013125  0.0221165   0.01505062\n",
      "  0.         -0.01235797  0.          0.02055543  0.0121373   0.\n",
      " -0.02429667 -0.01550944  0.         -0.00799631  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01505062 -0.01520203  0.         -0.0203507   0.         -0.0098152\n",
      " -0.0149605   0.          0.03885478  0.          0.020148    0.0152325\n",
      "  0.0121373  -0.01514128  0.         -0.01206462 -0.01192057 -0.01466396\n",
      "  0.02207496 -0.01201641  0.          0.          0.03063486  0.0121373\n",
      "  0.          0.          0.01982788  0.02441727 -0.03190159 -0.01245733\n",
      " -0.01538574 -0.02446855  0.02047329  0.         -0.0098152   0.\n",
      "  0.03315047  0.01499048 -0.0198505   0.          0.01270698 -0.0203507\n",
      "  0.01990743  0.0147523   0.01505062 -0.0098152 ], Bias: 0.0\n",
      "Iteration 200: Weights: [-0.01480475  0.          0.         -0.02465333  0.          0.\n",
      " -0.01203502  0.01556459 -0.02038221 -0.01486415  0.01177288  0.01556459\n",
      "  0.01501369  0.03300397  0.         -0.01498366  0.          0.01501369\n",
      "  0.01556459  0.02889588  0.          0.          0.02042305  0.\n",
      "  0.01550239 -0.01203502  0.01985858 -0.01522557  0.01993826 -0.01275216\n",
      "  0.          0.          0.01177288  0.01507392  0.         -0.01193903\n",
      " -0.01227839 -0.0120833   0.          0.          0.01172584  0.\n",
      " -0.01516473  0.         -0.01540956  0.01172584 -0.01498366  0.02656598\n",
      "  0.01525608 -0.01540956  0.01172584 -0.01227839 -0.00983039  0.03280378\n",
      "  0.          0.         -0.0120833  -0.01480475  0.03307124  0.01525608\n",
      " -0.0120833  -0.01247662 -0.0494785  -0.01553346 -0.02236365 -0.01486415\n",
      "  0.         -0.01247662  0.01177288 -0.01988123  0.          0.\n",
      "  0.02270617  0.          0.         -0.02016242  0.02215075  0.01507392\n",
      "  0.         -0.01237711  0.          0.02058726  0.0121561   0.\n",
      " -0.02433429 -0.01553346  0.         -0.00800869  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01507392 -0.01522557  0.         -0.02038221  0.         -0.00983039\n",
      " -0.01498366  0.          0.03891494  0.          0.0201792   0.01525608\n",
      "  0.0121561  -0.01516473  0.         -0.0120833  -0.01193903 -0.01468667\n",
      "  0.02210914 -0.01203502  0.          0.          0.03068229  0.0121561\n",
      "  0.          0.          0.01985858  0.02445507 -0.03195098 -0.01247662\n",
      " -0.01540956 -0.02450644  0.02050499  0.         -0.00983039  0.\n",
      "  0.03320179  0.01501369 -0.01988123  0.          0.01272665 -0.02038221\n",
      "  0.01993826  0.01477514  0.01507392 -0.00983039], Bias: 0.0\n",
      "Iteration 300: Weights: [-0.01480479  0.          0.         -0.02465339  0.          0.\n",
      " -0.01203505  0.01556463 -0.02038226 -0.01486419  0.01177291  0.01556463\n",
      "  0.01501373  0.03300406  0.         -0.0149837   0.          0.01501373\n",
      "  0.01556463  0.02889595  0.          0.          0.02042311  0.\n",
      "  0.01550243 -0.01203505  0.01985863 -0.01522561  0.01993831 -0.01275219\n",
      "  0.          0.          0.01177291  0.01507396  0.         -0.01193906\n",
      " -0.01227842 -0.01208334  0.          0.          0.01172587  0.\n",
      " -0.01516477  0.         -0.0154096   0.01172587 -0.0149837   0.02656605\n",
      "  0.01525612 -0.0154096   0.01172587 -0.01227842 -0.00983042  0.03280386\n",
      "  0.          0.         -0.01208334 -0.01480479  0.03307132  0.01525612\n",
      " -0.01208334 -0.01247665 -0.04947862 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.         -0.01247665  0.01177291 -0.01988128  0.          0.\n",
      "  0.02270623  0.          0.         -0.02016247  0.0221508   0.01507396\n",
      "  0.         -0.01237714  0.          0.02058731  0.01215613  0.\n",
      " -0.02433435 -0.0155335   0.         -0.00800871  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01507396 -0.01522561  0.         -0.02038226  0.         -0.00983042\n",
      " -0.0149837   0.          0.03891504  0.          0.02017925  0.01525612\n",
      "  0.01215613 -0.01516477  0.         -0.01208334 -0.01193906 -0.01468671\n",
      "  0.0221092  -0.01203505  0.          0.          0.03068237  0.01215613\n",
      "  0.          0.          0.01985863  0.02445514 -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.02050504  0.         -0.00983042  0.\n",
      "  0.03320188  0.01501373 -0.01988128  0.          0.01272669 -0.02038226\n",
      "  0.01993831  0.01477518  0.01507396 -0.00983042], Bias: 0.0\n",
      "Iteration 400: Weights: [-0.01480479  0.          0.         -0.02465339  0.          0.\n",
      " -0.01203505  0.01556463 -0.02038226 -0.01486419  0.01177291  0.01556463\n",
      "  0.01501373  0.03300406  0.         -0.0149837   0.          0.01501373\n",
      "  0.01556463  0.02889595  0.          0.          0.02042311  0.\n",
      "  0.01550243 -0.01203505  0.01985863 -0.01522561  0.01993831 -0.01275219\n",
      "  0.          0.          0.01177291  0.01507396  0.         -0.01193906\n",
      " -0.01227842 -0.01208334  0.          0.          0.01172587  0.\n",
      " -0.01516477  0.         -0.0154096   0.01172587 -0.0149837   0.02656605\n",
      "  0.01525612 -0.0154096   0.01172587 -0.01227842 -0.00983042  0.03280387\n",
      "  0.          0.         -0.01208334 -0.01480479  0.03307132  0.01525612\n",
      " -0.01208334 -0.01247665 -0.04947862 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.         -0.01247665  0.01177291 -0.01988128  0.          0.\n",
      "  0.02270623  0.          0.         -0.02016247  0.0221508   0.01507396\n",
      "  0.         -0.01237714  0.          0.02058731  0.01215613  0.\n",
      " -0.02433435 -0.0155335   0.         -0.00800871  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01507396 -0.01522561  0.         -0.02038226  0.         -0.00983042\n",
      " -0.0149837   0.          0.03891504  0.          0.02017925  0.01525612\n",
      "  0.01215613 -0.01516477  0.         -0.01208334 -0.01193906 -0.01468671\n",
      "  0.0221092  -0.01203505  0.          0.          0.03068237  0.01215613\n",
      "  0.          0.          0.01985863  0.02445514 -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.02050504  0.         -0.00983042  0.\n",
      "  0.03320188  0.01501373 -0.01988128  0.          0.01272669 -0.02038226\n",
      "  0.01993831  0.01477518  0.01507396 -0.00983042], Bias: 0.0\n",
      "Iteration 500: Weights: [-0.01480479  0.          0.         -0.02465339  0.          0.\n",
      " -0.01203505  0.01556463 -0.02038226 -0.01486419  0.01177291  0.01556463\n",
      "  0.01501373  0.03300406  0.         -0.0149837   0.          0.01501373\n",
      "  0.01556463  0.02889595  0.          0.          0.02042311  0.\n",
      "  0.01550243 -0.01203505  0.01985863 -0.01522561  0.01993831 -0.01275219\n",
      "  0.          0.          0.01177291  0.01507396  0.         -0.01193906\n",
      " -0.01227842 -0.01208334  0.          0.          0.01172587  0.\n",
      " -0.01516477  0.         -0.0154096   0.01172587 -0.0149837   0.02656605\n",
      "  0.01525612 -0.0154096   0.01172587 -0.01227842 -0.00983042  0.03280387\n",
      "  0.          0.         -0.01208334 -0.01480479  0.03307132  0.01525612\n",
      " -0.01208334 -0.01247665 -0.04947862 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.         -0.01247665  0.01177291 -0.01988128  0.          0.\n",
      "  0.02270623  0.          0.         -0.02016247  0.0221508   0.01507396\n",
      "  0.         -0.01237714  0.          0.02058731  0.01215613  0.\n",
      " -0.02433435 -0.0155335   0.         -0.00800871  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01507396 -0.01522561  0.         -0.02038226  0.         -0.00983042\n",
      " -0.0149837   0.          0.03891504  0.          0.02017925  0.01525612\n",
      "  0.01215613 -0.01516477  0.         -0.01208334 -0.01193906 -0.01468671\n",
      "  0.0221092  -0.01203505  0.          0.          0.03068237  0.01215613\n",
      "  0.          0.          0.01985863  0.02445514 -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.02050504  0.         -0.00983042  0.\n",
      "  0.03320188  0.01501373 -0.01988128  0.          0.01272669 -0.02038226\n",
      "  0.01993831  0.01477518  0.01507396 -0.00983042], Bias: 0.0\n",
      "Iteration 600: Weights: [-0.01480479  0.          0.         -0.02465339  0.          0.\n",
      " -0.01203505  0.01556463 -0.02038226 -0.01486419  0.01177291  0.01556463\n",
      "  0.01501373  0.03300406  0.         -0.0149837   0.          0.01501373\n",
      "  0.01556463  0.02889595  0.          0.          0.02042311  0.\n",
      "  0.01550243 -0.01203505  0.01985863 -0.01522561  0.01993831 -0.01275219\n",
      "  0.          0.          0.01177291  0.01507396  0.         -0.01193906\n",
      " -0.01227842 -0.01208334  0.          0.          0.01172587  0.\n",
      " -0.01516477  0.         -0.0154096   0.01172587 -0.0149837   0.02656605\n",
      "  0.01525612 -0.0154096   0.01172587 -0.01227842 -0.00983042  0.03280387\n",
      "  0.          0.         -0.01208334 -0.01480479  0.03307132  0.01525612\n",
      " -0.01208334 -0.01247665 -0.04947862 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.         -0.01247665  0.01177291 -0.01988128  0.          0.\n",
      "  0.02270623  0.          0.         -0.02016247  0.0221508   0.01507396\n",
      "  0.         -0.01237714  0.          0.02058731  0.01215613  0.\n",
      " -0.02433435 -0.0155335   0.         -0.00800871  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01507396 -0.01522561  0.         -0.02038226  0.         -0.00983042\n",
      " -0.0149837   0.          0.03891504  0.          0.02017925  0.01525612\n",
      "  0.01215613 -0.01516477  0.         -0.01208334 -0.01193906 -0.01468671\n",
      "  0.0221092  -0.01203505  0.          0.          0.03068237  0.01215613\n",
      "  0.          0.          0.01985863  0.02445514 -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.02050504  0.         -0.00983042  0.\n",
      "  0.03320188  0.01501373 -0.01988128  0.          0.01272669 -0.02038226\n",
      "  0.01993831  0.01477518  0.01507396 -0.00983042], Bias: 0.0\n",
      "Iteration 700: Weights: [-0.01480479  0.          0.         -0.02465339  0.          0.\n",
      " -0.01203505  0.01556463 -0.02038226 -0.01486419  0.01177291  0.01556463\n",
      "  0.01501373  0.03300406  0.         -0.0149837   0.          0.01501373\n",
      "  0.01556463  0.02889595  0.          0.          0.02042311  0.\n",
      "  0.01550243 -0.01203505  0.01985863 -0.01522561  0.01993831 -0.01275219\n",
      "  0.          0.          0.01177291  0.01507396  0.         -0.01193906\n",
      " -0.01227842 -0.01208334  0.          0.          0.01172587  0.\n",
      " -0.01516477  0.         -0.0154096   0.01172587 -0.0149837   0.02656605\n",
      "  0.01525612 -0.0154096   0.01172587 -0.01227842 -0.00983042  0.03280387\n",
      "  0.          0.         -0.01208334 -0.01480479  0.03307132  0.01525612\n",
      " -0.01208334 -0.01247665 -0.04947862 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.         -0.01247665  0.01177291 -0.01988128  0.          0.\n",
      "  0.02270623  0.          0.         -0.02016247  0.0221508   0.01507396\n",
      "  0.         -0.01237714  0.          0.02058731  0.01215613  0.\n",
      " -0.02433435 -0.0155335   0.         -0.00800871  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01507396 -0.01522561  0.         -0.02038226  0.         -0.00983042\n",
      " -0.0149837   0.          0.03891504  0.          0.02017925  0.01525612\n",
      "  0.01215613 -0.01516477  0.         -0.01208334 -0.01193906 -0.01468671\n",
      "  0.0221092  -0.01203505  0.          0.          0.03068237  0.01215613\n",
      "  0.          0.          0.01985863  0.02445514 -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.02050504  0.         -0.00983042  0.\n",
      "  0.03320188  0.01501373 -0.01988128  0.          0.01272669 -0.02038226\n",
      "  0.01993831  0.01477518  0.01507396 -0.00983042], Bias: 0.0\n",
      "Iteration 800: Weights: [-0.01480479  0.          0.         -0.02465339  0.          0.\n",
      " -0.01203505  0.01556463 -0.02038226 -0.01486419  0.01177291  0.01556463\n",
      "  0.01501373  0.03300406  0.         -0.0149837   0.          0.01501373\n",
      "  0.01556463  0.02889595  0.          0.          0.02042311  0.\n",
      "  0.01550243 -0.01203505  0.01985863 -0.01522561  0.01993831 -0.01275219\n",
      "  0.          0.          0.01177291  0.01507396  0.         -0.01193906\n",
      " -0.01227842 -0.01208334  0.          0.          0.01172587  0.\n",
      " -0.01516477  0.         -0.0154096   0.01172587 -0.0149837   0.02656605\n",
      "  0.01525612 -0.0154096   0.01172587 -0.01227842 -0.00983042  0.03280387\n",
      "  0.          0.         -0.01208334 -0.01480479  0.03307132  0.01525612\n",
      " -0.01208334 -0.01247665 -0.04947862 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.         -0.01247665  0.01177291 -0.01988128  0.          0.\n",
      "  0.02270623  0.          0.         -0.02016247  0.0221508   0.01507396\n",
      "  0.         -0.01237714  0.          0.02058731  0.01215613  0.\n",
      " -0.02433435 -0.0155335   0.         -0.00800871  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01507396 -0.01522561  0.         -0.02038226  0.         -0.00983042\n",
      " -0.0149837   0.          0.03891504  0.          0.02017925  0.01525612\n",
      "  0.01215613 -0.01516477  0.         -0.01208334 -0.01193906 -0.01468671\n",
      "  0.0221092  -0.01203505  0.          0.          0.03068237  0.01215613\n",
      "  0.          0.          0.01985863  0.02445514 -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.02050504  0.         -0.00983042  0.\n",
      "  0.03320188  0.01501373 -0.01988128  0.          0.01272669 -0.02038226\n",
      "  0.01993831  0.01477518  0.01507396 -0.00983042], Bias: 0.0\n",
      "Iteration 900: Weights: [-0.01480479  0.          0.         -0.02465339  0.          0.\n",
      " -0.01203505  0.01556463 -0.02038226 -0.01486419  0.01177291  0.01556463\n",
      "  0.01501373  0.03300406  0.         -0.0149837   0.          0.01501373\n",
      "  0.01556463  0.02889595  0.          0.          0.02042311  0.\n",
      "  0.01550243 -0.01203505  0.01985863 -0.01522561  0.01993831 -0.01275219\n",
      "  0.          0.          0.01177291  0.01507396  0.         -0.01193906\n",
      " -0.01227842 -0.01208334  0.          0.          0.01172587  0.\n",
      " -0.01516477  0.         -0.0154096   0.01172587 -0.0149837   0.02656605\n",
      "  0.01525612 -0.0154096   0.01172587 -0.01227842 -0.00983042  0.03280387\n",
      "  0.          0.         -0.01208334 -0.01480479  0.03307132  0.01525612\n",
      " -0.01208334 -0.01247665 -0.04947862 -0.0155335  -0.02236371 -0.01486419\n",
      "  0.         -0.01247665  0.01177291 -0.01988128  0.          0.\n",
      "  0.02270623  0.          0.         -0.02016247  0.0221508   0.01507396\n",
      "  0.         -0.01237714  0.          0.02058731  0.01215613  0.\n",
      " -0.02433435 -0.0155335   0.         -0.00800871  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01507396 -0.01522561  0.         -0.02038226  0.         -0.00983042\n",
      " -0.0149837   0.          0.03891504  0.          0.02017925  0.01525612\n",
      "  0.01215613 -0.01516477  0.         -0.01208334 -0.01193906 -0.01468671\n",
      "  0.0221092  -0.01203505  0.          0.          0.03068237  0.01215613\n",
      "  0.          0.          0.01985863  0.02445514 -0.03195106 -0.01247665\n",
      " -0.0154096  -0.0245065   0.02050504  0.         -0.00983042  0.\n",
      "  0.03320188  0.01501373 -0.01988128  0.          0.01272669 -0.02038226\n",
      "  0.01993831  0.01477518  0.01507396 -0.00983042], Bias: 0.0\n",
      "Classifier (1, 2) predictions: [ 1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1.\n",
      "  1. -1.  1.  1. -1.  0.  1. -1. -1.  1. -1. -1.  1. -1.  0.  1. -1. -1.\n",
      "  1. -1. -1.  1. -1.  1.  1. -1.  1.  1. -1.  1.]\n",
      "Classifier (1, 3) predictions: [ 1. -1. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  1. -1.  1.  1. -1.  1.  0. -1.]\n",
      "Classifier (2, 3) predictions: [-1.  1. -1.  0.  1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  0.  1. -1.\n",
      " -1.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1. -1.  1. -1. -1.  1. -1.\n",
      "  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.]\n",
      "Votes: [[2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 1. 0.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 1. 0.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]]\n",
      "Final predictions: [0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0\n",
      " 1 2 0 1 2 0 1 2 0 1 2]\n",
      "Predictions: [0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0\n",
      " 1 2 0 1 2 0 1 2 0 1 2]\n",
      "Actual Labels: [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1\n",
      " 2 3 1 2 3 1 2 3 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function to compute the TF-IDF (simplified)\n",
    "def compute_tfidf(corpus):\n",
    "    from collections import Counter\n",
    "    from math import log\n",
    "\n",
    "    vocabulary = list(set(word for document in corpus for word in document.split()))\n",
    "    vocabulary.sort()\n",
    "    tfidf_matrix = np.zeros((len(corpus), len(vocabulary)))\n",
    "\n",
    "    for i, document in enumerate(corpus):\n",
    "        word_counts = Counter(document.split())\n",
    "        for word, count in word_counts.items():\n",
    "            tf = count / len(document.split())\n",
    "            idf = log(len(corpus) / sum(1 for doc in corpus if word in doc.split()))\n",
    "            tfidf_matrix[i, vocabulary.index(word)] = tf * idf\n",
    "\n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Binary SVM Classifier\n",
    "class BinarySVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for iteration in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights - np.dot(x_i, y[idx]))\n",
    "                    self.bias -= self.learning_rate * y[idx]\n",
    "\n",
    "            # Debugging: print weights and bias at every 100 iterations\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}: Weights: {self.weights}, Bias: {self.bias}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) - self.bias)\n",
    "\n",
    "# One-vs-One SVM Classifier\n",
    "class OneVsOneSVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        n_classes = len(unique_classes)\n",
    "        print(f\"Unique classes: {unique_classes}\")  # Debugging line\n",
    "        for i in range(n_classes):\n",
    "            for j in range(i + 1, n_classes):\n",
    "                class_i = unique_classes[i]\n",
    "                class_j = unique_classes[j]\n",
    "\n",
    "                # Filter the data for the two classes\n",
    "                idx = np.where((y == class_i) | (y == class_j))\n",
    "                X_filtered = X[idx]\n",
    "                y_filtered = y[idx]\n",
    "\n",
    "                # Convert class labels to +1 and -1\n",
    "                y_filtered = np.where(y_filtered == class_i, 1, -1)\n",
    "\n",
    "                # Train the binary classifier\n",
    "                clf = BinarySVM(C=self.C, learning_rate=self.learning_rate, n_iters=self.n_iters)\n",
    "                clf.fit(X_filtered, y_filtered)\n",
    "                self.classifiers.append((clf, (class_i, class_j)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = len(np.unique(y))\n",
    "        votes = np.zeros((X.shape[0], n_classes))\n",
    "\n",
    "        for clf, class_labels in self.classifiers:\n",
    "            predictions = clf.predict(X)\n",
    "            print(f\"Classifier {class_labels} predictions: {predictions}\")  # Debugging line\n",
    "            for idx, pred in enumerate(predictions):\n",
    "                if pred == 1:\n",
    "                    votes[idx, class_labels[0] - 1] += 1  # Adjust index for zero-based array\n",
    "                else:\n",
    "                    votes[idx, class_labels[1] - 1] += 1  # Adjust index for zero-based array\n",
    "\n",
    "        # Return the class with the most votes\n",
    "        final_predictions = np.argmax(votes, axis=1)\n",
    "        print(f\"Votes: {votes}\")  # Debugging line\n",
    "        print(f\"Final predictions: {final_predictions}\")  # Debugging line\n",
    "        return final_predictions\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Sample dataset\n",
    "news_articles = [\n",
    "    \"government passes new law\",                     # Politics\n",
    "    \"football match ends in draw\",                    # Sports\n",
    "    \"new technology in smartphones\",                  # Technology\n",
    "    \"politician gives a speech\",                      # Politics\n",
    "    \"sports event attracts large crowd\",              # Sports\n",
    "    \"new tech gadgets released this year\",            # Technology\n",
    "    \"election results announced\",                     # Politics\n",
    "    \"soccer team wins championship\",                  # Sports\n",
    "    \"AI advancements in healthcare\",                  # Technology\n",
    "    \"local council meeting updates\",                  # Politics\n",
    "    \"basketball game highlights\",                     # Sports\n",
    "    \"innovations in artificial intelligence\",         # Technology\n",
    "    \"politician's new policy proposal\",               # Politics\n",
    "    \"swimming competition results\",                   # Sports\n",
    "    \"latest trends in smartphone design\",             # Technology\n",
    "    \"government budget allocation review\",            # Politics\n",
    "    \"volleyball tournament concludes\",                # Sports\n",
    "    \"breakthrough in renewable energy\",               # Technology\n",
    "    \"senator's speech on climate change\",             # Politics\n",
    "    \"baseball team training camp\",                    # Sports\n",
    "    \"tech company announces new software\",            # Technology\n",
    "    \"international relations summit\",                 # Politics\n",
    "    \"world cup qualifying matches\",                   # Sports\n",
    "    \"smart home devices market growth\",               # Technology\n",
    "    \"legislative bill discussion\",                    # Politics\n",
    "    \"rugby game results\",                            # Sports\n",
    "    \"technology in education sector\",                # Technology\n",
    "    \"mayoral election debate\",                        # Politics\n",
    "    \"national soccer league season start\",            # Sports\n",
    "    \"advancements in quantum computing\",              # Technology\n",
    "    \"press conference on new laws\",                   # Politics\n",
    "    \"hockey match final scores\",                      # Sports\n",
    "    \"virtual reality applications\",                   # Technology\n",
    "    \"parliamentary debate on economy\",                # Politics\n",
    "    \"college basketball championship\",                # Sports\n",
    "    \"latest trends in gadget development\",            # Technology\n",
    "    \"congressional committee meeting\",                # Politics\n",
    "    \"tennis tournament highlights\",                   # Sports\n",
    "    \"emerging technologies in fintech\",               # Technology\n",
    "    \"state of the union address\",                     # Politics\n",
    "    \"motorsports event results\",                      # Sports\n",
    "    \"new innovations in medical tech\",                # Technology\n",
    "    \"political rally speeches\",                       # Politics\n",
    "    \"community sports league updates\",                # Sports\n",
    "    \"tech industry conference news\",                  # Technology\n",
    "    \"government response to natural disaster\",        # Politics\n",
    "    \"annual sports awards ceremony\",                  # Sports\n",
    "    \"technological impacts on job market\"             # Technology\n",
    "]\n",
    "\n",
    "\n",
    "labels = [\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "     2, # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,   # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,   # Technology\n",
    "]\n",
    "\n",
    "# Convert news articles to TF-IDF features\n",
    "X, vocabulary = compute_tfidf(news_articles)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize and train the One-vs-One SVM\n",
    "ovo_svm = OneVsOneSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "ovo_svm.fit(X, y)\n",
    "\n",
    "# Test the classifier on the training data\n",
    "predictions = ovo_svm.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "for i in range(len(predictions)):\n",
    "    predictions[i]+=1\n",
    "print(\"Actual Labels:\", y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (35,12) and (142,) not aligned: 12 (dim 1) != 142 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m New_Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnational soccer league season start\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m X_new, vocabulary \u001b[38;5;241m=\u001b[39m compute_tfidf(New_Text)\n\u001b[1;32m----> 3\u001b[0m \u001b[43movo_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 87\u001b[0m, in \u001b[0;36mOneVsOneSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     84\u001b[0m votes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_classes))\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf, class_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifiers:\n\u001b[1;32m---> 87\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging line\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predictions):\n",
      "Cell \u001b[1;32mIn[28], line 50\u001b[0m, in \u001b[0;36mBinarySVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msign(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (35,12) and (142,) not aligned: 12 (dim 1) != 142 (dim 0)"
     ]
    }
   ],
   "source": [
    "New_Text = 'national soccer league season start'\n",
    "X_new, vocabulary = compute_tfidf(New_Text)\n",
    "ovo_svm.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [1 2 3]\n",
      "Iteration 0: Weights: [ 0.00128782  0.          0.          0.00093542  0.00101168  0.\n",
      "  0.          0.         -0.00073792  0.         -0.00094105 -0.00206867\n",
      "  0.00126229  0.          0.00093542 -0.00094105  0.         -0.00181118\n",
      "  0.00093917  0.00093917 -0.00128011  0.00128267  0.          0.\n",
      " -0.00124473  0.         -0.00124972  0.00078346  0.00128267  0.00092796\n",
      " -0.00073792 -0.00094483  0.00208923  0.          0.          0.\n",
      "  0.          0.00126229 -0.00091871  0.00127755  0.          0.00205211\n",
      "  0.         -0.00091871  0.         -0.00166514 -0.00095624  0.\n",
      " -0.00091871  0.          0.         -0.00205613  0.00122741  0.00132663\n",
      "  0.          0.         -0.00207289 -0.00095624  0.          0.\n",
      "  0.          0.          0.          0.00125725  0.         -0.00073792\n",
      "  0.          0.00091688  0.00095433 -0.00062551  0.00126229  0.00092796\n",
      "  0.         -0.00201955  0.00126736  0.          0.00181481 -0.0012904\n",
      " -0.00076194  0.          0.00139398  0.          0.00127755  0.00091688\n",
      "  0.00093168  0.          0.00177249  0.00095433  0.00093168 -0.00094483\n",
      "  0.          0.          0.          0.00125725  0.          0.\n",
      "  0.         -0.00164814  0.00093542 -0.00126482 -0.00095624 -0.00076194\n",
      "  0.          0.00093917  0.          0.          0.         -0.00138579\n",
      "  0.          0.00177864  0.         -0.0005285  -0.00076194  0.00128782\n",
      "  0.00125725 -0.00124473 -0.00153283  0.          0.          0.\n",
      "  0.         -0.00128524 -0.00208107 -0.00094105  0.          0.00128782\n",
      "  0.0007618   0.         -0.00124972 -0.0009261  -0.00094483  0.        ], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.02354157  0.          0.          0.01709957  0.01849366  0.\n",
      "  0.          0.         -0.01348929  0.         -0.01720258 -0.03781573\n",
      "  0.02307495  0.          0.01709957 -0.01720258  0.         -0.03310871\n",
      "  0.01716818  0.01716818 -0.0234006   0.02344749  0.          0.\n",
      " -0.02275383  0.         -0.02284512  0.01432173  0.02344749  0.01696319\n",
      " -0.01348929 -0.0172716   0.03819158  0.          0.          0.\n",
      "  0.          0.02307495 -0.01679423  0.0233538   0.          0.03751298\n",
      "  0.         -0.01679423  0.         -0.03043913 -0.01748032  0.\n",
      " -0.01679423  0.          0.         -0.03758635  0.02243718  0.02425099\n",
      "  0.          0.         -0.03789281 -0.01748032  0.          0.\n",
      "  0.          0.          0.          0.02298274  0.         -0.01348929\n",
      "  0.          0.01676064  0.01744536 -0.01143447  0.02307495  0.01696319\n",
      "  0.         -0.03691781  0.02316753  0.          0.03317506 -0.02358874\n",
      " -0.01392837  0.          0.02548217  0.          0.0233538   0.01676064\n",
      "  0.01703124  0.          0.03240152  0.01744536  0.01703124 -0.0172716\n",
      "  0.          0.          0.          0.02298274  0.          0.\n",
      "  0.         -0.03012837  0.01709957 -0.02312119 -0.01748032 -0.01392837\n",
      "  0.          0.01716818  0.          0.          0.         -0.0253325\n",
      "  0.          0.03251393  0.         -0.00966115 -0.01392837  0.02354157\n",
      "  0.02298274 -0.02275383 -0.02802046  0.          0.          0.\n",
      "  0.         -0.02349448 -0.03804239 -0.01720258  0.          0.02354157\n",
      "  0.01392589  0.         -0.02284512 -0.01692926 -0.0172716   0.        ], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.0236234   0.          0.          0.01715901  0.01855794  0.\n",
      "  0.          0.         -0.01353618  0.         -0.01726238 -0.03794718\n",
      "  0.02315516  0.          0.01715901 -0.01726238  0.         -0.03322379\n",
      "  0.01722785  0.01722785 -0.02348194  0.023529    0.          0.\n",
      " -0.02283292  0.         -0.02292453  0.01437151  0.023529    0.01702215\n",
      " -0.01353618 -0.01733164  0.03832433  0.          0.          0.\n",
      "  0.          0.02315516 -0.01685261  0.02343498  0.          0.03764338\n",
      "  0.         -0.01685261  0.         -0.03054493 -0.01754108  0.\n",
      " -0.01685261  0.          0.         -0.037717    0.02251517  0.02433529\n",
      "  0.          0.         -0.03802452 -0.01754108  0.          0.\n",
      "  0.          0.          0.          0.02306263  0.         -0.01353618\n",
      "  0.          0.0168189   0.017506   -0.01147421  0.02315516  0.01702215\n",
      "  0.         -0.03704614  0.02324806  0.          0.03329037 -0.02367074\n",
      " -0.01397679  0.          0.02557074  0.          0.02343498  0.0168189\n",
      "  0.01709044  0.          0.03251414  0.017506    0.01709044 -0.01733164\n",
      "  0.          0.          0.          0.02306263  0.          0.\n",
      "  0.         -0.0302331   0.01715901 -0.02320156 -0.01754108 -0.01397679\n",
      "  0.          0.01722785  0.          0.          0.         -0.02542056\n",
      "  0.          0.03262695  0.         -0.00969473 -0.01397679  0.0236234\n",
      "  0.02306263 -0.02283292 -0.02811786  0.          0.          0.\n",
      "  0.         -0.02357615 -0.03817463 -0.01726238  0.          0.0236234\n",
      "  0.0139743   0.         -0.02292453 -0.01698811 -0.01733164  0.        ], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.0236237   0.          0.          0.01715923  0.01855818  0.\n",
      "  0.          0.         -0.01353635  0.         -0.0172626  -0.03794766\n",
      "  0.02315545  0.          0.01715923 -0.0172626   0.         -0.03322422\n",
      "  0.01722807  0.01722807 -0.02348224  0.0235293   0.          0.\n",
      " -0.02283322  0.         -0.02292482  0.01437169  0.0235293   0.01702237\n",
      " -0.01353635 -0.01733186  0.03832482  0.          0.          0.\n",
      "  0.          0.02315545 -0.01685282  0.02343527  0.          0.03764386\n",
      "  0.         -0.01685282  0.         -0.03054532 -0.0175413   0.\n",
      " -0.01685282  0.          0.         -0.03771748  0.02251546  0.0243356\n",
      "  0.          0.         -0.03802501 -0.0175413   0.          0.\n",
      "  0.          0.          0.          0.02306292  0.         -0.01353635\n",
      "  0.          0.01681912  0.01750622 -0.01147436  0.02315545  0.01702237\n",
      "  0.         -0.03704661  0.02324835  0.          0.0332908  -0.02367104\n",
      " -0.01397697  0.          0.02557107  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066 -0.01733186\n",
      "  0.          0.          0.          0.02306292  0.          0.\n",
      "  0.         -0.03023348  0.01715923 -0.02320186 -0.0175413  -0.01397697\n",
      "  0.          0.01722807  0.          0.          0.         -0.02542088\n",
      "  0.          0.03262737  0.         -0.00969485 -0.01397697  0.0236237\n",
      "  0.02306292 -0.02283322 -0.02811822  0.          0.          0.\n",
      "  0.         -0.02357645 -0.03817512 -0.0172626   0.          0.0236237\n",
      "  0.01397447  0.         -0.02292482 -0.01698832 -0.01733186  0.        ], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.0236237   0.          0.          0.01715923  0.01855818  0.\n",
      "  0.          0.         -0.01353635  0.         -0.0172626  -0.03794767\n",
      "  0.02315545  0.          0.01715923 -0.0172626   0.         -0.03322422\n",
      "  0.01722807  0.01722807 -0.02348224  0.0235293   0.          0.\n",
      " -0.02283322  0.         -0.02292482  0.01437169  0.0235293   0.01702237\n",
      " -0.01353635 -0.01733186  0.03832482  0.          0.          0.\n",
      "  0.          0.02315545 -0.01685282  0.02343527  0.          0.03764386\n",
      "  0.         -0.01685282  0.         -0.03054532 -0.0175413   0.\n",
      " -0.01685282  0.          0.         -0.03771749  0.02251546  0.0243356\n",
      "  0.          0.         -0.03802501 -0.0175413   0.          0.\n",
      "  0.          0.          0.          0.02306292  0.         -0.01353635\n",
      "  0.          0.01681912  0.01750622 -0.01147436  0.02315545  0.01702237\n",
      "  0.         -0.03704661  0.02324835  0.          0.0332908  -0.02367104\n",
      " -0.01397697  0.          0.02557107  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066 -0.01733186\n",
      "  0.          0.          0.          0.02306292  0.          0.\n",
      "  0.         -0.03023348  0.01715923 -0.02320186 -0.0175413  -0.01397697\n",
      "  0.          0.01722807  0.          0.          0.         -0.02542089\n",
      "  0.          0.03262737  0.         -0.00969485 -0.01397697  0.0236237\n",
      "  0.02306292 -0.02283322 -0.02811822  0.          0.          0.\n",
      "  0.         -0.02357645 -0.03817512 -0.0172626   0.          0.0236237\n",
      "  0.01397448  0.         -0.02292482 -0.01698832 -0.01733186  0.        ], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.0236237   0.          0.          0.01715923  0.01855818  0.\n",
      "  0.          0.         -0.01353635  0.         -0.0172626  -0.03794767\n",
      "  0.02315545  0.          0.01715923 -0.0172626   0.         -0.03322422\n",
      "  0.01722807  0.01722807 -0.02348224  0.0235293   0.          0.\n",
      " -0.02283322  0.         -0.02292482  0.01437169  0.0235293   0.01702237\n",
      " -0.01353635 -0.01733186  0.03832482  0.          0.          0.\n",
      "  0.          0.02315545 -0.01685282  0.02343527  0.          0.03764386\n",
      "  0.         -0.01685282  0.         -0.03054532 -0.0175413   0.\n",
      " -0.01685282  0.          0.         -0.03771749  0.02251546  0.0243356\n",
      "  0.          0.         -0.03802501 -0.0175413   0.          0.\n",
      "  0.          0.          0.          0.02306292  0.         -0.01353635\n",
      "  0.          0.01681912  0.01750622 -0.01147436  0.02315545  0.01702237\n",
      "  0.         -0.03704661  0.02324835  0.          0.0332908  -0.02367104\n",
      " -0.01397697  0.          0.02557107  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066 -0.01733186\n",
      "  0.          0.          0.          0.02306292  0.          0.\n",
      "  0.         -0.03023348  0.01715923 -0.02320186 -0.0175413  -0.01397697\n",
      "  0.          0.01722807  0.          0.          0.         -0.02542089\n",
      "  0.          0.03262737  0.         -0.00969485 -0.01397697  0.0236237\n",
      "  0.02306292 -0.02283322 -0.02811822  0.          0.          0.\n",
      "  0.         -0.02357645 -0.03817512 -0.0172626   0.          0.0236237\n",
      "  0.01397448  0.         -0.02292482 -0.01698832 -0.01733186  0.        ], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.0236237   0.          0.          0.01715923  0.01855818  0.\n",
      "  0.          0.         -0.01353635  0.         -0.0172626  -0.03794767\n",
      "  0.02315545  0.          0.01715923 -0.0172626   0.         -0.03322422\n",
      "  0.01722807  0.01722807 -0.02348224  0.0235293   0.          0.\n",
      " -0.02283322  0.         -0.02292482  0.01437169  0.0235293   0.01702237\n",
      " -0.01353635 -0.01733186  0.03832482  0.          0.          0.\n",
      "  0.          0.02315545 -0.01685282  0.02343527  0.          0.03764386\n",
      "  0.         -0.01685282  0.         -0.03054532 -0.0175413   0.\n",
      " -0.01685282  0.          0.         -0.03771749  0.02251546  0.0243356\n",
      "  0.          0.         -0.03802501 -0.0175413   0.          0.\n",
      "  0.          0.          0.          0.02306292  0.         -0.01353635\n",
      "  0.          0.01681912  0.01750622 -0.01147436  0.02315545  0.01702237\n",
      "  0.         -0.03704661  0.02324835  0.          0.0332908  -0.02367104\n",
      " -0.01397697  0.          0.02557107  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066 -0.01733186\n",
      "  0.          0.          0.          0.02306292  0.          0.\n",
      "  0.         -0.03023348  0.01715923 -0.02320186 -0.0175413  -0.01397697\n",
      "  0.          0.01722807  0.          0.          0.         -0.02542089\n",
      "  0.          0.03262737  0.         -0.00969485 -0.01397697  0.0236237\n",
      "  0.02306292 -0.02283322 -0.02811822  0.          0.          0.\n",
      "  0.         -0.02357645 -0.03817512 -0.0172626   0.          0.0236237\n",
      "  0.01397448  0.         -0.02292482 -0.01698832 -0.01733186  0.        ], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.0236237   0.          0.          0.01715923  0.01855818  0.\n",
      "  0.          0.         -0.01353635  0.         -0.0172626  -0.03794767\n",
      "  0.02315545  0.          0.01715923 -0.0172626   0.         -0.03322422\n",
      "  0.01722807  0.01722807 -0.02348224  0.0235293   0.          0.\n",
      " -0.02283322  0.         -0.02292482  0.01437169  0.0235293   0.01702237\n",
      " -0.01353635 -0.01733186  0.03832482  0.          0.          0.\n",
      "  0.          0.02315545 -0.01685282  0.02343527  0.          0.03764386\n",
      "  0.         -0.01685282  0.         -0.03054532 -0.0175413   0.\n",
      " -0.01685282  0.          0.         -0.03771749  0.02251546  0.0243356\n",
      "  0.          0.         -0.03802501 -0.0175413   0.          0.\n",
      "  0.          0.          0.          0.02306292  0.         -0.01353635\n",
      "  0.          0.01681912  0.01750622 -0.01147436  0.02315545  0.01702237\n",
      "  0.         -0.03704661  0.02324835  0.          0.0332908  -0.02367104\n",
      " -0.01397697  0.          0.02557107  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066 -0.01733186\n",
      "  0.          0.          0.          0.02306292  0.          0.\n",
      "  0.         -0.03023348  0.01715923 -0.02320186 -0.0175413  -0.01397697\n",
      "  0.          0.01722807  0.          0.          0.         -0.02542089\n",
      "  0.          0.03262737  0.         -0.00969485 -0.01397697  0.0236237\n",
      "  0.02306292 -0.02283322 -0.02811822  0.          0.          0.\n",
      "  0.         -0.02357645 -0.03817512 -0.0172626   0.          0.0236237\n",
      "  0.01397448  0.         -0.02292482 -0.01698832 -0.01733186  0.        ], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.0236237   0.          0.          0.01715923  0.01855818  0.\n",
      "  0.          0.         -0.01353635  0.         -0.0172626  -0.03794767\n",
      "  0.02315545  0.          0.01715923 -0.0172626   0.         -0.03322422\n",
      "  0.01722807  0.01722807 -0.02348224  0.0235293   0.          0.\n",
      " -0.02283322  0.         -0.02292482  0.01437169  0.0235293   0.01702237\n",
      " -0.01353635 -0.01733186  0.03832482  0.          0.          0.\n",
      "  0.          0.02315545 -0.01685282  0.02343527  0.          0.03764386\n",
      "  0.         -0.01685282  0.         -0.03054532 -0.0175413   0.\n",
      " -0.01685282  0.          0.         -0.03771749  0.02251546  0.0243356\n",
      "  0.          0.         -0.03802501 -0.0175413   0.          0.\n",
      "  0.          0.          0.          0.02306292  0.         -0.01353635\n",
      "  0.          0.01681912  0.01750622 -0.01147436  0.02315545  0.01702237\n",
      "  0.         -0.03704661  0.02324835  0.          0.0332908  -0.02367104\n",
      " -0.01397697  0.          0.02557107  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066 -0.01733186\n",
      "  0.          0.          0.          0.02306292  0.          0.\n",
      "  0.         -0.03023348  0.01715923 -0.02320186 -0.0175413  -0.01397697\n",
      "  0.          0.01722807  0.          0.          0.         -0.02542089\n",
      "  0.          0.03262737  0.         -0.00969485 -0.01397697  0.0236237\n",
      "  0.02306292 -0.02283322 -0.02811822  0.          0.          0.\n",
      "  0.         -0.02357645 -0.03817512 -0.0172626   0.          0.0236237\n",
      "  0.01397448  0.         -0.02292482 -0.01698832 -0.01733186  0.        ], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.0236237   0.          0.          0.01715923  0.01855818  0.\n",
      "  0.          0.         -0.01353635  0.         -0.0172626  -0.03794767\n",
      "  0.02315545  0.          0.01715923 -0.0172626   0.         -0.03322422\n",
      "  0.01722807  0.01722807 -0.02348224  0.0235293   0.          0.\n",
      " -0.02283322  0.         -0.02292482  0.01437169  0.0235293   0.01702237\n",
      " -0.01353635 -0.01733186  0.03832482  0.          0.          0.\n",
      "  0.          0.02315545 -0.01685282  0.02343527  0.          0.03764386\n",
      "  0.         -0.01685282  0.         -0.03054532 -0.0175413   0.\n",
      " -0.01685282  0.          0.         -0.03771749  0.02251546  0.0243356\n",
      "  0.          0.         -0.03802501 -0.0175413   0.          0.\n",
      "  0.          0.          0.          0.02306292  0.         -0.01353635\n",
      "  0.          0.01681912  0.01750622 -0.01147436  0.02315545  0.01702237\n",
      "  0.         -0.03704661  0.02324835  0.          0.0332908  -0.02367104\n",
      " -0.01397697  0.          0.02557107  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066 -0.01733186\n",
      "  0.          0.          0.          0.02306292  0.          0.\n",
      "  0.         -0.03023348  0.01715923 -0.02320186 -0.0175413  -0.01397697\n",
      "  0.          0.01722807  0.          0.          0.         -0.02542089\n",
      "  0.          0.03262737  0.         -0.00969485 -0.01397697  0.0236237\n",
      "  0.02306292 -0.02283322 -0.02811822  0.          0.          0.\n",
      "  0.         -0.02357645 -0.03817512 -0.0172626   0.          0.0236237\n",
      "  0.01397448  0.         -0.02292482 -0.01698832 -0.01733186  0.        ], Bias: 0.0\n",
      "Iteration 0: Weights: [ 0.00128782 -0.00205623 -0.0012348   0.00093542  0.00039364  0.\n",
      " -0.00127499 -0.00123975  0.          0.          0.          0.\n",
      "  0.00126229 -0.00124972  0.00093542  0.          0.          0.\n",
      "  0.00093917  0.00093917  0.          0.00128267  0.         -0.00075284\n",
      "  0.         -0.0012699   0.          0.00078346  0.00128267  0.00092796\n",
      "  0.          0.          0.00208923 -0.00093355 -0.00096008 -0.00075586\n",
      "  0.          0.00126229  0.          0.00127755 -0.00126482  0.00205211\n",
      " -0.00128524  0.         -0.00124972  0.          0.         -0.00128524\n",
      "  0.         -0.00096008 -0.00073792  0.          0.00122741  0.00132663\n",
      " -0.00075586 -0.0012348   0.          0.         -0.00075586  0.\n",
      "  0.         -0.00181229 -0.00123975  0.00125725  0.          0.\n",
      " -0.00155457  0.00091688  0.00095433  0.          0.00126229  0.00092796\n",
      " -0.00062052  0.          0.00126736 -0.0009678   0.00181481  0.\n",
      "  0.          0.         -0.00043797  0.          0.00127755  0.00091688\n",
      "  0.00093168  0.          0.00177249  0.00095433  0.00093168  0.\n",
      " -0.0012699   0.         -0.00127499  0.00125725 -0.00073792 -0.00124972\n",
      "  0.          0.00079103  0.00093542  0.          0.          0.\n",
      " -0.00126482  0.00093917 -0.00075586 -0.00093355 -0.00122495  0.\n",
      " -0.00075284  0.00177864  0.          0.          0.          0.00128782\n",
      "  0.00125725  0.          0.         -0.00157814  0.         -0.00128524\n",
      " -0.00204397  0.          0.          0.         -0.00155457  0.00128782\n",
      "  0.0007618  -0.00127499  0.          0.          0.         -0.00073792], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.02354157 -0.03758816 -0.02257235  0.01709957  0.00719572  0.\n",
      " -0.02330709 -0.02266291  0.          0.          0.          0.\n",
      "  0.02307495 -0.02284512  0.01709957  0.          0.          0.\n",
      "  0.01716818  0.01716818  0.          0.02344749  0.         -0.01376207\n",
      "  0.         -0.02321395  0.          0.01432173  0.02344749  0.01696319\n",
      "  0.          0.          0.03819158 -0.01706537 -0.01755045 -0.01381728\n",
      "  0.          0.02307495  0.          0.0233538  -0.02312119  0.03751298\n",
      " -0.02349448  0.         -0.02284512  0.          0.         -0.02349448\n",
      "  0.         -0.01755045 -0.01348929  0.          0.02243718  0.02425099\n",
      " -0.01381728 -0.02257235  0.          0.         -0.01381728  0.\n",
      "  0.         -0.03312891 -0.02266291  0.02298274  0.          0.\n",
      " -0.02841778  0.01676064  0.01744536  0.          0.02307495  0.01696319\n",
      " -0.01134326  0.          0.02316753 -0.01769156  0.03317506  0.\n",
      "  0.          0.         -0.00800612  0.          0.0233538   0.01676064\n",
      "  0.01703124  0.          0.03240152  0.01744536  0.01703124  0.\n",
      " -0.02321395  0.         -0.02330709  0.02298274 -0.01348929 -0.02284512\n",
      "  0.          0.01446011  0.01709957  0.          0.          0.\n",
      " -0.02312119  0.01716818 -0.01381728 -0.01706537 -0.02239231  0.\n",
      " -0.01376207  0.03251393  0.          0.          0.          0.02354157\n",
      "  0.02298274  0.          0.         -0.02884865  0.         -0.02349448\n",
      " -0.03736421  0.          0.          0.         -0.02841778  0.02354157\n",
      "  0.01392589 -0.02330709  0.          0.          0.         -0.01348929], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.0236234  -0.03771882 -0.02265081  0.01715901  0.00722073  0.\n",
      " -0.02338811 -0.02274168  0.          0.          0.          0.\n",
      "  0.02315516 -0.02292453  0.01715901  0.          0.          0.\n",
      "  0.01722785  0.01722785  0.          0.023529    0.         -0.0138099\n",
      "  0.         -0.02329465  0.          0.01437151  0.023529    0.01702215\n",
      "  0.          0.          0.03832433 -0.01712469 -0.01761145 -0.01386531\n",
      "  0.          0.02315516  0.          0.02343498 -0.02320156  0.03764338\n",
      " -0.02357615  0.         -0.02292453  0.          0.         -0.02357615\n",
      "  0.         -0.01761145 -0.01353618  0.          0.02251517  0.02433529\n",
      " -0.01386531 -0.02265081  0.          0.         -0.01386531  0.\n",
      "  0.         -0.03324406 -0.02274168  0.02306263  0.          0.\n",
      " -0.02851656  0.0168189   0.017506    0.          0.02315516  0.01702215\n",
      " -0.01138269  0.          0.02324806 -0.01775305  0.03329037  0.\n",
      "  0.          0.         -0.00803395  0.          0.02343498  0.0168189\n",
      "  0.01709044  0.          0.03251414  0.017506    0.01709044  0.\n",
      " -0.02329465  0.         -0.02338811  0.02306263 -0.01353618 -0.02292453\n",
      "  0.          0.01451037  0.01715901  0.          0.          0.\n",
      " -0.02320156  0.01722785 -0.01386531 -0.01712469 -0.02247014  0.\n",
      " -0.0138099   0.03262695  0.          0.          0.          0.0236234\n",
      "  0.02306263  0.          0.         -0.02894893  0.         -0.02357615\n",
      " -0.03749408  0.          0.          0.         -0.02851656  0.0236234\n",
      "  0.0139743  -0.02338811  0.          0.          0.         -0.01353618], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.0236237  -0.0377193  -0.0226511   0.01715923  0.00722082  0.\n",
      " -0.0233884  -0.02274197  0.          0.          0.          0.\n",
      "  0.02315545 -0.02292482  0.01715923  0.          0.          0.\n",
      "  0.01722807  0.01722807  0.          0.0235293   0.         -0.01381008\n",
      "  0.         -0.02329494  0.          0.01437169  0.0235293   0.01702237\n",
      "  0.          0.          0.03832482 -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.02315545  0.          0.02343527 -0.02320186  0.03764386\n",
      " -0.02357645  0.         -0.02292482  0.          0.         -0.02357645\n",
      "  0.         -0.01761168 -0.01353635  0.          0.02251546  0.0243356\n",
      " -0.01386549 -0.0226511   0.          0.         -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.02306292  0.          0.\n",
      " -0.02851693  0.01681912  0.01750622  0.          0.02315545  0.01702237\n",
      " -0.01138284  0.          0.02324835 -0.01775328  0.0332908   0.\n",
      "  0.          0.         -0.00803405  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066  0.\n",
      " -0.02329494  0.         -0.0233884   0.02306292 -0.01353635 -0.02292482\n",
      "  0.          0.01451056  0.01715923  0.          0.          0.\n",
      " -0.02320186  0.01722807 -0.01386549 -0.01712491 -0.02247043  0.\n",
      " -0.01381008  0.03262737  0.          0.          0.          0.0236237\n",
      "  0.02306292  0.          0.         -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.          0.          0.         -0.02851693  0.0236237\n",
      "  0.01397447 -0.0233884   0.          0.          0.         -0.01353635], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.0236237  -0.0377193  -0.0226511   0.01715923  0.00722082  0.\n",
      " -0.0233884  -0.02274197  0.          0.          0.          0.\n",
      "  0.02315545 -0.02292482  0.01715923  0.          0.          0.\n",
      "  0.01722807  0.01722807  0.          0.0235293   0.         -0.01381008\n",
      "  0.         -0.02329494  0.          0.01437169  0.0235293   0.01702237\n",
      "  0.          0.          0.03832482 -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.02315545  0.          0.02343527 -0.02320186  0.03764386\n",
      " -0.02357645  0.         -0.02292482  0.          0.         -0.02357645\n",
      "  0.         -0.01761168 -0.01353635  0.          0.02251546  0.0243356\n",
      " -0.01386549 -0.0226511   0.          0.         -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.02306292  0.          0.\n",
      " -0.02851693  0.01681912  0.01750622  0.          0.02315545  0.01702237\n",
      " -0.01138284  0.          0.02324835 -0.01775328  0.0332908   0.\n",
      "  0.          0.         -0.00803405  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066  0.\n",
      " -0.02329494  0.         -0.0233884   0.02306292 -0.01353635 -0.02292482\n",
      "  0.          0.01451056  0.01715923  0.          0.          0.\n",
      " -0.02320186  0.01722807 -0.01386549 -0.01712491 -0.02247043  0.\n",
      " -0.01381008  0.03262737  0.          0.          0.          0.0236237\n",
      "  0.02306292  0.          0.         -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.          0.          0.         -0.02851693  0.0236237\n",
      "  0.01397448 -0.0233884   0.          0.          0.         -0.01353635], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.0236237  -0.0377193  -0.0226511   0.01715923  0.00722082  0.\n",
      " -0.0233884  -0.02274197  0.          0.          0.          0.\n",
      "  0.02315545 -0.02292482  0.01715923  0.          0.          0.\n",
      "  0.01722807  0.01722807  0.          0.0235293   0.         -0.01381008\n",
      "  0.         -0.02329494  0.          0.01437169  0.0235293   0.01702237\n",
      "  0.          0.          0.03832482 -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.02315545  0.          0.02343527 -0.02320186  0.03764386\n",
      " -0.02357645  0.         -0.02292482  0.          0.         -0.02357645\n",
      "  0.         -0.01761168 -0.01353635  0.          0.02251546  0.0243356\n",
      " -0.01386549 -0.0226511   0.          0.         -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.02306292  0.          0.\n",
      " -0.02851693  0.01681912  0.01750622  0.          0.02315545  0.01702237\n",
      " -0.01138284  0.          0.02324835 -0.01775328  0.0332908   0.\n",
      "  0.          0.         -0.00803405  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066  0.\n",
      " -0.02329494  0.         -0.0233884   0.02306292 -0.01353635 -0.02292482\n",
      "  0.          0.01451056  0.01715923  0.          0.          0.\n",
      " -0.02320186  0.01722807 -0.01386549 -0.01712491 -0.02247043  0.\n",
      " -0.01381008  0.03262737  0.          0.          0.          0.0236237\n",
      "  0.02306292  0.          0.         -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.          0.          0.         -0.02851693  0.0236237\n",
      "  0.01397448 -0.0233884   0.          0.          0.         -0.01353635], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.0236237  -0.0377193  -0.0226511   0.01715923  0.00722082  0.\n",
      " -0.0233884  -0.02274197  0.          0.          0.          0.\n",
      "  0.02315545 -0.02292482  0.01715923  0.          0.          0.\n",
      "  0.01722807  0.01722807  0.          0.0235293   0.         -0.01381008\n",
      "  0.         -0.02329494  0.          0.01437169  0.0235293   0.01702237\n",
      "  0.          0.          0.03832482 -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.02315545  0.          0.02343527 -0.02320186  0.03764386\n",
      " -0.02357645  0.         -0.02292482  0.          0.         -0.02357645\n",
      "  0.         -0.01761168 -0.01353635  0.          0.02251546  0.0243356\n",
      " -0.01386549 -0.0226511   0.          0.         -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.02306292  0.          0.\n",
      " -0.02851693  0.01681912  0.01750622  0.          0.02315545  0.01702237\n",
      " -0.01138284  0.          0.02324835 -0.01775328  0.0332908   0.\n",
      "  0.          0.         -0.00803405  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066  0.\n",
      " -0.02329494  0.         -0.0233884   0.02306292 -0.01353635 -0.02292482\n",
      "  0.          0.01451056  0.01715923  0.          0.          0.\n",
      " -0.02320186  0.01722807 -0.01386549 -0.01712491 -0.02247043  0.\n",
      " -0.01381008  0.03262737  0.          0.          0.          0.0236237\n",
      "  0.02306292  0.          0.         -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.          0.          0.         -0.02851693  0.0236237\n",
      "  0.01397448 -0.0233884   0.          0.          0.         -0.01353635], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.0236237  -0.0377193  -0.0226511   0.01715923  0.00722082  0.\n",
      " -0.0233884  -0.02274197  0.          0.          0.          0.\n",
      "  0.02315545 -0.02292482  0.01715923  0.          0.          0.\n",
      "  0.01722807  0.01722807  0.          0.0235293   0.         -0.01381008\n",
      "  0.         -0.02329494  0.          0.01437169  0.0235293   0.01702237\n",
      "  0.          0.          0.03832482 -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.02315545  0.          0.02343527 -0.02320186  0.03764386\n",
      " -0.02357645  0.         -0.02292482  0.          0.         -0.02357645\n",
      "  0.         -0.01761168 -0.01353635  0.          0.02251546  0.0243356\n",
      " -0.01386549 -0.0226511   0.          0.         -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.02306292  0.          0.\n",
      " -0.02851693  0.01681912  0.01750622  0.          0.02315545  0.01702237\n",
      " -0.01138284  0.          0.02324835 -0.01775328  0.0332908   0.\n",
      "  0.          0.         -0.00803405  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066  0.\n",
      " -0.02329494  0.         -0.0233884   0.02306292 -0.01353635 -0.02292482\n",
      "  0.          0.01451056  0.01715923  0.          0.          0.\n",
      " -0.02320186  0.01722807 -0.01386549 -0.01712491 -0.02247043  0.\n",
      " -0.01381008  0.03262737  0.          0.          0.          0.0236237\n",
      "  0.02306292  0.          0.         -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.          0.          0.         -0.02851693  0.0236237\n",
      "  0.01397448 -0.0233884   0.          0.          0.         -0.01353635], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.0236237  -0.0377193  -0.0226511   0.01715923  0.00722082  0.\n",
      " -0.0233884  -0.02274197  0.          0.          0.          0.\n",
      "  0.02315545 -0.02292482  0.01715923  0.          0.          0.\n",
      "  0.01722807  0.01722807  0.          0.0235293   0.         -0.01381008\n",
      "  0.         -0.02329494  0.          0.01437169  0.0235293   0.01702237\n",
      "  0.          0.          0.03832482 -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.02315545  0.          0.02343527 -0.02320186  0.03764386\n",
      " -0.02357645  0.         -0.02292482  0.          0.         -0.02357645\n",
      "  0.         -0.01761168 -0.01353635  0.          0.02251546  0.0243356\n",
      " -0.01386549 -0.0226511   0.          0.         -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.02306292  0.          0.\n",
      " -0.02851693  0.01681912  0.01750622  0.          0.02315545  0.01702237\n",
      " -0.01138284  0.          0.02324835 -0.01775328  0.0332908   0.\n",
      "  0.          0.         -0.00803405  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066  0.\n",
      " -0.02329494  0.         -0.0233884   0.02306292 -0.01353635 -0.02292482\n",
      "  0.          0.01451056  0.01715923  0.          0.          0.\n",
      " -0.02320186  0.01722807 -0.01386549 -0.01712491 -0.02247043  0.\n",
      " -0.01381008  0.03262737  0.          0.          0.          0.0236237\n",
      "  0.02306292  0.          0.         -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.          0.          0.         -0.02851693  0.0236237\n",
      "  0.01397448 -0.0233884   0.          0.          0.         -0.01353635], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.0236237  -0.0377193  -0.0226511   0.01715923  0.00722082  0.\n",
      " -0.0233884  -0.02274197  0.          0.          0.          0.\n",
      "  0.02315545 -0.02292482  0.01715923  0.          0.          0.\n",
      "  0.01722807  0.01722807  0.          0.0235293   0.         -0.01381008\n",
      "  0.         -0.02329494  0.          0.01437169  0.0235293   0.01702237\n",
      "  0.          0.          0.03832482 -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.02315545  0.          0.02343527 -0.02320186  0.03764386\n",
      " -0.02357645  0.         -0.02292482  0.          0.         -0.02357645\n",
      "  0.         -0.01761168 -0.01353635  0.          0.02251546  0.0243356\n",
      " -0.01386549 -0.0226511   0.          0.         -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.02306292  0.          0.\n",
      " -0.02851693  0.01681912  0.01750622  0.          0.02315545  0.01702237\n",
      " -0.01138284  0.          0.02324835 -0.01775328  0.0332908   0.\n",
      "  0.          0.         -0.00803405  0.          0.02343527  0.01681912\n",
      "  0.01709066  0.          0.03251456  0.01750622  0.01709066  0.\n",
      " -0.02329494  0.         -0.0233884   0.02306292 -0.01353635 -0.02292482\n",
      "  0.          0.01451056  0.01715923  0.          0.          0.\n",
      " -0.02320186  0.01722807 -0.01386549 -0.01712491 -0.02247043  0.\n",
      " -0.01381008  0.03262737  0.          0.          0.          0.0236237\n",
      "  0.02306292  0.          0.         -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.          0.          0.         -0.02851693  0.0236237\n",
      "  0.01397448 -0.0233884   0.          0.          0.         -0.01353635], Bias: 0.0\n",
      "Iteration 0: Weights: [ 0.         -0.00205623 -0.0012348   0.         -0.00061804  0.\n",
      " -0.00127499 -0.00123975  0.00073644  0.          0.00093917  0.00206454\n",
      "  0.         -0.00124972  0.          0.00093917  0.          0.00180756\n",
      "  0.          0.          0.00127755  0.          0.         -0.00075284\n",
      "  0.00124224 -0.0012699   0.00124722  0.          0.          0.\n",
      "  0.00073644  0.00094294  0.         -0.00093355 -0.00096008 -0.00075586\n",
      "  0.          0.          0.00091688  0.         -0.00126482  0.\n",
      " -0.00128524  0.00091688 -0.00124972  0.00166181  0.00095433 -0.00128524\n",
      "  0.00091688 -0.00096008 -0.00073792  0.00205201  0.          0.\n",
      " -0.00075586 -0.0012348   0.00206875  0.00095433 -0.00075586  0.\n",
      "  0.         -0.00181229 -0.00123975  0.          0.          0.00073644\n",
      " -0.00155457  0.          0.          0.00062426  0.          0.\n",
      " -0.00062052  0.00201552  0.         -0.0009678   0.          0.00128782\n",
      "  0.00076041  0.         -0.00183195  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.00094294\n",
      " -0.0012699   0.         -0.00127499  0.         -0.00073792 -0.00124972\n",
      "  0.          0.00243429  0.          0.00126229  0.00095433  0.00076041\n",
      " -0.00126482  0.         -0.00075586 -0.00093355 -0.00122495  0.00138302\n",
      " -0.00075284  0.          0.          0.00052745  0.00076041  0.\n",
      "  0.          0.00124224  0.00152977 -0.00157814  0.         -0.00128524\n",
      " -0.00204397  0.00128267  0.00207691  0.00093917 -0.00155457  0.\n",
      "  0.         -0.00127499  0.00124722  0.00092425  0.00094294 -0.00073792], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.         -0.03758816 -0.02257235  0.         -0.01129794  0.\n",
      " -0.02330709 -0.02266291  0.01346231  0.          0.01716818  0.0377401\n",
      "  0.         -0.02284512  0.          0.01716818  0.          0.03304249\n",
      "  0.          0.          0.0233538   0.          0.         -0.01376207\n",
      "  0.02270832 -0.02321395  0.02279943  0.          0.          0.\n",
      "  0.01346231  0.01723706  0.         -0.01706537 -0.01755045 -0.01381728\n",
      "  0.          0.          0.01676064  0.         -0.02312119  0.\n",
      " -0.02349448  0.01676064 -0.02284512  0.03037825  0.01744536 -0.02349448\n",
      "  0.01676064 -0.01755045 -0.01348929  0.03751118  0.          0.\n",
      " -0.01381728 -0.02257235  0.03781702  0.01744536 -0.01381728  0.\n",
      "  0.         -0.03312891 -0.02266291  0.          0.          0.01346231\n",
      " -0.02841778  0.          0.          0.0114116   0.          0.\n",
      " -0.01134326  0.03684398  0.         -0.01769156  0.          0.02354157\n",
      "  0.01390052  0.         -0.03348828  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01723706\n",
      " -0.02321395  0.         -0.02330709  0.         -0.01348929 -0.02284512\n",
      "  0.          0.0444993   0.          0.02307495  0.01744536  0.01390052\n",
      " -0.02312119  0.         -0.01381728 -0.01706537 -0.02239231  0.02528184\n",
      " -0.01376207  0.          0.          0.00964183  0.01390052  0.\n",
      "  0.          0.02270832  0.02796442 -0.02884865  0.         -0.02349448\n",
      " -0.03736421  0.02344749  0.03796631  0.01716818 -0.02841778  0.\n",
      "  0.         -0.02330709  0.02279943  0.0168954   0.01723706 -0.01348929], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.         -0.03771882 -0.02265081  0.         -0.01133721  0.\n",
      " -0.02338811 -0.02274168  0.0135091   0.          0.01722785  0.03787129\n",
      "  0.         -0.02292453  0.          0.01722785  0.          0.03315735\n",
      "  0.          0.          0.02343498  0.          0.         -0.0138099\n",
      "  0.02278726 -0.02329465  0.02287868  0.          0.          0.\n",
      "  0.0135091   0.01729697  0.         -0.01712469 -0.01761145 -0.01386531\n",
      "  0.          0.          0.0168189   0.         -0.02320156  0.\n",
      " -0.02357615  0.0168189  -0.02292453  0.03048384  0.017506   -0.02357615\n",
      "  0.0168189  -0.01761145 -0.01353618  0.03764157  0.          0.\n",
      " -0.01386531 -0.02265081  0.03794847  0.017506   -0.01386531  0.\n",
      "  0.         -0.03324406 -0.02274168  0.          0.          0.0135091\n",
      " -0.02851656  0.          0.          0.01145126  0.          0.\n",
      " -0.01138269  0.03697205  0.         -0.01775305  0.          0.0236234\n",
      "  0.01394883  0.         -0.03360469  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01729697\n",
      " -0.02329465  0.         -0.02338811  0.         -0.01353618 -0.02292453\n",
      "  0.          0.04465398  0.          0.02315516  0.017506    0.01394883\n",
      " -0.02320156  0.         -0.01386531 -0.01712469 -0.02247014  0.02536972\n",
      " -0.0138099   0.          0.          0.00967534  0.01394883  0.\n",
      "  0.          0.02278726  0.02806162 -0.02894893  0.         -0.02357615\n",
      " -0.03749408  0.023529    0.03809828  0.01722785 -0.02851656  0.\n",
      "  0.         -0.02338811  0.02287868  0.01695413  0.01729697 -0.01353618], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.         -0.0377193  -0.0226511   0.         -0.01133735  0.\n",
      " -0.0233884  -0.02274197  0.01350928  0.          0.01722807  0.03787177\n",
      "  0.         -0.02292482  0.          0.01722807  0.          0.03315777\n",
      "  0.          0.          0.02343527  0.          0.         -0.01381008\n",
      "  0.02278755 -0.02329494  0.02287897  0.          0.          0.\n",
      "  0.01350928  0.01729719  0.         -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.          0.01681912  0.         -0.02320186  0.\n",
      " -0.02357645  0.01681912 -0.02292482  0.03048423  0.01750622 -0.02357645\n",
      "  0.01681912 -0.01761168 -0.01353635  0.03764205  0.          0.\n",
      " -0.01386549 -0.0226511   0.03794896  0.01750622 -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.          0.          0.01350928\n",
      " -0.02851693  0.          0.          0.01145141  0.          0.\n",
      " -0.01138284  0.03697252  0.         -0.01775328  0.          0.0236237\n",
      "  0.01394901  0.         -0.03360512  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01729719\n",
      " -0.02329494  0.         -0.0233884   0.         -0.01353635 -0.02292482\n",
      "  0.          0.04465455  0.          0.02315545  0.01750622  0.01394901\n",
      " -0.02320186  0.         -0.01386549 -0.01712491 -0.02247043  0.02537004\n",
      " -0.01381008  0.          0.          0.00967546  0.01394901  0.\n",
      "  0.          0.02278755  0.02806198 -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.0235293   0.03809877  0.01722807 -0.02851693  0.\n",
      "  0.         -0.0233884   0.02287897  0.01695435  0.01729719 -0.01353635], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.         -0.0377193  -0.0226511   0.         -0.01133735  0.\n",
      " -0.0233884  -0.02274197  0.01350928  0.          0.01722807  0.03787177\n",
      "  0.         -0.02292482  0.          0.01722807  0.          0.03315777\n",
      "  0.          0.          0.02343527  0.          0.         -0.01381008\n",
      "  0.02278755 -0.02329494  0.02287897  0.          0.          0.\n",
      "  0.01350928  0.01729719  0.         -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.          0.01681912  0.         -0.02320186  0.\n",
      " -0.02357645  0.01681912 -0.02292482  0.03048423  0.01750622 -0.02357645\n",
      "  0.01681912 -0.01761168 -0.01353635  0.03764205  0.          0.\n",
      " -0.01386549 -0.0226511   0.03794896  0.01750622 -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.          0.          0.01350928\n",
      " -0.02851693  0.          0.          0.01145141  0.          0.\n",
      " -0.01138284  0.03697252  0.         -0.01775328  0.          0.0236237\n",
      "  0.01394901  0.         -0.03360512  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01729719\n",
      " -0.02329494  0.         -0.0233884   0.         -0.01353635 -0.02292482\n",
      "  0.          0.04465456  0.          0.02315545  0.01750622  0.01394901\n",
      " -0.02320186  0.         -0.01386549 -0.01712491 -0.02247043  0.02537004\n",
      " -0.01381008  0.          0.          0.00967547  0.01394901  0.\n",
      "  0.          0.02278755  0.02806198 -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.0235293   0.03809877  0.01722807 -0.02851693  0.\n",
      "  0.         -0.0233884   0.02287897  0.01695435  0.01729719 -0.01353635], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.         -0.0377193  -0.0226511   0.         -0.01133735  0.\n",
      " -0.0233884  -0.02274197  0.01350928  0.          0.01722807  0.03787177\n",
      "  0.         -0.02292482  0.          0.01722807  0.          0.03315777\n",
      "  0.          0.          0.02343527  0.          0.         -0.01381008\n",
      "  0.02278755 -0.02329494  0.02287897  0.          0.          0.\n",
      "  0.01350928  0.01729719  0.         -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.          0.01681912  0.         -0.02320186  0.\n",
      " -0.02357645  0.01681912 -0.02292482  0.03048423  0.01750622 -0.02357645\n",
      "  0.01681912 -0.01761168 -0.01353635  0.03764205  0.          0.\n",
      " -0.01386549 -0.0226511   0.03794896  0.01750622 -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.          0.          0.01350928\n",
      " -0.02851693  0.          0.          0.01145141  0.          0.\n",
      " -0.01138284  0.03697252  0.         -0.01775328  0.          0.0236237\n",
      "  0.01394901  0.         -0.03360512  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01729719\n",
      " -0.02329494  0.         -0.0233884   0.         -0.01353635 -0.02292482\n",
      "  0.          0.04465456  0.          0.02315545  0.01750622  0.01394901\n",
      " -0.02320186  0.         -0.01386549 -0.01712491 -0.02247043  0.02537004\n",
      " -0.01381008  0.          0.          0.00967547  0.01394901  0.\n",
      "  0.          0.02278755  0.02806198 -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.0235293   0.03809877  0.01722807 -0.02851693  0.\n",
      "  0.         -0.0233884   0.02287897  0.01695435  0.01729719 -0.01353635], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.         -0.0377193  -0.0226511   0.         -0.01133735  0.\n",
      " -0.0233884  -0.02274197  0.01350928  0.          0.01722807  0.03787177\n",
      "  0.         -0.02292482  0.          0.01722807  0.          0.03315777\n",
      "  0.          0.          0.02343527  0.          0.         -0.01381008\n",
      "  0.02278755 -0.02329494  0.02287897  0.          0.          0.\n",
      "  0.01350928  0.01729719  0.         -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.          0.01681912  0.         -0.02320186  0.\n",
      " -0.02357645  0.01681912 -0.02292482  0.03048423  0.01750622 -0.02357645\n",
      "  0.01681912 -0.01761168 -0.01353635  0.03764205  0.          0.\n",
      " -0.01386549 -0.0226511   0.03794896  0.01750622 -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.          0.          0.01350928\n",
      " -0.02851693  0.          0.          0.01145141  0.          0.\n",
      " -0.01138284  0.03697252  0.         -0.01775328  0.          0.0236237\n",
      "  0.01394901  0.         -0.03360512  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01729719\n",
      " -0.02329494  0.         -0.0233884   0.         -0.01353635 -0.02292482\n",
      "  0.          0.04465456  0.          0.02315545  0.01750622  0.01394901\n",
      " -0.02320186  0.         -0.01386549 -0.01712491 -0.02247043  0.02537004\n",
      " -0.01381008  0.          0.          0.00967547  0.01394901  0.\n",
      "  0.          0.02278755  0.02806198 -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.0235293   0.03809877  0.01722807 -0.02851693  0.\n",
      "  0.         -0.0233884   0.02287897  0.01695435  0.01729719 -0.01353635], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.         -0.0377193  -0.0226511   0.         -0.01133735  0.\n",
      " -0.0233884  -0.02274197  0.01350928  0.          0.01722807  0.03787177\n",
      "  0.         -0.02292482  0.          0.01722807  0.          0.03315777\n",
      "  0.          0.          0.02343527  0.          0.         -0.01381008\n",
      "  0.02278755 -0.02329494  0.02287897  0.          0.          0.\n",
      "  0.01350928  0.01729719  0.         -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.          0.01681912  0.         -0.02320186  0.\n",
      " -0.02357645  0.01681912 -0.02292482  0.03048423  0.01750622 -0.02357645\n",
      "  0.01681912 -0.01761168 -0.01353635  0.03764205  0.          0.\n",
      " -0.01386549 -0.0226511   0.03794896  0.01750622 -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.          0.          0.01350928\n",
      " -0.02851693  0.          0.          0.01145141  0.          0.\n",
      " -0.01138284  0.03697252  0.         -0.01775328  0.          0.0236237\n",
      "  0.01394901  0.         -0.03360512  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01729719\n",
      " -0.02329494  0.         -0.0233884   0.         -0.01353635 -0.02292482\n",
      "  0.          0.04465456  0.          0.02315545  0.01750622  0.01394901\n",
      " -0.02320186  0.         -0.01386549 -0.01712491 -0.02247043  0.02537004\n",
      " -0.01381008  0.          0.          0.00967547  0.01394901  0.\n",
      "  0.          0.02278755  0.02806198 -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.0235293   0.03809877  0.01722807 -0.02851693  0.\n",
      "  0.         -0.0233884   0.02287897  0.01695435  0.01729719 -0.01353635], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.         -0.0377193  -0.0226511   0.         -0.01133735  0.\n",
      " -0.0233884  -0.02274197  0.01350928  0.          0.01722807  0.03787177\n",
      "  0.         -0.02292482  0.          0.01722807  0.          0.03315777\n",
      "  0.          0.          0.02343527  0.          0.         -0.01381008\n",
      "  0.02278755 -0.02329494  0.02287897  0.          0.          0.\n",
      "  0.01350928  0.01729719  0.         -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.          0.01681912  0.         -0.02320186  0.\n",
      " -0.02357645  0.01681912 -0.02292482  0.03048423  0.01750622 -0.02357645\n",
      "  0.01681912 -0.01761168 -0.01353635  0.03764205  0.          0.\n",
      " -0.01386549 -0.0226511   0.03794896  0.01750622 -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.          0.          0.01350928\n",
      " -0.02851693  0.          0.          0.01145141  0.          0.\n",
      " -0.01138284  0.03697252  0.         -0.01775328  0.          0.0236237\n",
      "  0.01394901  0.         -0.03360512  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01729719\n",
      " -0.02329494  0.         -0.0233884   0.         -0.01353635 -0.02292482\n",
      "  0.          0.04465456  0.          0.02315545  0.01750622  0.01394901\n",
      " -0.02320186  0.         -0.01386549 -0.01712491 -0.02247043  0.02537004\n",
      " -0.01381008  0.          0.          0.00967547  0.01394901  0.\n",
      "  0.          0.02278755  0.02806198 -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.0235293   0.03809877  0.01722807 -0.02851693  0.\n",
      "  0.         -0.0233884   0.02287897  0.01695435  0.01729719 -0.01353635], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.         -0.0377193  -0.0226511   0.         -0.01133735  0.\n",
      " -0.0233884  -0.02274197  0.01350928  0.          0.01722807  0.03787177\n",
      "  0.         -0.02292482  0.          0.01722807  0.          0.03315777\n",
      "  0.          0.          0.02343527  0.          0.         -0.01381008\n",
      "  0.02278755 -0.02329494  0.02287897  0.          0.          0.\n",
      "  0.01350928  0.01729719  0.         -0.01712491 -0.01761168 -0.01386549\n",
      "  0.          0.          0.01681912  0.         -0.02320186  0.\n",
      " -0.02357645  0.01681912 -0.02292482  0.03048423  0.01750622 -0.02357645\n",
      "  0.01681912 -0.01761168 -0.01353635  0.03764205  0.          0.\n",
      " -0.01386549 -0.0226511   0.03794896  0.01750622 -0.01386549  0.\n",
      "  0.         -0.03324449 -0.02274197  0.          0.          0.01350928\n",
      " -0.02851693  0.          0.          0.01145141  0.          0.\n",
      " -0.01138284  0.03697252  0.         -0.01775328  0.          0.0236237\n",
      "  0.01394901  0.         -0.03360512  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01729719\n",
      " -0.02329494  0.         -0.0233884   0.         -0.01353635 -0.02292482\n",
      "  0.          0.04465456  0.          0.02315545  0.01750622  0.01394901\n",
      " -0.02320186  0.         -0.01386549 -0.01712491 -0.02247043  0.02537004\n",
      " -0.01381008  0.          0.          0.00967547  0.01394901  0.\n",
      "  0.          0.02278755  0.02806198 -0.0289493   0.         -0.02357645\n",
      " -0.03749456  0.0235293   0.03809877  0.01722807 -0.02851693  0.\n",
      "  0.         -0.0233884   0.02287897  0.01695435  0.01729719 -0.01353635], Bias: 0.0\n",
      "Classifier (1, 2) predictions: [ 1. -1.  1.  1. -1.  1.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.\n",
      "  1. -1.  1.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.\n",
      "  1. -1.  0.  1. -1.  1.  0. -1.  1.  1. -1.  0.]\n",
      "Classifier (1, 3) predictions: [ 1.  0. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  1. -1.  0.  1. -1.  1.  0. -1.]\n",
      "Classifier (2, 3) predictions: [-1.  1. -1.  0.  1. -1.  1.  1. -1.  0.  1. -1. -1.  1. -1.  0.  1. -1.\n",
      "  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1. -1.  1. -1.  0.  1. -1.\n",
      "  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.]\n",
      "Votes: [[2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 1. 0.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [1. 0. 2.]\n",
      " [0. 1. 2.]\n",
      " [1. 2. 0.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]]\n",
      "Final predictions: [0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0\n",
      " 1 2 0 1 2 2 1 2 0 1 2]\n",
      "Predictions: [0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0\n",
      " 1 2 0 1 2 2 1 2 0 1 2]\n",
      "Actual Labels: [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1\n",
      " 2 3 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_list = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    # Remove numbers and special symbols\n",
    "    filtered_list = [w for w in filtered_list if w.isalnum() and not w.isdigit()]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_list = [lemmatizer.lemmatize(w, 'v') for w in filtered_list]\n",
    "    \n",
    "    return ' '.join(lemmatized_list)\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    vocabulary = list(set(word for document in corpus for word in document.split()))\n",
    "    vocabulary.sort()\n",
    "    tfidf_matrix = np.zeros((len(corpus), len(vocabulary)))\n",
    "\n",
    "    for i, document in enumerate(corpus):\n",
    "        word_counts = Counter(document.split())\n",
    "        for word, count in word_counts.items():\n",
    "            tf = count / len(document.split())\n",
    "            idf = log(len(corpus) / sum(1 for doc in corpus if word in doc.split()))\n",
    "            tfidf_matrix[i, vocabulary.index(word)] = tf * idf\n",
    "\n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Binary SVM Classifier\n",
    "class BinarySVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for iteration in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights - np.dot(x_i, y[idx]))\n",
    "                    self.bias -= self.learning_rate * y[idx]\n",
    "\n",
    "            # Debugging: print weights and bias at every 100 iterations\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}: Weights: {self.weights}, Bias: {self.bias}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) - self.bias)\n",
    "\n",
    "# One-vs-One SVM Classifier\n",
    "class OneVsOneSVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        n_classes = len(unique_classes)\n",
    "        print(f\"Unique classes: {unique_classes}\")  # Debugging line\n",
    "        for i in range(n_classes):\n",
    "            for j in range(i + 1, n_classes):\n",
    "                class_i = unique_classes[i]\n",
    "                class_j = unique_classes[j]\n",
    "\n",
    "                # Filter the data for the two classes\n",
    "                idx = np.where((y == class_i) | (y == class_j))\n",
    "                X_filtered = X[idx]\n",
    "                y_filtered = y[idx]\n",
    "\n",
    "                # Convert class labels to +1 and -1\n",
    "                y_filtered = np.where(y_filtered == class_i, 1, -1)\n",
    "\n",
    "                # Train the binary classifier\n",
    "                clf = BinarySVM(C=self.C, learning_rate=self.learning_rate, n_iters=self.n_iters)\n",
    "                clf.fit(X_filtered, y_filtered)\n",
    "                self.classifiers.append((clf, (class_i, class_j)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = len(np.unique(y))\n",
    "        votes = np.zeros((X.shape[0], n_classes))\n",
    "\n",
    "        for clf, class_labels in self.classifiers:\n",
    "            predictions = clf.predict(X)\n",
    "            print(f\"Classifier {class_labels} predictions: {predictions}\")  # Debugging line\n",
    "            for idx, pred in enumerate(predictions):\n",
    "                if pred == 1:\n",
    "                    votes[idx, class_labels[0] - 1] += 1  # Adjust index for zero-based array\n",
    "                else:\n",
    "                    votes[idx, class_labels[1] - 1] += 1  # Adjust index for zero-based array\n",
    "\n",
    "        # Return the class with the most votes\n",
    "        final_predictions = np.argmax(votes, axis=1)\n",
    "        print(f\"Votes: {votes}\")  # Debugging line\n",
    "        print(f\"Final predictions: {final_predictions}\")  # Debugging line\n",
    "        return final_predictions\n",
    "\n",
    "# Sample dataset\n",
    "news_articles = [\n",
    "    \"government passes new law\",                     # Politics\n",
    "    \"football match ends in draw\",                    # Sports\n",
    "    \"new technology in smartphones\",                  # Technology\n",
    "    \"politician gives a speech\",                      # Politics\n",
    "    \"sports event attracts large crowd\",              # Sports\n",
    "    \"new tech gadgets released this year\",            # Technology\n",
    "    \"election results announced\",                     # Politics\n",
    "    \"soccer team wins championship\",                  # Sports\n",
    "    \"AI advancements in healthcare\",                  # Technology\n",
    "    \"local council meeting updates\",                  # Politics\n",
    "    \"basketball game highlights\",                     # Sports\n",
    "    \"innovations in artificial intelligence\",         # Technology\n",
    "    \"politician's new policy proposal\",               # Politics\n",
    "    \"swimming competition results\",                   # Sports\n",
    "    \"latest trends in smartphone design\",             # Technology\n",
    "    \"government budget allocation review\",            # Politics\n",
    "    \"volleyball tournament concludes\",                # Sports\n",
    "    \"breakthrough in renewable energy\",               # Technology\n",
    "    \"senator's speech on climate change\",             # Politics\n",
    "    \"baseball team training camp\",                    # Sports\n",
    "    \"tech company announces new software\",            # Technology\n",
    "    \"international relations summit\",                 # Politics\n",
    "    \"world cup qualifying matches\",                   # Sports\n",
    "    \"smart home devices market growth\",               # Technology\n",
    "    \"legislative bill discussion\",                    # Politics\n",
    "    \"rugby game results\",                            # Sports\n",
    "    \"technology in education sector\",                # Technology\n",
    "    \"mayoral election debate\",                        # Politics\n",
    "    \"national soccer league season start\",            # Sports\n",
    "    \"advancements in quantum computing\",              # Technology\n",
    "    \"press conference on new laws\",                   # Politics\n",
    "    \"hockey match final scores\",                      # Sports\n",
    "    \"virtual reality applications\",                   # Technology\n",
    "    \"parliamentary debate on economy\",                # Politics\n",
    "    \"college basketball championship\",                # Sports\n",
    "    \"latest trends in gadget development\",            # Technology\n",
    "    \"congressional committee meeting\",                # Politics\n",
    "    \"tennis tournament highlights\",                   # Sports\n",
    "    \"emerging technologies in fintech\",               # Technology\n",
    "    \"state of the union address\",                     # Politics\n",
    "    \"motorsports event results\",                      # Sports\n",
    "    \"new innovations in medical tech\",                # Technology\n",
    "    \"political rally speeches\",                       # Politics\n",
    "    \"community sports league updates\",                # Sports\n",
    "    \"tech industry conference news\",                  # Technology\n",
    "    \"government response to natural disaster\",        # Politics\n",
    "    \"annual sports awards ceremony\",                  # Sports\n",
    "    \"technological impacts on job market\"             # Technology\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "]\n",
    "\n",
    "# Convert news articles to TF-IDF features\n",
    "preprocessed_articles = [preprocess_text(article) for article in news_articles]\n",
    "X, vocabulary = compute_tfidf(preprocessed_articles)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize and train the One-vs-One SVM\n",
    "ovo_svm = OneVsOneSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "ovo_svm.fit(X, y)\n",
    "\n",
    "# Test the classifier on the training data\n",
    "predictions = ovo_svm.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual Labels:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [1 2 3]\n",
      "Iteration 0: Weights: [ 1.28267341e-03  0.00000000e+00  0.00000000e+00  9.31678376e-04\n",
      "  1.00763566e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -7.34970303e-04  0.00000000e+00 -9.37290881e-04 -2.06040819e-03\n",
      "  1.25724960e-03  0.00000000e+00  9.31678376e-04 -9.37290881e-04\n",
      "  0.00000000e+00 -1.80394368e-03  9.35416299e-04  9.35416299e-04\n",
      " -1.27499275e-03  1.27754785e-03 -9.67800253e-04  0.00000000e+00\n",
      " -1.23975336e-03  0.00000000e+00 -1.24472729e-03  7.80326093e-04\n",
      "  1.27754785e-03  9.24247279e-04 -7.34970303e-04 -9.41051322e-04\n",
      "  2.08088626e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.25724960e-03 -9.15041702e-04  1.27244277e-03\n",
      "  0.00000000e+00  2.04391287e-03  0.00000000e+00 -9.15041702e-04\n",
      "  0.00000000e+00 -1.65849034e-03 -9.52423410e-04  0.00000000e+00\n",
      " -9.15041702e-04  0.00000000e+00  0.00000000e+00 -2.04791039e-03\n",
      "  1.22250060e-03  1.32132668e-03  0.00000000e+00  0.00000000e+00\n",
      " -2.06460759e-03 -9.52423410e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.25222563e-03\n",
      "  0.00000000e+00 -7.34970303e-04  0.00000000e+00  9.13211619e-04\n",
      "  9.50518563e-04 -1.41752581e-03  1.25724960e-03  9.24247279e-04\n",
      "  0.00000000e+00 -2.01148454e-03  1.26229372e-03  0.00000000e+00\n",
      "  1.80755879e-03 -1.28524390e-03 -7.58894021e-04  0.00000000e+00\n",
      "  1.38840807e-03  0.00000000e+00  1.27244277e-03  9.13211619e-04\n",
      "  9.27955389e-04  1.28781954e-03  1.76541205e-03  9.50518563e-04\n",
      "  9.27955389e-04 -9.41051322e-04  0.00000000e+00  1.28781954e-03\n",
      "  0.00000000e+00  1.25222563e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.64155866e-03  9.31678376e-04 -1.25976913e-03\n",
      " -9.52423410e-04 -7.58894021e-04  0.00000000e+00  9.35416299e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.38025358e-03\n",
      "  0.00000000e+00  1.77153707e-03  1.28781954e-03 -1.21953947e-03\n",
      " -7.58894021e-04  1.28267341e-03  1.25222563e-03 -1.23975336e-03\n",
      " -1.52670808e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.28010806e-03 -2.07275795e-03 -9.37290881e-04\n",
      "  0.00000000e+00  1.28267341e-03 -3.57547164e-05  0.00000000e+00\n",
      " -1.24472729e-03 -9.22398785e-04 -9.41051322e-04  0.00000000e+00], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.02195321  0.          0.          0.01594586  0.01724588  0.\n",
      "  0.          0.         -0.01257916  0.         -0.01604192 -0.03526429\n",
      "  0.02151807  0.          0.01594586 -0.01604192  0.         -0.03087485\n",
      "  0.01600983  0.01600983 -0.02182175  0.02186548 -0.01656409  0.\n",
      " -0.02121862  0.         -0.02130375  0.01335543  0.02186548  0.01581867\n",
      " -0.01257916 -0.01610628  0.03561477  0.          0.          0.\n",
      "  0.          0.02151807 -0.01566112  0.02177811  0.          0.03498197\n",
      "  0.         -0.01566112  0.         -0.02838539 -0.01630091  0.\n",
      " -0.01566112  0.          0.         -0.03505039  0.02092334  0.02261476\n",
      "  0.          0.         -0.03533616 -0.01630091  0.          0.\n",
      "  0.          0.          0.          0.02143209  0.         -0.01257916\n",
      "  0.          0.01562979  0.01626831 -0.02426123  0.02151807  0.01581867\n",
      "  0.         -0.03442695  0.0216044   0.          0.03093672 -0.0219972\n",
      " -0.01298862  0.          0.02376287  0.          0.02177811  0.01562979\n",
      "  0.01588214  0.02204128  0.03021537  0.01626831  0.01588214 -0.01610628\n",
      "  0.          0.02204128  0.          0.02143209  0.          0.\n",
      "  0.         -0.0280956   0.01594586 -0.02156119 -0.01630091 -0.01298862\n",
      "  0.          0.01600983  0.          0.          0.         -0.02362331\n",
      "  0.          0.0303202   0.02204128 -0.02087266 -0.01298862  0.02195321\n",
      "  0.02143209 -0.02121862 -0.02612991  0.          0.          0.\n",
      "  0.         -0.0219093  -0.03547566 -0.01604192  0.          0.02195321\n",
      " -0.00061195  0.         -0.02130375 -0.01578703 -0.01610628  0.        ], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.02200414  0.          0.          0.01598285  0.01728589  0.\n",
      "  0.          0.         -0.01260834  0.         -0.01607913 -0.0353461\n",
      "  0.02156799  0.          0.01598285 -0.01607913  0.         -0.03094648\n",
      "  0.01604697  0.01604697 -0.02187237  0.02191621 -0.01660252  0.\n",
      " -0.02126785  0.         -0.02135317  0.01338642  0.02191621  0.01585537\n",
      " -0.01260834 -0.01614364  0.0356974   0.          0.          0.\n",
      "  0.          0.02156799 -0.01569745  0.02182863  0.          0.03506312\n",
      "  0.         -0.01569745  0.         -0.02845124 -0.01633873  0.\n",
      " -0.01569745  0.          0.         -0.0351317   0.02097188  0.02266723\n",
      "  0.          0.         -0.03541814 -0.01633873  0.          0.\n",
      "  0.          0.          0.          0.02148181  0.         -0.01260834\n",
      "  0.          0.01566605  0.01630605 -0.02431752  0.02156799  0.01585537\n",
      "  0.         -0.03450682  0.02165452  0.          0.03100849 -0.02204823\n",
      " -0.01301875  0.          0.023818    0.          0.02182863  0.01566605\n",
      "  0.01591898  0.02209242  0.03028547  0.01630605  0.01591898 -0.01614364\n",
      "  0.          0.02209242  0.          0.02148181  0.          0.\n",
      "  0.         -0.02816078  0.01598285 -0.02161121 -0.01633873 -0.01301875\n",
      "  0.          0.01604697  0.          0.          0.         -0.02367811\n",
      "  0.          0.03039054  0.02209242 -0.02092108 -0.01301875  0.02200414\n",
      "  0.02148181 -0.02126785 -0.02619053  0.          0.          0.\n",
      "  0.         -0.02196013 -0.03555796 -0.01607913  0.          0.02200414\n",
      " -0.00061337  0.         -0.02135317 -0.01582366 -0.01614364  0.        ], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.02200426  0.          0.          0.01598294  0.01728599  0.\n",
      "  0.          0.         -0.01260841  0.         -0.01607922 -0.0353463\n",
      "  0.02156812  0.          0.01598294 -0.01607922  0.         -0.03094665\n",
      "  0.01604707  0.01604707 -0.0218725   0.02191633 -0.01660261  0.\n",
      " -0.02126797  0.         -0.0213533   0.01338649  0.02191633  0.01585546\n",
      " -0.01260841 -0.01614373  0.0356976   0.          0.          0.\n",
      "  0.          0.02156812 -0.01569754  0.02182875  0.          0.03506332\n",
      "  0.         -0.01569754  0.         -0.0284514  -0.01633882  0.\n",
      " -0.01569754  0.          0.         -0.0351319   0.020972    0.02266736\n",
      "  0.          0.         -0.03541834 -0.01633882  0.          0.\n",
      "  0.          0.          0.          0.02148193  0.         -0.01260841\n",
      "  0.          0.01566614  0.01630614 -0.02431765  0.02156812  0.01585546\n",
      "  0.         -0.03450702  0.02165465  0.          0.03100867 -0.02204836\n",
      " -0.01301883  0.          0.02381814  0.          0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630614  0.01591907 -0.01614373\n",
      "  0.          0.02209254  0.          0.02148193  0.          0.\n",
      "  0.         -0.02816094  0.01598294 -0.02161134 -0.01633882 -0.01301883\n",
      "  0.          0.01604707  0.          0.          0.         -0.02367825\n",
      "  0.          0.03039072  0.02209254 -0.0209212  -0.01301883  0.02200426\n",
      "  0.02148193 -0.02126797 -0.02619068  0.          0.          0.\n",
      "  0.         -0.02196025 -0.03555816 -0.01607922  0.          0.02200426\n",
      " -0.00061337  0.         -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.02200426  0.          0.          0.01598294  0.01728599  0.\n",
      "  0.          0.         -0.01260841  0.         -0.01607922 -0.0353463\n",
      "  0.02156812  0.          0.01598294 -0.01607922  0.         -0.03094665\n",
      "  0.01604707  0.01604707 -0.0218725   0.02191633 -0.01660261  0.\n",
      " -0.02126797  0.         -0.0213533   0.01338649  0.02191633  0.01585546\n",
      " -0.01260841 -0.01614373  0.0356976   0.          0.          0.\n",
      "  0.          0.02156812 -0.01569754  0.02182875  0.          0.03506332\n",
      "  0.         -0.01569754  0.         -0.0284514  -0.01633882  0.\n",
      " -0.01569754  0.          0.         -0.0351319   0.020972    0.02266736\n",
      "  0.          0.         -0.03541834 -0.01633882  0.          0.\n",
      "  0.          0.          0.          0.02148193  0.         -0.01260841\n",
      "  0.          0.01566614  0.01630615 -0.02431765  0.02156812  0.01585546\n",
      "  0.         -0.03450702  0.02165465  0.          0.03100867 -0.02204836\n",
      " -0.01301883  0.          0.02381814  0.          0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907 -0.01614373\n",
      "  0.          0.02209254  0.          0.02148193  0.          0.\n",
      "  0.         -0.02816094  0.01598294 -0.02161134 -0.01633882 -0.01301883\n",
      "  0.          0.01604707  0.          0.          0.         -0.02367825\n",
      "  0.          0.03039072  0.02209254 -0.0209212  -0.01301883  0.02200426\n",
      "  0.02148193 -0.02126797 -0.02619068  0.          0.          0.\n",
      "  0.         -0.02196025 -0.03555816 -0.01607922  0.          0.02200426\n",
      " -0.00061337  0.         -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.02200426  0.          0.          0.01598294  0.01728599  0.\n",
      "  0.          0.         -0.01260841  0.         -0.01607922 -0.0353463\n",
      "  0.02156812  0.          0.01598294 -0.01607922  0.         -0.03094665\n",
      "  0.01604707  0.01604707 -0.0218725   0.02191633 -0.01660261  0.\n",
      " -0.02126797  0.         -0.0213533   0.01338649  0.02191633  0.01585546\n",
      " -0.01260841 -0.01614373  0.0356976   0.          0.          0.\n",
      "  0.          0.02156812 -0.01569754  0.02182875  0.          0.03506332\n",
      "  0.         -0.01569754  0.         -0.0284514  -0.01633882  0.\n",
      " -0.01569754  0.          0.         -0.0351319   0.020972    0.02266736\n",
      "  0.          0.         -0.03541834 -0.01633882  0.          0.\n",
      "  0.          0.          0.          0.02148193  0.         -0.01260841\n",
      "  0.          0.01566614  0.01630615 -0.02431765  0.02156812  0.01585546\n",
      "  0.         -0.03450702  0.02165465  0.          0.03100867 -0.02204836\n",
      " -0.01301883  0.          0.02381814  0.          0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907 -0.01614373\n",
      "  0.          0.02209254  0.          0.02148193  0.          0.\n",
      "  0.         -0.02816094  0.01598294 -0.02161134 -0.01633882 -0.01301883\n",
      "  0.          0.01604707  0.          0.          0.         -0.02367825\n",
      "  0.          0.03039072  0.02209254 -0.0209212  -0.01301883  0.02200426\n",
      "  0.02148193 -0.02126797 -0.02619068  0.          0.          0.\n",
      "  0.         -0.02196025 -0.03555816 -0.01607922  0.          0.02200426\n",
      " -0.00061337  0.         -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.02200426  0.          0.          0.01598294  0.01728599  0.\n",
      "  0.          0.         -0.01260841  0.         -0.01607922 -0.0353463\n",
      "  0.02156812  0.          0.01598294 -0.01607922  0.         -0.03094665\n",
      "  0.01604707  0.01604707 -0.0218725   0.02191633 -0.01660261  0.\n",
      " -0.02126797  0.         -0.0213533   0.01338649  0.02191633  0.01585546\n",
      " -0.01260841 -0.01614373  0.0356976   0.          0.          0.\n",
      "  0.          0.02156812 -0.01569754  0.02182875  0.          0.03506332\n",
      "  0.         -0.01569754  0.         -0.0284514  -0.01633882  0.\n",
      " -0.01569754  0.          0.         -0.0351319   0.020972    0.02266736\n",
      "  0.          0.         -0.03541834 -0.01633882  0.          0.\n",
      "  0.          0.          0.          0.02148193  0.         -0.01260841\n",
      "  0.          0.01566614  0.01630615 -0.02431765  0.02156812  0.01585546\n",
      "  0.         -0.03450702  0.02165465  0.          0.03100867 -0.02204836\n",
      " -0.01301883  0.          0.02381814  0.          0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907 -0.01614373\n",
      "  0.          0.02209254  0.          0.02148193  0.          0.\n",
      "  0.         -0.02816094  0.01598294 -0.02161134 -0.01633882 -0.01301883\n",
      "  0.          0.01604707  0.          0.          0.         -0.02367825\n",
      "  0.          0.03039072  0.02209254 -0.0209212  -0.01301883  0.02200426\n",
      "  0.02148193 -0.02126797 -0.02619068  0.          0.          0.\n",
      "  0.         -0.02196025 -0.03555816 -0.01607922  0.          0.02200426\n",
      " -0.00061337  0.         -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.02200426  0.          0.          0.01598294  0.01728599  0.\n",
      "  0.          0.         -0.01260841  0.         -0.01607922 -0.0353463\n",
      "  0.02156812  0.          0.01598294 -0.01607922  0.         -0.03094665\n",
      "  0.01604707  0.01604707 -0.0218725   0.02191633 -0.01660261  0.\n",
      " -0.02126797  0.         -0.0213533   0.01338649  0.02191633  0.01585546\n",
      " -0.01260841 -0.01614373  0.0356976   0.          0.          0.\n",
      "  0.          0.02156812 -0.01569754  0.02182875  0.          0.03506332\n",
      "  0.         -0.01569754  0.         -0.0284514  -0.01633882  0.\n",
      " -0.01569754  0.          0.         -0.0351319   0.020972    0.02266736\n",
      "  0.          0.         -0.03541834 -0.01633882  0.          0.\n",
      "  0.          0.          0.          0.02148193  0.         -0.01260841\n",
      "  0.          0.01566614  0.01630615 -0.02431765  0.02156812  0.01585546\n",
      "  0.         -0.03450702  0.02165465  0.          0.03100867 -0.02204836\n",
      " -0.01301883  0.          0.02381814  0.          0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907 -0.01614373\n",
      "  0.          0.02209254  0.          0.02148193  0.          0.\n",
      "  0.         -0.02816094  0.01598294 -0.02161134 -0.01633882 -0.01301883\n",
      "  0.          0.01604707  0.          0.          0.         -0.02367825\n",
      "  0.          0.03039072  0.02209254 -0.0209212  -0.01301883  0.02200426\n",
      "  0.02148193 -0.02126797 -0.02619068  0.          0.          0.\n",
      "  0.         -0.02196025 -0.03555816 -0.01607922  0.          0.02200426\n",
      " -0.00061337  0.         -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.02200426  0.          0.          0.01598294  0.01728599  0.\n",
      "  0.          0.         -0.01260841  0.         -0.01607922 -0.0353463\n",
      "  0.02156812  0.          0.01598294 -0.01607922  0.         -0.03094665\n",
      "  0.01604707  0.01604707 -0.0218725   0.02191633 -0.01660261  0.\n",
      " -0.02126797  0.         -0.0213533   0.01338649  0.02191633  0.01585546\n",
      " -0.01260841 -0.01614373  0.0356976   0.          0.          0.\n",
      "  0.          0.02156812 -0.01569754  0.02182875  0.          0.03506332\n",
      "  0.         -0.01569754  0.         -0.0284514  -0.01633882  0.\n",
      " -0.01569754  0.          0.         -0.0351319   0.020972    0.02266736\n",
      "  0.          0.         -0.03541834 -0.01633882  0.          0.\n",
      "  0.          0.          0.          0.02148193  0.         -0.01260841\n",
      "  0.          0.01566614  0.01630615 -0.02431765  0.02156812  0.01585546\n",
      "  0.         -0.03450702  0.02165465  0.          0.03100867 -0.02204836\n",
      " -0.01301883  0.          0.02381814  0.          0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907 -0.01614373\n",
      "  0.          0.02209254  0.          0.02148193  0.          0.\n",
      "  0.         -0.02816094  0.01598294 -0.02161134 -0.01633882 -0.01301883\n",
      "  0.          0.01604707  0.          0.          0.         -0.02367825\n",
      "  0.          0.03039072  0.02209254 -0.0209212  -0.01301883  0.02200426\n",
      "  0.02148193 -0.02126797 -0.02619068  0.          0.          0.\n",
      "  0.         -0.02196025 -0.03555816 -0.01607922  0.          0.02200426\n",
      " -0.00061337  0.         -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.02200426  0.          0.          0.01598294  0.01728599  0.\n",
      "  0.          0.         -0.01260841  0.         -0.01607922 -0.0353463\n",
      "  0.02156812  0.          0.01598294 -0.01607922  0.         -0.03094665\n",
      "  0.01604707  0.01604707 -0.0218725   0.02191633 -0.01660261  0.\n",
      " -0.02126797  0.         -0.0213533   0.01338649  0.02191633  0.01585546\n",
      " -0.01260841 -0.01614373  0.0356976   0.          0.          0.\n",
      "  0.          0.02156812 -0.01569754  0.02182875  0.          0.03506332\n",
      "  0.         -0.01569754  0.         -0.0284514  -0.01633882  0.\n",
      " -0.01569754  0.          0.         -0.0351319   0.020972    0.02266736\n",
      "  0.          0.         -0.03541834 -0.01633882  0.          0.\n",
      "  0.          0.          0.          0.02148193  0.         -0.01260841\n",
      "  0.          0.01566614  0.01630615 -0.02431765  0.02156812  0.01585546\n",
      "  0.         -0.03450702  0.02165465  0.          0.03100867 -0.02204836\n",
      " -0.01301883  0.          0.02381814  0.          0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907 -0.01614373\n",
      "  0.          0.02209254  0.          0.02148193  0.          0.\n",
      "  0.         -0.02816094  0.01598294 -0.02161134 -0.01633882 -0.01301883\n",
      "  0.          0.01604707  0.          0.          0.         -0.02367825\n",
      "  0.          0.03039072  0.02209254 -0.0209212  -0.01301883  0.02200426\n",
      "  0.02148193 -0.02126797 -0.02619068  0.          0.          0.\n",
      "  0.         -0.02196025 -0.03555816 -0.01607922  0.          0.02200426\n",
      " -0.00061337  0.         -0.0213533  -0.01582375 -0.01614373  0.        ], Bias: 0.0\n",
      "Iteration 0: Weights: [ 1.28267341e-03 -2.04800889e-03 -1.22986505e-03  9.31678376e-04\n",
      "  3.92062169e-04  0.00000000e+00 -1.26989788e-03 -1.23479930e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.25724960e-03 -1.24472729e-03  9.31678376e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  9.35416299e-04  9.35416299e-04\n",
      "  0.00000000e+00  1.27754785e-03  0.00000000e+00 -7.49832705e-04\n",
      "  0.00000000e+00 -1.26482337e-03  0.00000000e+00 -1.41873646e-05\n",
      "  1.27754785e-03  9.24247279e-04  0.00000000e+00  0.00000000e+00\n",
      "  2.08088626e-03 -9.29815019e-04 -9.56244563e-04 -7.52841058e-04\n",
      "  0.00000000e+00  1.25724960e-03  0.00000000e+00  1.27244277e-03\n",
      " -1.25976913e-03  2.04391287e-03 -1.28010806e-03  0.00000000e+00\n",
      " -1.24472729e-03  0.00000000e+00  0.00000000e+00 -1.28010806e-03\n",
      "  0.00000000e+00 -9.56244563e-04 -7.34970303e-04  0.00000000e+00\n",
      "  1.22250060e-03  1.32132668e-03 -7.52841058e-04 -1.22986505e-03\n",
      "  0.00000000e+00  0.00000000e+00 -7.52841058e-04  0.00000000e+00\n",
      " -9.67800253e-04 -1.80504431e-03 -1.23479930e-03  1.25222563e-03\n",
      "  0.00000000e+00  0.00000000e+00 -1.54835640e-03  9.13211619e-04\n",
      "  9.50518563e-04  0.00000000e+00  1.25724960e-03  9.24247279e-04\n",
      " -6.18043186e-04  0.00000000e+00  1.26229372e-03 -9.63932923e-04\n",
      "  1.80755879e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -4.36217105e-04 -9.67800253e-04  1.27244277e-03  9.13211619e-04\n",
      "  9.27955389e-04  1.28781954e-03  1.76541205e-03  9.50518563e-04\n",
      "  9.27955389e-04  0.00000000e+00 -1.26482337e-03  1.28781954e-03\n",
      " -1.26989788e-03  1.25222563e-03 -7.34970303e-04 -1.24472729e-03\n",
      "  0.00000000e+00  7.87865994e-04  9.31678376e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.25976913e-03  9.35416299e-04\n",
      " -7.52841058e-04 -9.29815019e-04 -1.22005560e-03  0.00000000e+00\n",
      " -7.49832705e-04  1.77153707e-03  1.28781954e-03  0.00000000e+00\n",
      "  0.00000000e+00  1.28267341e-03  1.25222563e-03  0.00000000e+00\n",
      "  0.00000000e+00 -2.19305933e-03  0.00000000e+00 -1.28010806e-03\n",
      " -2.03580658e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.54835640e-03  1.28267341e-03  7.58758741e-04 -1.26989788e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.34970303e-04], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.02195321 -0.03505207 -0.02104938  0.01594586  0.00671022  0.\n",
      " -0.02173455 -0.02113383  0.          0.          0.          0.\n",
      "  0.02151807 -0.02130375  0.01594586  0.          0.          0.\n",
      "  0.01600983  0.01600983  0.          0.02186548  0.         -0.01283353\n",
      "  0.         -0.0216477   0.         -0.00024282  0.02186548  0.01581867\n",
      "  0.          0.          0.03561477 -0.01591397 -0.01636631 -0.01288502\n",
      "  0.          0.02151807  0.          0.02177811 -0.02156119  0.03498197\n",
      " -0.0219093   0.         -0.02130375  0.          0.         -0.0219093\n",
      "  0.         -0.01636631 -0.01257916  0.          0.02092334  0.02261476\n",
      " -0.01288502 -0.02104938  0.          0.         -0.01288502  0.\n",
      " -0.01656409 -0.03089369 -0.02113383  0.02143209  0.          0.\n",
      " -0.02650042  0.01562979  0.01626831  0.          0.02151807  0.01581867\n",
      " -0.01057793  0.          0.0216044  -0.0164979   0.03093672  0.\n",
      "  0.          0.         -0.00746594 -0.01656409  0.02177811  0.01562979\n",
      "  0.01588214  0.02204128  0.03021537  0.01626831  0.01588214  0.\n",
      " -0.0216477   0.02204128 -0.02173455  0.02143209 -0.01257916 -0.02130375\n",
      "  0.          0.01348448  0.01594586  0.          0.          0.\n",
      " -0.02156119  0.01600983 -0.01288502 -0.01591397 -0.02088149  0.\n",
      " -0.01283353  0.0303202   0.02204128  0.          0.          0.02195321\n",
      "  0.02143209  0.          0.         -0.03753464  0.         -0.0219093\n",
      " -0.03484323  0.          0.          0.         -0.02650042  0.02195321\n",
      "  0.0129863  -0.02173455  0.          0.          0.         -0.01257916], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.02200414 -0.03513339 -0.02109821  0.01598285  0.00672579  0.\n",
      " -0.02178497 -0.02118286  0.          0.          0.          0.\n",
      "  0.02156799 -0.02135317  0.01598285  0.          0.          0.\n",
      "  0.01604697  0.01604697  0.          0.02191621  0.         -0.01286331\n",
      "  0.         -0.02169792  0.         -0.00024338  0.02191621  0.01585537\n",
      "  0.          0.          0.0356974  -0.01595088 -0.01640428 -0.01291491\n",
      "  0.          0.02156799  0.          0.02182863 -0.02161121  0.03506312\n",
      " -0.02196013  0.         -0.02135317  0.          0.         -0.02196013\n",
      "  0.         -0.01640428 -0.01260834  0.          0.02097188  0.02266723\n",
      " -0.01291491 -0.02109821  0.          0.         -0.01291491  0.\n",
      " -0.01660252 -0.03096536 -0.02118286  0.02148181  0.          0.\n",
      " -0.0265619   0.01566605  0.01630605  0.          0.02156799  0.01585537\n",
      " -0.01060247  0.          0.02165452 -0.01653617  0.03100849  0.\n",
      "  0.          0.         -0.00748326 -0.01660252  0.02182863  0.01566605\n",
      "  0.01591898  0.02209242  0.03028547  0.01630605  0.01591898  0.\n",
      " -0.02169792  0.02209242 -0.02178497  0.02148181 -0.01260834 -0.02135317\n",
      "  0.          0.01351576  0.01598285  0.          0.          0.\n",
      " -0.02161121  0.01604697 -0.01291491 -0.01595088 -0.02092993  0.\n",
      " -0.01286331  0.03039054  0.02209242  0.          0.          0.02200414\n",
      "  0.02148181  0.          0.         -0.03762172  0.         -0.02196013\n",
      " -0.03492406  0.          0.          0.         -0.0265619   0.02200414\n",
      "  0.01301643 -0.02178497  0.          0.          0.         -0.01260834], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.02200426 -0.03513359 -0.02109833  0.01598294  0.00672583  0.\n",
      " -0.0217851  -0.02118298  0.          0.          0.          0.\n",
      "  0.02156812 -0.0213533   0.01598294  0.          0.          0.\n",
      "  0.01604707  0.01604707  0.          0.02191633  0.         -0.01286338\n",
      "  0.         -0.02169804  0.         -0.00024338  0.02191633  0.01585546\n",
      "  0.          0.          0.0356976  -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.02156812  0.          0.02182875 -0.02161134  0.03506332\n",
      " -0.02196025  0.         -0.0213533   0.          0.         -0.02196025\n",
      "  0.         -0.01640437 -0.01260841  0.          0.020972    0.02266736\n",
      " -0.01291499 -0.02109833  0.          0.         -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.02148193  0.          0.\n",
      " -0.02656205  0.01566614  0.01630614  0.          0.02156812  0.01585546\n",
      " -0.01060253  0.          0.02165465 -0.01653627  0.03100867  0.\n",
      "  0.          0.         -0.0074833  -0.01660261  0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630614  0.01591907  0.\n",
      " -0.02169804  0.02209254 -0.0217851   0.02148193 -0.01260841 -0.0213533\n",
      "  0.          0.01351584  0.01598294  0.          0.          0.\n",
      " -0.02161134  0.01604707 -0.01291499 -0.01595098 -0.02093005  0.\n",
      " -0.01286338  0.03039072  0.02209254  0.          0.          0.02200426\n",
      "  0.02148193  0.          0.         -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.          0.          0.         -0.02656205  0.02200426\n",
      "  0.01301651 -0.0217851   0.          0.          0.         -0.01260841], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.02200426 -0.03513359 -0.02109833  0.01598294  0.00672583  0.\n",
      " -0.0217851  -0.02118298  0.          0.          0.          0.\n",
      "  0.02156812 -0.0213533   0.01598294  0.          0.          0.\n",
      "  0.01604707  0.01604707  0.          0.02191633  0.         -0.01286338\n",
      "  0.         -0.02169804  0.         -0.00024338  0.02191633  0.01585546\n",
      "  0.          0.          0.0356976  -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.02156812  0.          0.02182875 -0.02161134  0.03506332\n",
      " -0.02196025  0.         -0.0213533   0.          0.         -0.02196025\n",
      "  0.         -0.01640437 -0.01260841  0.          0.020972    0.02266736\n",
      " -0.01291499 -0.02109833  0.          0.         -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.02148193  0.          0.\n",
      " -0.02656205  0.01566614  0.01630615  0.          0.02156812  0.01585546\n",
      " -0.01060253  0.          0.02165465 -0.01653627  0.03100867  0.\n",
      "  0.          0.         -0.0074833  -0.01660261  0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907  0.\n",
      " -0.02169804  0.02209254 -0.0217851   0.02148193 -0.01260841 -0.0213533\n",
      "  0.          0.01351584  0.01598294  0.          0.          0.\n",
      " -0.02161134  0.01604707 -0.01291499 -0.01595098 -0.02093005  0.\n",
      " -0.01286338  0.03039072  0.02209254  0.          0.          0.02200426\n",
      "  0.02148193  0.          0.         -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.          0.          0.         -0.02656205  0.02200426\n",
      "  0.01301651 -0.0217851   0.          0.          0.         -0.01260841], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.02200426 -0.03513359 -0.02109833  0.01598294  0.00672583  0.\n",
      " -0.0217851  -0.02118298  0.          0.          0.          0.\n",
      "  0.02156812 -0.0213533   0.01598294  0.          0.          0.\n",
      "  0.01604707  0.01604707  0.          0.02191633  0.         -0.01286338\n",
      "  0.         -0.02169804  0.         -0.00024338  0.02191633  0.01585546\n",
      "  0.          0.          0.0356976  -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.02156812  0.          0.02182875 -0.02161134  0.03506332\n",
      " -0.02196025  0.         -0.0213533   0.          0.         -0.02196025\n",
      "  0.         -0.01640437 -0.01260841  0.          0.020972    0.02266736\n",
      " -0.01291499 -0.02109833  0.          0.         -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.02148193  0.          0.\n",
      " -0.02656205  0.01566614  0.01630615  0.          0.02156812  0.01585546\n",
      " -0.01060253  0.          0.02165465 -0.01653627  0.03100867  0.\n",
      "  0.          0.         -0.0074833  -0.01660261  0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907  0.\n",
      " -0.02169804  0.02209254 -0.0217851   0.02148193 -0.01260841 -0.0213533\n",
      "  0.          0.01351584  0.01598294  0.          0.          0.\n",
      " -0.02161134  0.01604707 -0.01291499 -0.01595098 -0.02093005  0.\n",
      " -0.01286338  0.03039072  0.02209254  0.          0.          0.02200426\n",
      "  0.02148193  0.          0.         -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.          0.          0.         -0.02656205  0.02200426\n",
      "  0.01301651 -0.0217851   0.          0.          0.         -0.01260841], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.02200426 -0.03513359 -0.02109833  0.01598294  0.00672583  0.\n",
      " -0.0217851  -0.02118298  0.          0.          0.          0.\n",
      "  0.02156812 -0.0213533   0.01598294  0.          0.          0.\n",
      "  0.01604707  0.01604707  0.          0.02191633  0.         -0.01286338\n",
      "  0.         -0.02169804  0.         -0.00024338  0.02191633  0.01585546\n",
      "  0.          0.          0.0356976  -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.02156812  0.          0.02182875 -0.02161134  0.03506332\n",
      " -0.02196025  0.         -0.0213533   0.          0.         -0.02196025\n",
      "  0.         -0.01640437 -0.01260841  0.          0.020972    0.02266736\n",
      " -0.01291499 -0.02109833  0.          0.         -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.02148193  0.          0.\n",
      " -0.02656205  0.01566614  0.01630615  0.          0.02156812  0.01585546\n",
      " -0.01060253  0.          0.02165465 -0.01653627  0.03100867  0.\n",
      "  0.          0.         -0.0074833  -0.01660261  0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907  0.\n",
      " -0.02169804  0.02209254 -0.0217851   0.02148193 -0.01260841 -0.0213533\n",
      "  0.          0.01351584  0.01598294  0.          0.          0.\n",
      " -0.02161134  0.01604707 -0.01291499 -0.01595098 -0.02093005  0.\n",
      " -0.01286338  0.03039072  0.02209254  0.          0.          0.02200426\n",
      "  0.02148193  0.          0.         -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.          0.          0.         -0.02656205  0.02200426\n",
      "  0.01301651 -0.0217851   0.          0.          0.         -0.01260841], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.02200426 -0.03513359 -0.02109833  0.01598294  0.00672583  0.\n",
      " -0.0217851  -0.02118298  0.          0.          0.          0.\n",
      "  0.02156812 -0.0213533   0.01598294  0.          0.          0.\n",
      "  0.01604707  0.01604707  0.          0.02191633  0.         -0.01286338\n",
      "  0.         -0.02169804  0.         -0.00024338  0.02191633  0.01585546\n",
      "  0.          0.          0.0356976  -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.02156812  0.          0.02182875 -0.02161134  0.03506332\n",
      " -0.02196025  0.         -0.0213533   0.          0.         -0.02196025\n",
      "  0.         -0.01640437 -0.01260841  0.          0.020972    0.02266736\n",
      " -0.01291499 -0.02109833  0.          0.         -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.02148193  0.          0.\n",
      " -0.02656205  0.01566614  0.01630615  0.          0.02156812  0.01585546\n",
      " -0.01060253  0.          0.02165465 -0.01653627  0.03100867  0.\n",
      "  0.          0.         -0.0074833  -0.01660261  0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907  0.\n",
      " -0.02169804  0.02209254 -0.0217851   0.02148193 -0.01260841 -0.0213533\n",
      "  0.          0.01351584  0.01598294  0.          0.          0.\n",
      " -0.02161134  0.01604707 -0.01291499 -0.01595098 -0.02093005  0.\n",
      " -0.01286338  0.03039072  0.02209254  0.          0.          0.02200426\n",
      "  0.02148193  0.          0.         -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.          0.          0.         -0.02656205  0.02200426\n",
      "  0.01301651 -0.0217851   0.          0.          0.         -0.01260841], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.02200426 -0.03513359 -0.02109833  0.01598294  0.00672583  0.\n",
      " -0.0217851  -0.02118298  0.          0.          0.          0.\n",
      "  0.02156812 -0.0213533   0.01598294  0.          0.          0.\n",
      "  0.01604707  0.01604707  0.          0.02191633  0.         -0.01286338\n",
      "  0.         -0.02169804  0.         -0.00024338  0.02191633  0.01585546\n",
      "  0.          0.          0.0356976  -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.02156812  0.          0.02182875 -0.02161134  0.03506332\n",
      " -0.02196025  0.         -0.0213533   0.          0.         -0.02196025\n",
      "  0.         -0.01640437 -0.01260841  0.          0.020972    0.02266736\n",
      " -0.01291499 -0.02109833  0.          0.         -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.02148193  0.          0.\n",
      " -0.02656205  0.01566614  0.01630615  0.          0.02156812  0.01585546\n",
      " -0.01060253  0.          0.02165465 -0.01653627  0.03100867  0.\n",
      "  0.          0.         -0.0074833  -0.01660261  0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907  0.\n",
      " -0.02169804  0.02209254 -0.0217851   0.02148193 -0.01260841 -0.0213533\n",
      "  0.          0.01351584  0.01598294  0.          0.          0.\n",
      " -0.02161134  0.01604707 -0.01291499 -0.01595098 -0.02093005  0.\n",
      " -0.01286338  0.03039072  0.02209254  0.          0.          0.02200426\n",
      "  0.02148193  0.          0.         -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.          0.          0.         -0.02656205  0.02200426\n",
      "  0.01301651 -0.0217851   0.          0.          0.         -0.01260841], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.02200426 -0.03513359 -0.02109833  0.01598294  0.00672583  0.\n",
      " -0.0217851  -0.02118298  0.          0.          0.          0.\n",
      "  0.02156812 -0.0213533   0.01598294  0.          0.          0.\n",
      "  0.01604707  0.01604707  0.          0.02191633  0.         -0.01286338\n",
      "  0.         -0.02169804  0.         -0.00024338  0.02191633  0.01585546\n",
      "  0.          0.          0.0356976  -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.02156812  0.          0.02182875 -0.02161134  0.03506332\n",
      " -0.02196025  0.         -0.0213533   0.          0.         -0.02196025\n",
      "  0.         -0.01640437 -0.01260841  0.          0.020972    0.02266736\n",
      " -0.01291499 -0.02109833  0.          0.         -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.02148193  0.          0.\n",
      " -0.02656205  0.01566614  0.01630615  0.          0.02156812  0.01585546\n",
      " -0.01060253  0.          0.02165465 -0.01653627  0.03100867  0.\n",
      "  0.          0.         -0.0074833  -0.01660261  0.02182875  0.01566614\n",
      "  0.01591907  0.02209254  0.03028564  0.01630615  0.01591907  0.\n",
      " -0.02169804  0.02209254 -0.0217851   0.02148193 -0.01260841 -0.0213533\n",
      "  0.          0.01351584  0.01598294  0.          0.          0.\n",
      " -0.02161134  0.01604707 -0.01291499 -0.01595098 -0.02093005  0.\n",
      " -0.01286338  0.03039072  0.02209254  0.          0.          0.02200426\n",
      "  0.02148193  0.          0.         -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.          0.          0.         -0.02656205  0.02200426\n",
      "  0.01301651 -0.0217851   0.          0.          0.         -0.01260841], Bias: 0.0\n",
      "Iteration 0: Weights: [ 0.         -0.00204801 -0.00122987  0.         -0.00061557  0.\n",
      " -0.0012699  -0.0012348   0.0007335   0.          0.00093542  0.00205629\n",
      "  0.         -0.00124473  0.          0.00093542  0.          0.00180034\n",
      "  0.          0.          0.00127244  0.          0.00096586 -0.00074983\n",
      "  0.00123727 -0.00126482  0.00124224 -0.00079451  0.          0.\n",
      "  0.0007335   0.00093917  0.         -0.00092982 -0.00095624 -0.00075284\n",
      "  0.          0.          0.00091321  0.         -0.00125977  0.\n",
      " -0.00128011  0.00091321 -0.00124473  0.00165517  0.00095052 -0.00128011\n",
      "  0.00091321 -0.00095624 -0.00073497  0.00204381  0.          0.\n",
      " -0.00075284 -0.00122987  0.00206048  0.00095052 -0.00075284  0.\n",
      " -0.0009678  -0.00180504 -0.0012348   0.          0.          0.0007335\n",
      " -0.00154836  0.          0.          0.00141469  0.          0.\n",
      " -0.00061804  0.00200746  0.         -0.00096393  0.          0.00128267\n",
      "  0.00075738  0.         -0.00182463 -0.0009678   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.00093917\n",
      " -0.00126482  0.         -0.0012699   0.         -0.00073497 -0.00124473\n",
      "  0.          0.00242457  0.          0.00125725  0.00095052  0.00075738\n",
      " -0.00125977  0.         -0.00075284 -0.00092982 -0.00122006  0.00137749\n",
      " -0.00074983  0.          0.          0.0012171   0.00075738  0.\n",
      "  0.          0.00123727  0.00152365 -0.00219306  0.         -0.00128011\n",
      " -0.00203581  0.00127755  0.00206861  0.00093542 -0.00154836  0.\n",
      "  0.00079292 -0.0012699   0.00124224  0.00092055  0.00093917 -0.00073497], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.         -0.03505207 -0.02104938  0.         -0.01053566  0.\n",
      " -0.02173455 -0.02113383  0.012554    0.          0.01600983  0.03519376\n",
      "  0.         -0.02130375  0.          0.01600983  0.          0.0308131\n",
      "  0.          0.          0.02177811  0.          0.01653096 -0.01283353\n",
      "  0.02117618 -0.0216477   0.02126114 -0.01359825  0.          0.\n",
      "  0.012554    0.01607406  0.         -0.01591397 -0.01636631 -0.01288502\n",
      "  0.          0.          0.01562979  0.         -0.02156119  0.\n",
      " -0.0219093   0.01562979 -0.02130375  0.02832861  0.01626831 -0.0219093\n",
      "  0.01562979 -0.01636631 -0.01257916  0.03498028  0.          0.\n",
      " -0.01288502 -0.02104938  0.03526549  0.01626831 -0.01288502  0.\n",
      " -0.01656409 -0.03089369 -0.02113383  0.          0.          0.012554\n",
      " -0.02650042  0.          0.          0.02421271  0.          0.\n",
      " -0.01057793  0.0343581   0.         -0.0164979   0.          0.02195321\n",
      "  0.01296264  0.         -0.03122882 -0.01656409  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01607406\n",
      " -0.0216477   0.         -0.02173455  0.         -0.01257916 -0.02130375\n",
      "  0.          0.04149692  0.          0.02151807  0.01626831  0.01296264\n",
      " -0.02156119  0.         -0.01288502 -0.01591397 -0.02088149  0.02357606\n",
      " -0.01283353  0.          0.          0.02083091  0.01296264  0.\n",
      "  0.          0.02117618  0.02607765 -0.03753464  0.         -0.0219093\n",
      " -0.03484323  0.02186548  0.0354047   0.01600983 -0.02650042  0.\n",
      "  0.01357106 -0.02173455  0.02126114  0.01575546  0.01607406 -0.01257916], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.         -0.03513339 -0.02109821  0.         -0.0105601   0.\n",
      " -0.02178497 -0.02118286  0.01258313  0.          0.01604697  0.03527541\n",
      "  0.         -0.02135317  0.          0.01604697  0.          0.03088458\n",
      "  0.          0.          0.02182863  0.          0.01656931 -0.01286331\n",
      "  0.02122531 -0.02169792  0.02131047 -0.0136298   0.          0.\n",
      "  0.01258313  0.01611136  0.         -0.01595088 -0.01640428 -0.01291491\n",
      "  0.          0.          0.01566605  0.         -0.02161121  0.\n",
      " -0.02196013  0.01566605 -0.02135317  0.02839434  0.01630605 -0.02196013\n",
      "  0.01566605 -0.01640428 -0.01260834  0.03506144  0.          0.\n",
      " -0.01291491 -0.02109821  0.0353473   0.01630605 -0.01291491  0.\n",
      " -0.01660252 -0.03096536 -0.02118286  0.          0.          0.01258313\n",
      " -0.0265619   0.          0.          0.02426888  0.          0.\n",
      " -0.01060247  0.0344378   0.         -0.01653617  0.          0.02200414\n",
      "  0.01299271  0.         -0.03130126 -0.01660252  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01611136\n",
      " -0.02169792  0.         -0.02178497  0.         -0.01260834 -0.02135317\n",
      "  0.          0.04159319  0.          0.02156799  0.01630605  0.01299271\n",
      " -0.02161121  0.         -0.01291491 -0.01595088 -0.02092993  0.02363076\n",
      " -0.01286331  0.          0.          0.02087924  0.01299271  0.\n",
      "  0.          0.02122531  0.02613815 -0.03762172  0.         -0.02196013\n",
      " -0.03492406  0.02191621  0.03548684  0.01604697 -0.0265619   0.\n",
      "  0.01360254 -0.02178497  0.02131047  0.01579201  0.01611136 -0.01260834], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.         -0.03513359 -0.02109833  0.         -0.01056016  0.\n",
      " -0.0217851  -0.02118298  0.0125832   0.          0.01604707  0.03527561\n",
      "  0.         -0.0213533   0.          0.01604707  0.          0.03088476\n",
      "  0.          0.          0.02182875  0.          0.01656941 -0.01286338\n",
      "  0.02122543 -0.02169804  0.02131059 -0.01362988  0.          0.\n",
      "  0.0125832   0.01611145  0.         -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.          0.01566614  0.         -0.02161134  0.\n",
      " -0.02196025  0.01566614 -0.0213533   0.0283945   0.01630614 -0.02196025\n",
      "  0.01566614 -0.01640437 -0.01260841  0.03506164  0.          0.\n",
      " -0.01291499 -0.02109833  0.0353475   0.01630614 -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.          0.          0.0125832\n",
      " -0.02656205  0.          0.          0.02426902  0.          0.\n",
      " -0.01060253  0.034438    0.         -0.01653627  0.          0.02200426\n",
      "  0.01299279  0.         -0.03130144 -0.01660261  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01611145\n",
      " -0.02169804  0.         -0.0217851   0.         -0.01260841 -0.0213533\n",
      "  0.          0.04159342  0.          0.02156812  0.01630614  0.01299279\n",
      " -0.02161134  0.         -0.01291499 -0.01595098 -0.02093005  0.02363089\n",
      " -0.01286338  0.          0.          0.02087936  0.01299279  0.\n",
      "  0.          0.02122543  0.02613829 -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.02191633  0.03548704  0.01604707 -0.02656205  0.\n",
      "  0.01360262 -0.0217851   0.02131059  0.0157921   0.01611145 -0.01260841], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.         -0.03513359 -0.02109833  0.         -0.01056016  0.\n",
      " -0.0217851  -0.02118298  0.0125832   0.          0.01604707  0.03527561\n",
      "  0.         -0.0213533   0.          0.01604707  0.          0.03088476\n",
      "  0.          0.          0.02182875  0.          0.01656941 -0.01286338\n",
      "  0.02122543 -0.02169804  0.02131059 -0.01362988  0.          0.\n",
      "  0.0125832   0.01611145  0.         -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.          0.01566614  0.         -0.02161134  0.\n",
      " -0.02196025  0.01566614 -0.0213533   0.0283945   0.01630615 -0.02196025\n",
      "  0.01566614 -0.01640437 -0.01260841  0.03506164  0.          0.\n",
      " -0.01291499 -0.02109833  0.0353475   0.01630615 -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.          0.          0.0125832\n",
      " -0.02656205  0.          0.          0.02426902  0.          0.\n",
      " -0.01060253  0.034438    0.         -0.01653627  0.          0.02200426\n",
      "  0.01299279  0.         -0.03130144 -0.01660261  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01611145\n",
      " -0.02169804  0.         -0.0217851   0.         -0.01260841 -0.0213533\n",
      "  0.          0.04159342  0.          0.02156812  0.01630615  0.01299279\n",
      " -0.02161134  0.         -0.01291499 -0.01595098 -0.02093005  0.02363089\n",
      " -0.01286338  0.          0.          0.02087936  0.01299279  0.\n",
      "  0.          0.02122543  0.02613829 -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.02191633  0.03548704  0.01604707 -0.02656205  0.\n",
      "  0.01360262 -0.0217851   0.02131059  0.0157921   0.01611145 -0.01260841], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.         -0.03513359 -0.02109833  0.         -0.01056016  0.\n",
      " -0.0217851  -0.02118298  0.0125832   0.          0.01604707  0.03527561\n",
      "  0.         -0.0213533   0.          0.01604707  0.          0.03088476\n",
      "  0.          0.          0.02182875  0.          0.01656941 -0.01286338\n",
      "  0.02122543 -0.02169804  0.02131059 -0.01362988  0.          0.\n",
      "  0.0125832   0.01611145  0.         -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.          0.01566614  0.         -0.02161134  0.\n",
      " -0.02196025  0.01566614 -0.0213533   0.0283945   0.01630615 -0.02196025\n",
      "  0.01566614 -0.01640437 -0.01260841  0.03506164  0.          0.\n",
      " -0.01291499 -0.02109833  0.0353475   0.01630615 -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.          0.          0.0125832\n",
      " -0.02656205  0.          0.          0.02426902  0.          0.\n",
      " -0.01060253  0.034438    0.         -0.01653627  0.          0.02200426\n",
      "  0.01299279  0.         -0.03130144 -0.01660261  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01611145\n",
      " -0.02169804  0.         -0.0217851   0.         -0.01260841 -0.0213533\n",
      "  0.          0.04159342  0.          0.02156812  0.01630615  0.01299279\n",
      " -0.02161134  0.         -0.01291499 -0.01595098 -0.02093005  0.02363089\n",
      " -0.01286338  0.          0.          0.02087936  0.01299279  0.\n",
      "  0.          0.02122543  0.02613829 -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.02191633  0.03548704  0.01604707 -0.02656205  0.\n",
      "  0.01360262 -0.0217851   0.02131059  0.0157921   0.01611145 -0.01260841], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.         -0.03513359 -0.02109833  0.         -0.01056016  0.\n",
      " -0.0217851  -0.02118298  0.0125832   0.          0.01604707  0.03527561\n",
      "  0.         -0.0213533   0.          0.01604707  0.          0.03088476\n",
      "  0.          0.          0.02182875  0.          0.01656941 -0.01286338\n",
      "  0.02122543 -0.02169804  0.02131059 -0.01362988  0.          0.\n",
      "  0.0125832   0.01611145  0.         -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.          0.01566614  0.         -0.02161134  0.\n",
      " -0.02196025  0.01566614 -0.0213533   0.0283945   0.01630615 -0.02196025\n",
      "  0.01566614 -0.01640437 -0.01260841  0.03506164  0.          0.\n",
      " -0.01291499 -0.02109833  0.0353475   0.01630615 -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.          0.          0.0125832\n",
      " -0.02656205  0.          0.          0.02426902  0.          0.\n",
      " -0.01060253  0.034438    0.         -0.01653627  0.          0.02200426\n",
      "  0.01299279  0.         -0.03130144 -0.01660261  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01611145\n",
      " -0.02169804  0.         -0.0217851   0.         -0.01260841 -0.0213533\n",
      "  0.          0.04159342  0.          0.02156812  0.01630615  0.01299279\n",
      " -0.02161134  0.         -0.01291499 -0.01595098 -0.02093005  0.02363089\n",
      " -0.01286338  0.          0.          0.02087936  0.01299279  0.\n",
      "  0.          0.02122543  0.02613829 -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.02191633  0.03548704  0.01604707 -0.02656205  0.\n",
      "  0.01360262 -0.0217851   0.02131059  0.0157921   0.01611145 -0.01260841], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.         -0.03513359 -0.02109833  0.         -0.01056016  0.\n",
      " -0.0217851  -0.02118298  0.0125832   0.          0.01604707  0.03527561\n",
      "  0.         -0.0213533   0.          0.01604707  0.          0.03088476\n",
      "  0.          0.          0.02182875  0.          0.01656941 -0.01286338\n",
      "  0.02122543 -0.02169804  0.02131059 -0.01362988  0.          0.\n",
      "  0.0125832   0.01611145  0.         -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.          0.01566614  0.         -0.02161134  0.\n",
      " -0.02196025  0.01566614 -0.0213533   0.0283945   0.01630615 -0.02196025\n",
      "  0.01566614 -0.01640437 -0.01260841  0.03506164  0.          0.\n",
      " -0.01291499 -0.02109833  0.0353475   0.01630615 -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.          0.          0.0125832\n",
      " -0.02656205  0.          0.          0.02426902  0.          0.\n",
      " -0.01060253  0.034438    0.         -0.01653627  0.          0.02200426\n",
      "  0.01299279  0.         -0.03130144 -0.01660261  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01611145\n",
      " -0.02169804  0.         -0.0217851   0.         -0.01260841 -0.0213533\n",
      "  0.          0.04159342  0.          0.02156812  0.01630615  0.01299279\n",
      " -0.02161134  0.         -0.01291499 -0.01595098 -0.02093005  0.02363089\n",
      " -0.01286338  0.          0.          0.02087936  0.01299279  0.\n",
      "  0.          0.02122543  0.02613829 -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.02191633  0.03548704  0.01604707 -0.02656205  0.\n",
      "  0.01360262 -0.0217851   0.02131059  0.0157921   0.01611145 -0.01260841], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.         -0.03513359 -0.02109833  0.         -0.01056016  0.\n",
      " -0.0217851  -0.02118298  0.0125832   0.          0.01604707  0.03527561\n",
      "  0.         -0.0213533   0.          0.01604707  0.          0.03088476\n",
      "  0.          0.          0.02182875  0.          0.01656941 -0.01286338\n",
      "  0.02122543 -0.02169804  0.02131059 -0.01362988  0.          0.\n",
      "  0.0125832   0.01611145  0.         -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.          0.01566614  0.         -0.02161134  0.\n",
      " -0.02196025  0.01566614 -0.0213533   0.0283945   0.01630615 -0.02196025\n",
      "  0.01566614 -0.01640437 -0.01260841  0.03506164  0.          0.\n",
      " -0.01291499 -0.02109833  0.0353475   0.01630615 -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.          0.          0.0125832\n",
      " -0.02656205  0.          0.          0.02426902  0.          0.\n",
      " -0.01060253  0.034438    0.         -0.01653627  0.          0.02200426\n",
      "  0.01299279  0.         -0.03130144 -0.01660261  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01611145\n",
      " -0.02169804  0.         -0.0217851   0.         -0.01260841 -0.0213533\n",
      "  0.          0.04159342  0.          0.02156812  0.01630615  0.01299279\n",
      " -0.02161134  0.         -0.01291499 -0.01595098 -0.02093005  0.02363089\n",
      " -0.01286338  0.          0.          0.02087936  0.01299279  0.\n",
      "  0.          0.02122543  0.02613829 -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.02191633  0.03548704  0.01604707 -0.02656205  0.\n",
      "  0.01360262 -0.0217851   0.02131059  0.0157921   0.01611145 -0.01260841], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.         -0.03513359 -0.02109833  0.         -0.01056016  0.\n",
      " -0.0217851  -0.02118298  0.0125832   0.          0.01604707  0.03527561\n",
      "  0.         -0.0213533   0.          0.01604707  0.          0.03088476\n",
      "  0.          0.          0.02182875  0.          0.01656941 -0.01286338\n",
      "  0.02122543 -0.02169804  0.02131059 -0.01362988  0.          0.\n",
      "  0.0125832   0.01611145  0.         -0.01595098 -0.01640437 -0.01291499\n",
      "  0.          0.          0.01566614  0.         -0.02161134  0.\n",
      " -0.02196025  0.01566614 -0.0213533   0.0283945   0.01630615 -0.02196025\n",
      "  0.01566614 -0.01640437 -0.01260841  0.03506164  0.          0.\n",
      " -0.01291499 -0.02109833  0.0353475   0.01630615 -0.01291499  0.\n",
      " -0.01660261 -0.03096553 -0.02118298  0.          0.          0.0125832\n",
      " -0.02656205  0.          0.          0.02426902  0.          0.\n",
      " -0.01060253  0.034438    0.         -0.01653627  0.          0.02200426\n",
      "  0.01299279  0.         -0.03130144 -0.01660261  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01611145\n",
      " -0.02169804  0.         -0.0217851   0.         -0.01260841 -0.0213533\n",
      "  0.          0.04159342  0.          0.02156812  0.01630615  0.01299279\n",
      " -0.02161134  0.         -0.01291499 -0.01595098 -0.02093005  0.02363089\n",
      " -0.01286338  0.          0.          0.02087936  0.01299279  0.\n",
      "  0.          0.02122543  0.02613829 -0.03762193  0.         -0.02196025\n",
      " -0.03492426  0.02191633  0.03548704  0.01604707 -0.02656205  0.\n",
      "  0.01360262 -0.0217851   0.02131059  0.0157921   0.01611145 -0.01260841], Bias: 0.0\n",
      "Classifier (1, 2) predictions: [ 1. -1.  1.  1. -1.  1.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.\n",
      "  1. -1.  1.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.\n",
      "  1. -1.  0.  1. -1.  1.  1. -1.  1.  1. -1.  0.]\n",
      "Classifier (1, 3) predictions: [ 1.  0. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  1. -1.  1.  1. -1.  1.  0. -1.]\n",
      "Classifier (2, 3) predictions: [-1.  1. -1.  0.  1. -1.  1.  1. -1.  1.  1. -1. -1.  1. -1.  0.  1. -1.\n",
      "  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1. -1.  1. -1.  0.  1. -1.\n",
      "  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.]\n",
      "Votes: [[2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 1. 0.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 1. 0.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]]\n",
      "Final predictions: [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1\n",
      " 2 3 1 2 3 1 2 3 1 2 3]\n",
      "Predictions: [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1\n",
      " 2 3 1 2 3 1 2 3 1 2 3]\n",
      "Actual Labels: [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1\n",
      " 2 3 1 2 3 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_list = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    # Remove numbers and special symbols\n",
    "    filtered_list = [w for w in filtered_list if w.isalnum() and not w.isdigit()]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_list = [lemmatizer.lemmatize(w, 'v') for w in filtered_list]\n",
    "    \n",
    "    return ' '.join(lemmatized_list)\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    vocabulary = list(set(word for document in corpus for word in document.split()))\n",
    "    vocabulary.sort()\n",
    "    tfidf_matrix = np.zeros((len(corpus), len(vocabulary)))\n",
    "\n",
    "    for i, document in enumerate(corpus):\n",
    "        word_counts = Counter(document.split())\n",
    "        for word, count in word_counts.items():\n",
    "            tf = count / len(document.split())\n",
    "            idf = log(len(corpus) / sum(1 for doc in corpus if word in doc.split()))\n",
    "            tfidf_matrix[i, vocabulary.index(word)] = tf * idf\n",
    "\n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Binary SVM Classifier\n",
    "class BinarySVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for iteration in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights - np.dot(x_i, y[idx]))\n",
    "                    self.bias -= self.learning_rate * y[idx]\n",
    "\n",
    "            # Debugging: print weights and bias at every 100 iterations\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}: Weights: {self.weights}, Bias: {self.bias}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) - self.bias)\n",
    "\n",
    "# One-vs-One SVM Classifier\n",
    "class OneVsOneSVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        n_classes = len(unique_classes)\n",
    "        print(f\"Unique classes: {unique_classes}\")  # Debugging line\n",
    "        for i in range(n_classes):\n",
    "            for j in range(i + 1, n_classes):\n",
    "                class_i = unique_classes[i]\n",
    "                class_j = unique_classes[j]\n",
    "\n",
    "                # Filter the data for the two classes\n",
    "                idx = np.where((y == class_i) | (y == class_j))\n",
    "                X_filtered = X[idx]\n",
    "                y_filtered = y[idx]\n",
    "\n",
    "                # Convert class labels to +1 and -1\n",
    "                y_filtered = np.where(y_filtered == class_i, 1, -1)\n",
    "\n",
    "                # Train the binary classifier\n",
    "                clf = BinarySVM(C=self.C, learning_rate=self.learning_rate, n_iters=self.n_iters)\n",
    "                clf.fit(X_filtered, y_filtered)\n",
    "                self.classifiers.append((clf, (class_i, class_j)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = len(np.unique(y))\n",
    "        votes = np.zeros((X.shape[0], n_classes))\n",
    "\n",
    "        for clf, class_labels in self.classifiers:\n",
    "            predictions = clf.predict(X)\n",
    "            print(f\"Classifier {class_labels} predictions: {predictions}\")  # Debugging line\n",
    "            for idx, pred in enumerate(predictions):\n",
    "                if pred == 1:\n",
    "                    votes[idx, class_labels[0] - 1] += 1  # Adjust index for zero-based array\n",
    "                else:\n",
    "                    votes[idx, class_labels[1] - 1] += 1  # Adjust index for zero-based array\n",
    "\n",
    "        # Return the class with the most votes\n",
    "        final_predictions = np.argmax(votes, axis=1) + 1  # Adjust index to match class labels starting from 1\n",
    "        print(f\"Votes: {votes}\")  # Debugging line\n",
    "        print(f\"Final predictions: {final_predictions}\")  # Debugging line\n",
    "        return final_predictions\n",
    "\n",
    "# Sample dataset\n",
    "news_articles = [\n",
    "    \"government passes new law\",                     # Politics\n",
    "    \"football match ends in draw\",                    # Sports\n",
    "    \"new technology in smartphones\",                  # Technology\n",
    "    \"politician gives a speech\",                      # Politics\n",
    "    \"sports event attracts large crowd\",              # Sports\n",
    "    \"new tech gadgets released this year\",            # Technology\n",
    "    \"election results announced\",                     # Politics\n",
    "    \"soccer team wins championship\",                  # Sports\n",
    "    \"AI advancements in healthcare\",                  # Technology\n",
    "    \"local council meeting updates\",                  # Politics\n",
    "    \"basketball game highlights\",                     # Sports\n",
    "    \"innovations in artificial intelligence\",         # Technology\n",
    "    \"politician's new policy proposal\",               # Politics\n",
    "    \"swimming competition results\",                   # Sports\n",
    "    \"latest trends in smartphone design\",             # Technology\n",
    "    \"government budget allocation review\",            # Politics\n",
    "    \"volleyball tournament concludes\",                # Sports\n",
    "    \"breakthrough in renewable energy\",               # Technology\n",
    "    \"senator's speech on climate change\",             # Politics\n",
    "    \"baseball team training camp\",                    # Sports\n",
    "    \"tech company announces new software\",            # Technology\n",
    "    \"international relations summit\",                 # Politics\n",
    "    \"world cup qualifying matches\",                   # Sports\n",
    "    \"smart home devices market growth\",               # Technology\n",
    "    \"legislative bill discussion\",                    # Politics\n",
    "    \"rugby game results\",                            # Sports\n",
    "    \"technology in education sector\",                # Technology\n",
    "    \"mayoral election debate\",                        # Politics\n",
    "    \"national soccer league season start\",            # Sports\n",
    "    \"advancements in quantum computing\",              # Technology\n",
    "    \"press conference on new laws\",                   # Politics\n",
    "    \"hockey match final scores\",                      # Sports\n",
    "    \"virtual reality applications\",                   # Technology\n",
    "    \"parliamentary debate on economy\",                # Politics\n",
    "    \"college basketball championship\",                # Sports\n",
    "    \"latest trends in gadget development\",            # Technology\n",
    "    \"congressional committee meeting\",                # Politics\n",
    "    \"tennis tournament highlights\",                   # Sports\n",
    "    \"emerging technologies in fintech\",               # Technology\n",
    "    \"state of the union address\",                     # Politics\n",
    "    \"motorsports event results\",                      # Sports\n",
    "    \"new innovations in medical tech\",                # Technology\n",
    "    \"political rally speeches\",                       # Politics\n",
    "    \"community sports league updates\",                # Sports\n",
    "    \"tech industry conference news\",                  # Technology\n",
    "    \"government response to natural disaster\",        # Politics\n",
    "    \"annual sports awards ceremony\",                  # Sports\n",
    "    \"technological impacts on job market\"             # Technology\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3   # Technology\n",
    "]\n",
    "\n",
    "# Convert news articles to TF-IDF features\n",
    "preprocessed_articles = [preprocess_text(article) for article in news_articles]\n",
    "X, vocabulary = compute_tfidf(preprocessed_articles)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize and train the One-vs-One SVM\n",
    "ovo_svm = OneVsOneSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "ovo_svm.fit(X, y)\n",
    "\n",
    "# Test the classifier on the training data\n",
    "predictions = ovo_svm.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual Labels:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [1 2 3]\n",
      "Iteration 0: Weights: [ 0.          0.          0.          0.00093917  0.00101574  0.\n",
      "  0.          0.         -0.00074088  0.         -0.00094483 -0.00207697\n",
      "  0.00126736  0.          0.00093917 -0.00094483  0.         -0.00181845\n",
      "  0.00094294  0.00094294 -0.00128524  0.00128782  0.          0.\n",
      " -0.00124972  0.         -0.00125474  0.0007866   0.00128782  0.00093168\n",
      " -0.00074088 -0.00094862  0.00209762  0.          0.          0.\n",
      "  0.          0.00126736 -0.0009224   0.00128267  0.          0.00206035\n",
      "  0.         -0.0009224   0.         -0.00060822 -0.00096008  0.\n",
      " -0.0009224   0.          0.         -0.00206438  0.00123233  0.00133195\n",
      "  0.          0.         -0.00208121 -0.00096008  0.          0.\n",
      "  0.          0.          0.          0.00126229  0.         -0.00074088\n",
      "  0.          0.00092055  0.00095816 -0.00062802  0.00126736  0.00093168\n",
      "  0.         -0.00202766  0.00127244  0.          0.00182209  0.\n",
      " -0.000765    0.          0.00139957  0.          0.00128267  0.00092055\n",
      "  0.00093542  0.          0.00177961  0.00095816  0.00093542 -0.00094862\n",
      "  0.          0.          0.          0.00126229  0.          0.\n",
      "  0.         -0.00082313  0.00093917 -0.0012699  -0.00096008 -0.000765\n",
      "  0.          0.00094294  0.          0.          0.         -0.00139135\n",
      "  0.          0.00178578  0.         -0.00053062 -0.000765    0.\n",
      "  0.00126229 -0.00124972 -0.00153898  0.          0.          0.\n",
      "  0.         -0.0012904  -0.00208942 -0.00094483  0.          0.\n",
      "  0.00076486  0.         -0.00125474 -0.00092982 -0.00094862  0.        ], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.          0.          0.          0.01842007  0.01992181  0.\n",
      "  0.          0.         -0.01453099  0.         -0.01853104 -0.04073602\n",
      "  0.02485689  0.          0.01842007 -0.01853104  0.         -0.0356655\n",
      "  0.01849398  0.01849398 -0.02520769  0.02525821  0.          0.\n",
      " -0.02451098  0.         -0.02460932  0.01542771  0.02525821  0.01827316\n",
      " -0.01453099 -0.01860539  0.04114089  0.          0.          0.\n",
      "  0.          0.02485689 -0.01809115  0.02515728  0.          0.0404099\n",
      "  0.         -0.01809115  0.         -0.01192918 -0.01883022  0.\n",
      " -0.01809115  0.          0.         -0.04048893  0.02416988  0.02612375\n",
      "  0.          0.         -0.04081905 -0.01883022  0.          0.\n",
      "  0.          0.          0.          0.02475757  0.         -0.01453099\n",
      "  0.          0.01805497  0.01879256 -0.01231748  0.02485689  0.01827316\n",
      "  0.         -0.03976876  0.02495662  0.          0.03573698  0.\n",
      " -0.01500398  0.          0.02745001  0.          0.02515728  0.01805497\n",
      "  0.01834647  0.          0.0349037   0.01879256  0.01834647 -0.01860539\n",
      "  0.          0.          0.          0.02475757  0.          0.\n",
      "  0.         -0.01614421  0.01842007 -0.02490671 -0.01883022 -0.01500398\n",
      "  0.          0.01849398  0.          0.          0.         -0.02728879\n",
      "  0.          0.0350248   0.         -0.01040722 -0.01500398  0.\n",
      "  0.02475757 -0.02451098 -0.03018432  0.          0.          0.\n",
      "  0.         -0.02530883 -0.04098019 -0.01853104  0.          0.\n",
      "  0.01500131  0.         -0.02460932 -0.01823661 -0.01860539  0.        ], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.          0.          0.          0.01851601  0.02002557  0.\n",
      "  0.          0.         -0.01460667  0.         -0.01862755 -0.04094818\n",
      "  0.02498635  0.          0.01851601 -0.01862755  0.         -0.03585125\n",
      "  0.0185903   0.0185903  -0.02533898  0.02538976  0.          0.\n",
      " -0.02463864  0.         -0.02473749  0.01550806  0.02538976  0.01836832\n",
      " -0.01460667 -0.01870228  0.04135516  0.          0.          0.\n",
      "  0.          0.02498635 -0.01818537  0.0252883   0.          0.04062036\n",
      "  0.         -0.01818537  0.         -0.01199131 -0.01892829  0.\n",
      " -0.01818537  0.          0.         -0.0406998   0.02429576  0.02625981\n",
      "  0.          0.         -0.04103164 -0.01892829  0.          0.\n",
      "  0.          0.          0.          0.02488651  0.         -0.01460667\n",
      "  0.          0.018149    0.01889043 -0.01238164  0.02498635  0.01836832\n",
      "  0.         -0.03997588  0.0250866   0.          0.0359231   0.\n",
      " -0.01508212  0.          0.02759297  0.          0.0252883   0.018149\n",
      "  0.01844202  0.          0.03508548  0.01889043  0.01844202 -0.01870228\n",
      "  0.          0.          0.          0.02488651  0.          0.\n",
      "  0.         -0.01622829  0.01851601 -0.02503643 -0.01892829 -0.01508212\n",
      "  0.          0.0185903   0.          0.          0.         -0.02743091\n",
      "  0.          0.03520721  0.         -0.01046143 -0.01508212  0.\n",
      "  0.02488651 -0.02463864 -0.03034152  0.          0.          0.\n",
      "  0.         -0.02544064 -0.04119362 -0.01862755  0.          0.\n",
      "  0.01507943  0.         -0.02473749 -0.01833159 -0.01870228  0.        ], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.          0.          0.          0.01851653  0.02002614  0.\n",
      "  0.          0.         -0.01460708  0.         -0.01862808 -0.04094935\n",
      "  0.02498706  0.          0.01851653 -0.01862808  0.         -0.03585227\n",
      "  0.01859082  0.01859082 -0.0253397   0.02539048  0.          0.\n",
      " -0.02463934  0.         -0.02473819  0.0155085   0.02539048  0.01836885\n",
      " -0.01460708 -0.01870282  0.04135634  0.          0.          0.\n",
      "  0.          0.02498706 -0.01818589  0.02528902  0.          0.04062151\n",
      "  0.         -0.01818589  0.         -0.01199165 -0.01892883  0.\n",
      " -0.01818589  0.          0.         -0.04070096  0.02429645  0.02626056\n",
      "  0.          0.         -0.04103281 -0.01892883  0.          0.\n",
      "  0.          0.          0.          0.02488721  0.         -0.01460708\n",
      "  0.          0.01814952  0.01889097 -0.01238199  0.02498706  0.01836885\n",
      "  0.         -0.03997702  0.02508731  0.          0.03592412  0.\n",
      " -0.01508255  0.          0.02759376  0.          0.02528902  0.01814952\n",
      "  0.01844254  0.          0.03508648  0.01889097  0.01844254 -0.01870282\n",
      "  0.          0.          0.          0.02488721  0.          0.\n",
      "  0.         -0.01622875  0.01851653 -0.02503714 -0.01892883 -0.01508255\n",
      "  0.          0.01859082  0.          0.          0.         -0.02743169\n",
      "  0.          0.03520821  0.         -0.01046172 -0.01508255  0.\n",
      "  0.02488721 -0.02463934 -0.03034238  0.          0.          0.\n",
      "  0.         -0.02544136 -0.04119479 -0.01862808  0.          0.\n",
      "  0.01507986  0.         -0.02473819 -0.01833211 -0.01870282  0.        ], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.          0.          0.          0.01851654  0.02002614  0.\n",
      "  0.          0.         -0.01460709  0.         -0.01862808 -0.04094935\n",
      "  0.02498707  0.          0.01851654 -0.01862808  0.         -0.03585228\n",
      "  0.01859083  0.01859083 -0.0253397   0.02539048  0.          0.\n",
      " -0.02463934  0.         -0.02473819  0.0155085   0.02539048  0.01836885\n",
      " -0.01460709 -0.01870282  0.04135634  0.          0.          0.\n",
      "  0.          0.02498707 -0.01818589  0.02528902  0.          0.04062152\n",
      "  0.         -0.01818589  0.         -0.01199165 -0.01892883  0.\n",
      " -0.01818589  0.          0.         -0.04070097  0.02429645  0.02626056\n",
      "  0.          0.         -0.04103281 -0.01892883  0.          0.\n",
      "  0.          0.          0.          0.02488722  0.         -0.01460709\n",
      "  0.          0.01814952  0.01889097 -0.01238199  0.02498707  0.01836885\n",
      "  0.         -0.03997702  0.02508732  0.          0.03592413  0.\n",
      " -0.01508255  0.          0.02759376  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255 -0.01870282\n",
      "  0.          0.          0.          0.02488722  0.          0.\n",
      "  0.         -0.01622876  0.01851654 -0.02503714 -0.01892883 -0.01508255\n",
      "  0.          0.01859083  0.          0.          0.         -0.0274317\n",
      "  0.          0.03520822  0.         -0.01046172 -0.01508255  0.\n",
      "  0.02488722 -0.02463934 -0.03034239  0.          0.          0.\n",
      "  0.         -0.02544136 -0.0411948  -0.01862808  0.          0.\n",
      "  0.01507987  0.         -0.02473819 -0.01833211 -0.01870282  0.        ], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.          0.          0.          0.01851654  0.02002614  0.\n",
      "  0.          0.         -0.01460709  0.         -0.01862808 -0.04094935\n",
      "  0.02498707  0.          0.01851654 -0.01862808  0.         -0.03585228\n",
      "  0.01859083  0.01859083 -0.0253397   0.02539048  0.          0.\n",
      " -0.02463934  0.         -0.02473819  0.0155085   0.02539048  0.01836885\n",
      " -0.01460709 -0.01870282  0.04135634  0.          0.          0.\n",
      "  0.          0.02498707 -0.01818589  0.02528902  0.          0.04062152\n",
      "  0.         -0.01818589  0.         -0.01199165 -0.01892883  0.\n",
      " -0.01818589  0.          0.         -0.04070097  0.02429645  0.02626056\n",
      "  0.          0.         -0.04103281 -0.01892883  0.          0.\n",
      "  0.          0.          0.          0.02488722  0.         -0.01460709\n",
      "  0.          0.01814952  0.01889097 -0.01238199  0.02498707  0.01836885\n",
      "  0.         -0.03997702  0.02508732  0.          0.03592413  0.\n",
      " -0.01508255  0.          0.02759376  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255 -0.01870282\n",
      "  0.          0.          0.          0.02488722  0.          0.\n",
      "  0.         -0.01622876  0.01851654 -0.02503714 -0.01892883 -0.01508255\n",
      "  0.          0.01859083  0.          0.          0.         -0.0274317\n",
      "  0.          0.03520822  0.         -0.01046172 -0.01508255  0.\n",
      "  0.02488722 -0.02463934 -0.03034239  0.          0.          0.\n",
      "  0.         -0.02544136 -0.0411948  -0.01862808  0.          0.\n",
      "  0.01507987  0.         -0.02473819 -0.01833211 -0.01870282  0.        ], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.          0.          0.          0.01851654  0.02002614  0.\n",
      "  0.          0.         -0.01460709  0.         -0.01862808 -0.04094935\n",
      "  0.02498707  0.          0.01851654 -0.01862808  0.         -0.03585228\n",
      "  0.01859083  0.01859083 -0.0253397   0.02539048  0.          0.\n",
      " -0.02463934  0.         -0.02473819  0.0155085   0.02539048  0.01836885\n",
      " -0.01460709 -0.01870282  0.04135634  0.          0.          0.\n",
      "  0.          0.02498707 -0.01818589  0.02528902  0.          0.04062152\n",
      "  0.         -0.01818589  0.         -0.01199165 -0.01892883  0.\n",
      " -0.01818589  0.          0.         -0.04070097  0.02429645  0.02626056\n",
      "  0.          0.         -0.04103281 -0.01892883  0.          0.\n",
      "  0.          0.          0.          0.02488722  0.         -0.01460709\n",
      "  0.          0.01814952  0.01889097 -0.01238199  0.02498707  0.01836885\n",
      "  0.         -0.03997702  0.02508732  0.          0.03592413  0.\n",
      " -0.01508255  0.          0.02759376  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255 -0.01870282\n",
      "  0.          0.          0.          0.02488722  0.          0.\n",
      "  0.         -0.01622876  0.01851654 -0.02503714 -0.01892883 -0.01508255\n",
      "  0.          0.01859083  0.          0.          0.         -0.0274317\n",
      "  0.          0.03520822  0.         -0.01046172 -0.01508255  0.\n",
      "  0.02488722 -0.02463934 -0.03034239  0.          0.          0.\n",
      "  0.         -0.02544136 -0.0411948  -0.01862808  0.          0.\n",
      "  0.01507987  0.         -0.02473819 -0.01833211 -0.01870282  0.        ], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.          0.          0.          0.01851654  0.02002614  0.\n",
      "  0.          0.         -0.01460709  0.         -0.01862808 -0.04094935\n",
      "  0.02498707  0.          0.01851654 -0.01862808  0.         -0.03585228\n",
      "  0.01859083  0.01859083 -0.0253397   0.02539048  0.          0.\n",
      " -0.02463934  0.         -0.02473819  0.0155085   0.02539048  0.01836885\n",
      " -0.01460709 -0.01870282  0.04135634  0.          0.          0.\n",
      "  0.          0.02498707 -0.01818589  0.02528902  0.          0.04062152\n",
      "  0.         -0.01818589  0.         -0.01199165 -0.01892883  0.\n",
      " -0.01818589  0.          0.         -0.04070097  0.02429645  0.02626056\n",
      "  0.          0.         -0.04103281 -0.01892883  0.          0.\n",
      "  0.          0.          0.          0.02488722  0.         -0.01460709\n",
      "  0.          0.01814952  0.01889097 -0.01238199  0.02498707  0.01836885\n",
      "  0.         -0.03997702  0.02508732  0.          0.03592413  0.\n",
      " -0.01508255  0.          0.02759376  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255 -0.01870282\n",
      "  0.          0.          0.          0.02488722  0.          0.\n",
      "  0.         -0.01622876  0.01851654 -0.02503714 -0.01892883 -0.01508255\n",
      "  0.          0.01859083  0.          0.          0.         -0.0274317\n",
      "  0.          0.03520822  0.         -0.01046172 -0.01508255  0.\n",
      "  0.02488722 -0.02463934 -0.03034239  0.          0.          0.\n",
      "  0.         -0.02544136 -0.0411948  -0.01862808  0.          0.\n",
      "  0.01507987  0.         -0.02473819 -0.01833211 -0.01870282  0.        ], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.          0.          0.          0.01851654  0.02002614  0.\n",
      "  0.          0.         -0.01460709  0.         -0.01862808 -0.04094935\n",
      "  0.02498707  0.          0.01851654 -0.01862808  0.         -0.03585228\n",
      "  0.01859083  0.01859083 -0.0253397   0.02539048  0.          0.\n",
      " -0.02463934  0.         -0.02473819  0.0155085   0.02539048  0.01836885\n",
      " -0.01460709 -0.01870282  0.04135634  0.          0.          0.\n",
      "  0.          0.02498707 -0.01818589  0.02528902  0.          0.04062152\n",
      "  0.         -0.01818589  0.         -0.01199165 -0.01892883  0.\n",
      " -0.01818589  0.          0.         -0.04070097  0.02429645  0.02626056\n",
      "  0.          0.         -0.04103281 -0.01892883  0.          0.\n",
      "  0.          0.          0.          0.02488722  0.         -0.01460709\n",
      "  0.          0.01814952  0.01889097 -0.01238199  0.02498707  0.01836885\n",
      "  0.         -0.03997702  0.02508732  0.          0.03592413  0.\n",
      " -0.01508255  0.          0.02759376  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255 -0.01870282\n",
      "  0.          0.          0.          0.02488722  0.          0.\n",
      "  0.         -0.01622876  0.01851654 -0.02503714 -0.01892883 -0.01508255\n",
      "  0.          0.01859083  0.          0.          0.         -0.0274317\n",
      "  0.          0.03520822  0.         -0.01046172 -0.01508255  0.\n",
      "  0.02488722 -0.02463934 -0.03034239  0.          0.          0.\n",
      "  0.         -0.02544136 -0.0411948  -0.01862808  0.          0.\n",
      "  0.01507987  0.         -0.02473819 -0.01833211 -0.01870282  0.        ], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.          0.          0.          0.01851654  0.02002614  0.\n",
      "  0.          0.         -0.01460709  0.         -0.01862808 -0.04094935\n",
      "  0.02498707  0.          0.01851654 -0.01862808  0.         -0.03585228\n",
      "  0.01859083  0.01859083 -0.0253397   0.02539048  0.          0.\n",
      " -0.02463934  0.         -0.02473819  0.0155085   0.02539048  0.01836885\n",
      " -0.01460709 -0.01870282  0.04135634  0.          0.          0.\n",
      "  0.          0.02498707 -0.01818589  0.02528902  0.          0.04062152\n",
      "  0.         -0.01818589  0.         -0.01199165 -0.01892883  0.\n",
      " -0.01818589  0.          0.         -0.04070097  0.02429645  0.02626056\n",
      "  0.          0.         -0.04103281 -0.01892883  0.          0.\n",
      "  0.          0.          0.          0.02488722  0.         -0.01460709\n",
      "  0.          0.01814952  0.01889097 -0.01238199  0.02498707  0.01836885\n",
      "  0.         -0.03997702  0.02508732  0.          0.03592413  0.\n",
      " -0.01508255  0.          0.02759376  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255 -0.01870282\n",
      "  0.          0.          0.          0.02488722  0.          0.\n",
      "  0.         -0.01622876  0.01851654 -0.02503714 -0.01892883 -0.01508255\n",
      "  0.          0.01859083  0.          0.          0.         -0.0274317\n",
      "  0.          0.03520822  0.         -0.01046172 -0.01508255  0.\n",
      "  0.02488722 -0.02463934 -0.03034239  0.          0.          0.\n",
      "  0.         -0.02544136 -0.0411948  -0.01862808  0.          0.\n",
      "  0.01507987  0.         -0.02473819 -0.01833211 -0.01870282  0.        ], Bias: 0.0\n",
      "Iteration 0: Weights: [ 0.00000000e+00 -2.06447521e-03 -1.23975336e-03  9.39169220e-04\n",
      "  3.95214412e-04  0.00000000e+00 -1.28010806e-03 -1.24472729e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.26735808e-03 -1.25473510e-03  9.39169220e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  9.42937197e-04  9.42937197e-04\n",
      "  0.00000000e+00  1.28781954e-03  0.00000000e+00 -7.55861480e-04\n",
      "  0.00000000e+00 -1.27499275e-03  0.00000000e+00  7.86600040e-04\n",
      "  1.28781954e-03  9.31678376e-04  0.00000000e+00  0.00000000e+00\n",
      "  2.09761692e-03 -9.37290881e-04 -9.63932923e-04 -7.58894021e-04\n",
      "  0.00000000e+00  1.26735808e-03  0.00000000e+00  1.28267341e-03\n",
      " -1.26989788e-03  2.06034626e-03 -1.29040034e-03  0.00000000e+00\n",
      " -1.25473510e-03  0.00000000e+00  0.00000000e+00 -1.29040034e-03\n",
      "  0.00000000e+00 -9.63932923e-04 -7.40879583e-04  0.00000000e+00\n",
      "  1.23232971e-03  1.33195036e-03 -7.58894021e-04 -1.23975336e-03\n",
      "  0.00000000e+00  0.00000000e+00 -7.58894021e-04  0.00000000e+00\n",
      "  0.00000000e+00 -1.02185609e-03 -1.24472729e-03  1.26229372e-03\n",
      "  0.00000000e+00  0.00000000e+00 -1.56080544e-03  9.20553987e-04\n",
      "  9.58160885e-04  0.00000000e+00  1.26735808e-03  9.31678376e-04\n",
      " -6.23012353e-04  0.00000000e+00  1.27244277e-03  0.00000000e+00\n",
      "  1.82209186e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  4.35294375e-05  0.00000000e+00  1.28267341e-03  9.20553987e-04\n",
      "  9.35416299e-04  0.00000000e+00  1.77960624e-03  9.58160885e-04\n",
      "  9.35416299e-04  0.00000000e+00 -1.27499275e-03  0.00000000e+00\n",
      " -1.28010806e-03  1.26229372e-03 -7.40879583e-04 -1.25473510e-03\n",
      "  0.00000000e+00  7.94200563e-04  9.39169220e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.26989788e-03  9.42937197e-04\n",
      " -7.58894021e-04 -9.37290881e-04 -1.22986505e-03  0.00000000e+00\n",
      " -7.55861480e-04  1.78578051e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.26229372e-03  0.00000000e+00\n",
      "  0.00000000e+00 -9.60751407e-04  0.00000000e+00 -1.29040034e-03\n",
      " -2.05217480e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.56080544e-03  0.00000000e+00  7.64859283e-04 -1.28010806e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.40879583e-04], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.         -0.04049088 -0.02431548  0.01842007  0.0077514   0.\n",
      " -0.02510696 -0.02441303  0.          0.          0.          0.\n",
      "  0.02485689 -0.02460932  0.01842007  0.          0.          0.\n",
      "  0.01849398  0.01849398  0.          0.02525821  0.         -0.01482483\n",
      "  0.         -0.02500663  0.          0.01542771  0.02525821  0.01827316\n",
      "  0.          0.          0.04114089 -0.01838323 -0.01890577 -0.01488431\n",
      "  0.          0.02485689  0.          0.02515728 -0.02490671  0.0404099\n",
      " -0.02530883  0.         -0.02460932  0.          0.         -0.02530883\n",
      "  0.         -0.01890577 -0.01453099  0.          0.02416988  0.02612375\n",
      " -0.01488431 -0.02431548  0.          0.         -0.01488431  0.\n",
      "  0.         -0.02004183 -0.02441303  0.02475757  0.          0.\n",
      " -0.03061232  0.01805497  0.01879256  0.          0.02485689  0.01827316\n",
      " -0.01221924  0.          0.02495662  0.          0.03573698  0.\n",
      "  0.          0.          0.00085375  0.          0.02515728  0.01805497\n",
      "  0.01834647  0.          0.0349037   0.01879256  0.01834647  0.\n",
      " -0.02500663  0.         -0.02510696  0.02475757 -0.01453099 -0.02460932\n",
      "  0.          0.01557678  0.01842007  0.          0.          0.\n",
      " -0.02490671  0.01849398 -0.01488431 -0.01838323 -0.02412154  0.\n",
      " -0.01482483  0.0350248   0.          0.          0.          0.\n",
      "  0.02475757  0.          0.         -0.01884337  0.         -0.02530883\n",
      " -0.04024963  0.          0.          0.         -0.03061232  0.\n",
      "  0.01500131 -0.02510696  0.          0.          0.         -0.01453099], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.         -0.04070176 -0.02444212  0.01851601  0.00779177  0.\n",
      " -0.02523772 -0.02454018  0.          0.          0.          0.\n",
      "  0.02498635 -0.02473749  0.01851601  0.          0.          0.\n",
      "  0.0185903   0.0185903   0.          0.02538976  0.         -0.01490204\n",
      "  0.         -0.02513687  0.          0.01550806  0.02538976  0.01836832\n",
      "  0.          0.          0.04135516 -0.01847898 -0.01900423 -0.01496183\n",
      "  0.          0.02498635  0.          0.0252883  -0.02503643  0.04062036\n",
      " -0.02544064  0.         -0.02473749  0.          0.         -0.02544064\n",
      "  0.         -0.01900423 -0.01460667  0.          0.02429576  0.02625981\n",
      " -0.01496183 -0.02444212  0.          0.         -0.01496183  0.\n",
      "  0.         -0.02014621 -0.02454018  0.02488651  0.          0.\n",
      " -0.03077176  0.018149    0.01889043  0.          0.02498635  0.01836832\n",
      " -0.01228288  0.          0.0250866   0.          0.0359231   0.\n",
      "  0.          0.          0.0008582   0.          0.0252883   0.018149\n",
      "  0.01844202  0.          0.03508548  0.01889043  0.01844202  0.\n",
      " -0.02513687  0.         -0.02523772  0.02488651 -0.01460667 -0.02473749\n",
      "  0.          0.01565791  0.01851601  0.          0.          0.\n",
      " -0.02503643  0.0185903  -0.01496183 -0.01847898 -0.02424717  0.\n",
      " -0.01490204  0.03520721  0.          0.          0.          0.\n",
      "  0.02488651  0.          0.         -0.01894151  0.         -0.02544064\n",
      " -0.04045925  0.          0.          0.         -0.03077176  0.\n",
      "  0.01507943 -0.02523772  0.          0.          0.         -0.01460667], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.         -0.04070292 -0.02444281  0.01851653  0.00779199  0.\n",
      " -0.02523844 -0.02454088  0.          0.          0.          0.\n",
      "  0.02498706 -0.02473819  0.01851653  0.          0.          0.\n",
      "  0.01859082  0.01859082  0.          0.02539048  0.         -0.01490246\n",
      "  0.         -0.02513759  0.          0.0155085   0.02539048  0.01836885\n",
      "  0.          0.          0.04135634 -0.0184795  -0.01900477 -0.01496225\n",
      "  0.          0.02498706  0.          0.02528902 -0.02503714  0.04062151\n",
      " -0.02544136  0.         -0.02473819  0.          0.         -0.02544136\n",
      "  0.         -0.01900477 -0.01460708  0.          0.02429645  0.02626056\n",
      " -0.01496225 -0.02444281  0.          0.         -0.01496225  0.\n",
      "  0.         -0.02014678 -0.02454088  0.02488721  0.          0.\n",
      " -0.03077263  0.01814952  0.01889097  0.          0.02498706  0.01836885\n",
      " -0.01228323  0.          0.02508731  0.          0.03592412  0.\n",
      "  0.          0.          0.00085822  0.          0.02528902  0.01814952\n",
      "  0.01844254  0.          0.03508648  0.01889097  0.01844254  0.\n",
      " -0.02513759  0.         -0.02523844  0.02488721 -0.01460708 -0.02473819\n",
      "  0.          0.01565835  0.01851653  0.          0.          0.\n",
      " -0.02503714  0.01859082 -0.01496225 -0.0184795  -0.02424785  0.\n",
      " -0.01490246  0.03520821  0.          0.          0.          0.\n",
      "  0.02488721  0.          0.         -0.01894205  0.         -0.02544136\n",
      " -0.0404604   0.          0.          0.         -0.03077263  0.\n",
      "  0.01507986 -0.02523844  0.          0.          0.         -0.01460708], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.         -0.04070292 -0.02444282  0.01851654  0.007792    0.\n",
      " -0.02523844 -0.02454088  0.          0.          0.          0.\n",
      "  0.02498707 -0.02473819  0.01851654  0.          0.          0.\n",
      "  0.01859083  0.01859083  0.          0.02539048  0.         -0.01490247\n",
      "  0.         -0.02513759  0.          0.0155085   0.02539048  0.01836885\n",
      "  0.          0.          0.04135634 -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.02498707  0.          0.02528902 -0.02503714  0.04062152\n",
      " -0.02544136  0.         -0.02473819  0.          0.         -0.02544136\n",
      "  0.         -0.01900478 -0.01460709  0.          0.02429645  0.02626056\n",
      " -0.01496226 -0.02444282  0.          0.         -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.02488722  0.          0.\n",
      " -0.03077264  0.01814952  0.01889097  0.          0.02498707  0.01836885\n",
      " -0.01228323  0.          0.02508732  0.          0.03592413  0.\n",
      "  0.          0.          0.00085822  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255  0.\n",
      " -0.02513759  0.         -0.02523844  0.02488722 -0.01460709 -0.02473819\n",
      "  0.          0.01565835  0.01851654  0.          0.          0.\n",
      " -0.02503714  0.01859083 -0.01496226 -0.0184795  -0.02424786  0.\n",
      " -0.01490247  0.03520822  0.          0.          0.          0.\n",
      "  0.02488722  0.          0.         -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.          0.          0.         -0.03077264  0.\n",
      "  0.01507987 -0.02523844  0.          0.          0.         -0.01460709], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.         -0.04070292 -0.02444282  0.01851654  0.007792    0.\n",
      " -0.02523844 -0.02454088  0.          0.          0.          0.\n",
      "  0.02498707 -0.02473819  0.01851654  0.          0.          0.\n",
      "  0.01859083  0.01859083  0.          0.02539048  0.         -0.01490247\n",
      "  0.         -0.02513759  0.          0.0155085   0.02539048  0.01836885\n",
      "  0.          0.          0.04135634 -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.02498707  0.          0.02528902 -0.02503714  0.04062152\n",
      " -0.02544136  0.         -0.02473819  0.          0.         -0.02544136\n",
      "  0.         -0.01900478 -0.01460709  0.          0.02429645  0.02626056\n",
      " -0.01496226 -0.02444282  0.          0.         -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.02488722  0.          0.\n",
      " -0.03077264  0.01814952  0.01889097  0.          0.02498707  0.01836885\n",
      " -0.01228323  0.          0.02508732  0.          0.03592413  0.\n",
      "  0.          0.          0.00085822  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255  0.\n",
      " -0.02513759  0.         -0.02523844  0.02488722 -0.01460709 -0.02473819\n",
      "  0.          0.01565835  0.01851654  0.          0.          0.\n",
      " -0.02503714  0.01859083 -0.01496226 -0.0184795  -0.02424786  0.\n",
      " -0.01490247  0.03520822  0.          0.          0.          0.\n",
      "  0.02488722  0.          0.         -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.          0.          0.         -0.03077264  0.\n",
      "  0.01507987 -0.02523844  0.          0.          0.         -0.01460709], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.         -0.04070292 -0.02444282  0.01851654  0.007792    0.\n",
      " -0.02523844 -0.02454088  0.          0.          0.          0.\n",
      "  0.02498707 -0.02473819  0.01851654  0.          0.          0.\n",
      "  0.01859083  0.01859083  0.          0.02539048  0.         -0.01490247\n",
      "  0.         -0.02513759  0.          0.0155085   0.02539048  0.01836885\n",
      "  0.          0.          0.04135634 -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.02498707  0.          0.02528902 -0.02503714  0.04062152\n",
      " -0.02544136  0.         -0.02473819  0.          0.         -0.02544136\n",
      "  0.         -0.01900478 -0.01460709  0.          0.02429645  0.02626056\n",
      " -0.01496226 -0.02444282  0.          0.         -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.02488722  0.          0.\n",
      " -0.03077264  0.01814952  0.01889097  0.          0.02498707  0.01836885\n",
      " -0.01228323  0.          0.02508732  0.          0.03592413  0.\n",
      "  0.          0.          0.00085822  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255  0.\n",
      " -0.02513759  0.         -0.02523844  0.02488722 -0.01460709 -0.02473819\n",
      "  0.          0.01565835  0.01851654  0.          0.          0.\n",
      " -0.02503714  0.01859083 -0.01496226 -0.0184795  -0.02424786  0.\n",
      " -0.01490247  0.03520822  0.          0.          0.          0.\n",
      "  0.02488722  0.          0.         -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.          0.          0.         -0.03077264  0.\n",
      "  0.01507987 -0.02523844  0.          0.          0.         -0.01460709], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.         -0.04070292 -0.02444282  0.01851654  0.007792    0.\n",
      " -0.02523844 -0.02454088  0.          0.          0.          0.\n",
      "  0.02498707 -0.02473819  0.01851654  0.          0.          0.\n",
      "  0.01859083  0.01859083  0.          0.02539048  0.         -0.01490247\n",
      "  0.         -0.02513759  0.          0.0155085   0.02539048  0.01836885\n",
      "  0.          0.          0.04135634 -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.02498707  0.          0.02528902 -0.02503714  0.04062152\n",
      " -0.02544136  0.         -0.02473819  0.          0.         -0.02544136\n",
      "  0.         -0.01900478 -0.01460709  0.          0.02429645  0.02626056\n",
      " -0.01496226 -0.02444282  0.          0.         -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.02488722  0.          0.\n",
      " -0.03077264  0.01814952  0.01889097  0.          0.02498707  0.01836885\n",
      " -0.01228323  0.          0.02508732  0.          0.03592413  0.\n",
      "  0.          0.          0.00085822  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255  0.\n",
      " -0.02513759  0.         -0.02523844  0.02488722 -0.01460709 -0.02473819\n",
      "  0.          0.01565835  0.01851654  0.          0.          0.\n",
      " -0.02503714  0.01859083 -0.01496226 -0.0184795  -0.02424786  0.\n",
      " -0.01490247  0.03520822  0.          0.          0.          0.\n",
      "  0.02488722  0.          0.         -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.          0.          0.         -0.03077264  0.\n",
      "  0.01507987 -0.02523844  0.          0.          0.         -0.01460709], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.         -0.04070292 -0.02444282  0.01851654  0.007792    0.\n",
      " -0.02523844 -0.02454088  0.          0.          0.          0.\n",
      "  0.02498707 -0.02473819  0.01851654  0.          0.          0.\n",
      "  0.01859083  0.01859083  0.          0.02539048  0.         -0.01490247\n",
      "  0.         -0.02513759  0.          0.0155085   0.02539048  0.01836885\n",
      "  0.          0.          0.04135634 -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.02498707  0.          0.02528902 -0.02503714  0.04062152\n",
      " -0.02544136  0.         -0.02473819  0.          0.         -0.02544136\n",
      "  0.         -0.01900478 -0.01460709  0.          0.02429645  0.02626056\n",
      " -0.01496226 -0.02444282  0.          0.         -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.02488722  0.          0.\n",
      " -0.03077264  0.01814952  0.01889097  0.          0.02498707  0.01836885\n",
      " -0.01228323  0.          0.02508732  0.          0.03592413  0.\n",
      "  0.          0.          0.00085822  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255  0.\n",
      " -0.02513759  0.         -0.02523844  0.02488722 -0.01460709 -0.02473819\n",
      "  0.          0.01565835  0.01851654  0.          0.          0.\n",
      " -0.02503714  0.01859083 -0.01496226 -0.0184795  -0.02424786  0.\n",
      " -0.01490247  0.03520822  0.          0.          0.          0.\n",
      "  0.02488722  0.          0.         -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.          0.          0.         -0.03077264  0.\n",
      "  0.01507987 -0.02523844  0.          0.          0.         -0.01460709], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.         -0.04070292 -0.02444282  0.01851654  0.007792    0.\n",
      " -0.02523844 -0.02454088  0.          0.          0.          0.\n",
      "  0.02498707 -0.02473819  0.01851654  0.          0.          0.\n",
      "  0.01859083  0.01859083  0.          0.02539048  0.         -0.01490247\n",
      "  0.         -0.02513759  0.          0.0155085   0.02539048  0.01836885\n",
      "  0.          0.          0.04135634 -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.02498707  0.          0.02528902 -0.02503714  0.04062152\n",
      " -0.02544136  0.         -0.02473819  0.          0.         -0.02544136\n",
      "  0.         -0.01900478 -0.01460709  0.          0.02429645  0.02626056\n",
      " -0.01496226 -0.02444282  0.          0.         -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.02488722  0.          0.\n",
      " -0.03077264  0.01814952  0.01889097  0.          0.02498707  0.01836885\n",
      " -0.01228323  0.          0.02508732  0.          0.03592413  0.\n",
      "  0.          0.          0.00085822  0.          0.02528902  0.01814952\n",
      "  0.01844255  0.          0.03508648  0.01889097  0.01844255  0.\n",
      " -0.02513759  0.         -0.02523844  0.02488722 -0.01460709 -0.02473819\n",
      "  0.          0.01565835  0.01851654  0.          0.          0.\n",
      " -0.02503714  0.01859083 -0.01496226 -0.0184795  -0.02424786  0.\n",
      " -0.01490247  0.03520822  0.          0.          0.          0.\n",
      "  0.02488722  0.          0.         -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.          0.          0.         -0.03077264  0.\n",
      "  0.01507987 -0.02523844  0.          0.          0.         -0.01460709], Bias: 0.0\n",
      "Iteration 0: Weights: [ 0.         -0.00206448 -0.00123975  0.         -0.00062052  0.\n",
      " -0.00128011 -0.00124473  0.0007394   0.          0.00094294  0.00207282\n",
      "  0.         -0.00125474  0.          0.00094294  0.          0.00181481\n",
      "  0.          0.          0.00128267  0.          0.         -0.00075586\n",
      "  0.00124722 -0.00127499  0.00125223  0.          0.          0.\n",
      "  0.0007394   0.00094672  0.         -0.00093729 -0.00096393 -0.00075889\n",
      "  0.          0.          0.00092055  0.         -0.0012699   0.\n",
      " -0.0012904   0.00092055 -0.00125474  0.00060701  0.00095816 -0.0012904\n",
      "  0.00092055 -0.00096393 -0.00074088  0.00206025  0.          0.\n",
      " -0.00075889 -0.00123975  0.00207704  0.00095816 -0.00075889  0.\n",
      "  0.         -0.00102186 -0.00124473  0.          0.          0.0007394\n",
      " -0.00156081  0.          0.          0.00062677  0.          0.\n",
      " -0.00062301  0.0020236   0.          0.          0.          0.\n",
      "  0.00076347  0.         -0.00135604  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.00094672\n",
      " -0.00127499  0.         -0.00128011  0.         -0.00074088 -0.00125474\n",
      "  0.          0.0016141   0.          0.00126736  0.00095816  0.00076347\n",
      " -0.0012699   0.         -0.00075889 -0.00093729 -0.00122987  0.00138857\n",
      " -0.00075586  0.          0.          0.00052956  0.00076347  0.\n",
      "  0.          0.00124722  0.00153591 -0.00096075  0.         -0.0012904\n",
      " -0.00205217  0.00128782  0.00208524  0.00094294 -0.00156081  0.\n",
      "  0.         -0.00128011  0.00125223  0.00092796  0.00094672 -0.00074088], Bias: 0.0\n",
      "Iteration 100: Weights: [ 0.         -0.04049088 -0.02431548  0.         -0.01217041  0.\n",
      " -0.02510696 -0.02441303  0.01450193  0.          0.01849398  0.04065455\n",
      "  0.         -0.02460932  0.          0.01849398  0.          0.03559417\n",
      "  0.          0.          0.02515728  0.          0.         -0.01482483\n",
      "  0.02446196 -0.02500663  0.0245601   0.          0.          0.\n",
      "  0.01450193  0.01856818  0.         -0.01838323 -0.01890577 -0.01488431\n",
      "  0.          0.          0.01805497  0.         -0.02490671  0.\n",
      " -0.02530883  0.01805497 -0.02460932  0.01190532  0.01879256 -0.02530883\n",
      "  0.01805497 -0.01890577 -0.01453099  0.04040795  0.          0.\n",
      " -0.01488431 -0.02431548  0.04073741  0.01879256 -0.01488431  0.\n",
      "  0.         -0.02004183 -0.02441303  0.          0.          0.01450193\n",
      " -0.03061232  0.          0.          0.01229285  0.          0.\n",
      " -0.01221924  0.03968922  0.          0.          0.          0.\n",
      "  0.01497397  0.         -0.02659626  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01856818\n",
      " -0.02500663  0.         -0.02510696  0.         -0.01453099 -0.02460932\n",
      "  0.          0.03165755  0.          0.02485689  0.01879256  0.01497397\n",
      " -0.02490671  0.         -0.01488431 -0.01838323 -0.02412154  0.02723421\n",
      " -0.01482483  0.          0.          0.01038641  0.01497397  0.\n",
      "  0.          0.02446196  0.03012395 -0.01884337  0.         -0.02530883\n",
      " -0.04024963  0.02525821  0.04089823  0.01849398 -0.03061232  0.\n",
      "  0.         -0.02510696  0.0245601   0.01820014  0.01856818 -0.01453099], Bias: 0.0\n",
      "Iteration 200: Weights: [ 0.         -0.04070176 -0.02444212  0.         -0.0122338   0.\n",
      " -0.02523772 -0.02454018  0.01457745  0.          0.0185903   0.04086629\n",
      "  0.         -0.02473749  0.          0.0185903   0.          0.03577955\n",
      "  0.          0.          0.0252883   0.          0.         -0.01490204\n",
      "  0.02458936 -0.02513687  0.02468801  0.          0.          0.\n",
      "  0.01457745  0.01866488  0.         -0.01847898 -0.01900423 -0.01496183\n",
      "  0.          0.          0.018149    0.         -0.02503643  0.\n",
      " -0.02544064  0.018149   -0.02473749  0.01196733  0.01889043 -0.02544064\n",
      "  0.018149   -0.01900423 -0.01460667  0.0406184   0.          0.\n",
      " -0.01496183 -0.02444212  0.04094958  0.01889043 -0.01496183  0.\n",
      "  0.         -0.02014621 -0.02454018  0.          0.          0.01457745\n",
      " -0.03077176  0.          0.          0.01235687  0.          0.\n",
      " -0.01228288  0.03989593  0.          0.          0.          0.\n",
      "  0.01505196  0.         -0.02673478  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01866488\n",
      " -0.02513687  0.         -0.02523772  0.         -0.01460667 -0.02473749\n",
      "  0.          0.03182243  0.          0.02498635  0.01889043  0.01505196\n",
      " -0.02503643  0.         -0.01496183 -0.01847898 -0.02424717  0.02737605\n",
      " -0.01490204  0.          0.          0.0104405   0.01505196  0.\n",
      "  0.          0.02458936  0.03028084 -0.01894151  0.         -0.02544064\n",
      " -0.04045925  0.02538976  0.04111123  0.0185903  -0.03077176  0.\n",
      "  0.         -0.02523772  0.02468801  0.01829492  0.01866488 -0.01460667], Bias: 0.0\n",
      "Iteration 300: Weights: [ 0.         -0.04070292 -0.02444281  0.         -0.01223414  0.\n",
      " -0.02523844 -0.02454088  0.01457787  0.          0.01859082  0.04086745\n",
      "  0.         -0.02473819  0.          0.01859082  0.          0.03578057\n",
      "  0.          0.          0.02528902  0.          0.         -0.01490246\n",
      "  0.02459006 -0.02513759  0.02468871  0.          0.          0.\n",
      "  0.01457787  0.01866541  0.         -0.0184795  -0.01900477 -0.01496225\n",
      "  0.          0.          0.01814952  0.         -0.02503714  0.\n",
      " -0.02544136  0.01814952 -0.02473819  0.01196767  0.01889097 -0.02544136\n",
      "  0.01814952 -0.01900477 -0.01460708  0.04061956  0.          0.\n",
      " -0.01496225 -0.02444281  0.04095074  0.01889097 -0.01496225  0.\n",
      "  0.         -0.02014678 -0.02454088  0.          0.          0.01457787\n",
      " -0.03077263  0.          0.          0.01235722  0.          0.\n",
      " -0.01228323  0.03989706  0.          0.          0.          0.\n",
      "  0.01505239  0.         -0.02673554  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01866541\n",
      " -0.02513759  0.         -0.02523844  0.         -0.01460708 -0.02473819\n",
      "  0.          0.03182333  0.          0.02498706  0.01889097  0.01505239\n",
      " -0.02503714  0.         -0.01496225 -0.0184795  -0.02424785  0.02737683\n",
      " -0.01490246  0.          0.          0.0104408   0.01505239  0.\n",
      "  0.          0.02459006  0.0302817  -0.01894205  0.         -0.02544136\n",
      " -0.0404604   0.02539048  0.0411124   0.01859082 -0.03077263  0.\n",
      "  0.         -0.02523844  0.02468871  0.01829544  0.01866541 -0.01460708], Bias: 0.0\n",
      "Iteration 400: Weights: [ 0.         -0.04070292 -0.02444282  0.         -0.01223415  0.\n",
      " -0.02523844 -0.02454088  0.01457787  0.          0.01859083  0.04086745\n",
      "  0.         -0.02473819  0.          0.01859083  0.          0.03578057\n",
      "  0.          0.          0.02528902  0.          0.         -0.01490247\n",
      "  0.02459006 -0.02513759  0.02468872  0.          0.          0.\n",
      "  0.01457787  0.01866541  0.         -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.          0.01814952  0.         -0.02503714  0.\n",
      " -0.02544136  0.01814952 -0.02473819  0.01196767  0.01889097 -0.02544136\n",
      "  0.01814952 -0.01900478 -0.01460709  0.04061956  0.          0.\n",
      " -0.01496226 -0.02444282  0.04095075  0.01889097 -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.          0.          0.01457787\n",
      " -0.03077264  0.          0.          0.01235723  0.          0.\n",
      " -0.01228323  0.03989707  0.          0.          0.          0.\n",
      "  0.01505239  0.         -0.02673554  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01866541\n",
      " -0.02513759  0.         -0.02523844  0.         -0.01460709 -0.02473819\n",
      "  0.          0.03182334  0.          0.02498707  0.01889097  0.01505239\n",
      " -0.02503714  0.         -0.01496226 -0.0184795  -0.02424786  0.02737683\n",
      " -0.01490247  0.          0.          0.0104408   0.01505239  0.\n",
      "  0.          0.02459006  0.0302817  -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.02539048  0.04111241  0.01859083 -0.03077264  0.\n",
      "  0.         -0.02523844  0.02468872  0.01829545  0.01866541 -0.01460709], Bias: 0.0\n",
      "Iteration 500: Weights: [ 0.         -0.04070292 -0.02444282  0.         -0.01223415  0.\n",
      " -0.02523844 -0.02454088  0.01457787  0.          0.01859083  0.04086745\n",
      "  0.         -0.02473819  0.          0.01859083  0.          0.03578057\n",
      "  0.          0.          0.02528902  0.          0.         -0.01490247\n",
      "  0.02459006 -0.02513759  0.02468872  0.          0.          0.\n",
      "  0.01457787  0.01866541  0.         -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.          0.01814952  0.         -0.02503714  0.\n",
      " -0.02544136  0.01814952 -0.02473819  0.01196767  0.01889097 -0.02544136\n",
      "  0.01814952 -0.01900478 -0.01460709  0.04061956  0.          0.\n",
      " -0.01496226 -0.02444282  0.04095075  0.01889097 -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.          0.          0.01457787\n",
      " -0.03077264  0.          0.          0.01235723  0.          0.\n",
      " -0.01228323  0.03989707  0.          0.          0.          0.\n",
      "  0.01505239  0.         -0.02673554  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01866541\n",
      " -0.02513759  0.         -0.02523844  0.         -0.01460709 -0.02473819\n",
      "  0.          0.03182334  0.          0.02498707  0.01889097  0.01505239\n",
      " -0.02503714  0.         -0.01496226 -0.0184795  -0.02424786  0.02737683\n",
      " -0.01490247  0.          0.          0.0104408   0.01505239  0.\n",
      "  0.          0.02459006  0.0302817  -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.02539048  0.04111241  0.01859083 -0.03077264  0.\n",
      "  0.         -0.02523844  0.02468872  0.01829545  0.01866541 -0.01460709], Bias: 0.0\n",
      "Iteration 600: Weights: [ 0.         -0.04070292 -0.02444282  0.         -0.01223415  0.\n",
      " -0.02523844 -0.02454088  0.01457787  0.          0.01859083  0.04086745\n",
      "  0.         -0.02473819  0.          0.01859083  0.          0.03578057\n",
      "  0.          0.          0.02528902  0.          0.         -0.01490247\n",
      "  0.02459006 -0.02513759  0.02468872  0.          0.          0.\n",
      "  0.01457787  0.01866541  0.         -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.          0.01814952  0.         -0.02503714  0.\n",
      " -0.02544136  0.01814952 -0.02473819  0.01196767  0.01889097 -0.02544136\n",
      "  0.01814952 -0.01900478 -0.01460709  0.04061956  0.          0.\n",
      " -0.01496226 -0.02444282  0.04095075  0.01889097 -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.          0.          0.01457787\n",
      " -0.03077264  0.          0.          0.01235723  0.          0.\n",
      " -0.01228323  0.03989707  0.          0.          0.          0.\n",
      "  0.01505239  0.         -0.02673554  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01866541\n",
      " -0.02513759  0.         -0.02523844  0.         -0.01460709 -0.02473819\n",
      "  0.          0.03182334  0.          0.02498707  0.01889097  0.01505239\n",
      " -0.02503714  0.         -0.01496226 -0.0184795  -0.02424786  0.02737683\n",
      " -0.01490247  0.          0.          0.0104408   0.01505239  0.\n",
      "  0.          0.02459006  0.0302817  -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.02539048  0.04111241  0.01859083 -0.03077264  0.\n",
      "  0.         -0.02523844  0.02468872  0.01829545  0.01866541 -0.01460709], Bias: 0.0\n",
      "Iteration 700: Weights: [ 0.         -0.04070292 -0.02444282  0.         -0.01223415  0.\n",
      " -0.02523844 -0.02454088  0.01457787  0.          0.01859083  0.04086745\n",
      "  0.         -0.02473819  0.          0.01859083  0.          0.03578057\n",
      "  0.          0.          0.02528902  0.          0.         -0.01490247\n",
      "  0.02459006 -0.02513759  0.02468872  0.          0.          0.\n",
      "  0.01457787  0.01866541  0.         -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.          0.01814952  0.         -0.02503714  0.\n",
      " -0.02544136  0.01814952 -0.02473819  0.01196767  0.01889097 -0.02544136\n",
      "  0.01814952 -0.01900478 -0.01460709  0.04061956  0.          0.\n",
      " -0.01496226 -0.02444282  0.04095075  0.01889097 -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.          0.          0.01457787\n",
      " -0.03077264  0.          0.          0.01235723  0.          0.\n",
      " -0.01228323  0.03989707  0.          0.          0.          0.\n",
      "  0.01505239  0.         -0.02673554  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01866541\n",
      " -0.02513759  0.         -0.02523844  0.         -0.01460709 -0.02473819\n",
      "  0.          0.03182334  0.          0.02498707  0.01889097  0.01505239\n",
      " -0.02503714  0.         -0.01496226 -0.0184795  -0.02424786  0.02737683\n",
      " -0.01490247  0.          0.          0.0104408   0.01505239  0.\n",
      "  0.          0.02459006  0.0302817  -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.02539048  0.04111241  0.01859083 -0.03077264  0.\n",
      "  0.         -0.02523844  0.02468872  0.01829545  0.01866541 -0.01460709], Bias: 0.0\n",
      "Iteration 800: Weights: [ 0.         -0.04070292 -0.02444282  0.         -0.01223415  0.\n",
      " -0.02523844 -0.02454088  0.01457787  0.          0.01859083  0.04086745\n",
      "  0.         -0.02473819  0.          0.01859083  0.          0.03578057\n",
      "  0.          0.          0.02528902  0.          0.         -0.01490247\n",
      "  0.02459006 -0.02513759  0.02468872  0.          0.          0.\n",
      "  0.01457787  0.01866541  0.         -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.          0.01814952  0.         -0.02503714  0.\n",
      " -0.02544136  0.01814952 -0.02473819  0.01196767  0.01889097 -0.02544136\n",
      "  0.01814952 -0.01900478 -0.01460709  0.04061956  0.          0.\n",
      " -0.01496226 -0.02444282  0.04095075  0.01889097 -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.          0.          0.01457787\n",
      " -0.03077264  0.          0.          0.01235723  0.          0.\n",
      " -0.01228323  0.03989707  0.          0.          0.          0.\n",
      "  0.01505239  0.         -0.02673554  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01866541\n",
      " -0.02513759  0.         -0.02523844  0.         -0.01460709 -0.02473819\n",
      "  0.          0.03182334  0.          0.02498707  0.01889097  0.01505239\n",
      " -0.02503714  0.         -0.01496226 -0.0184795  -0.02424786  0.02737683\n",
      " -0.01490247  0.          0.          0.0104408   0.01505239  0.\n",
      "  0.          0.02459006  0.0302817  -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.02539048  0.04111241  0.01859083 -0.03077264  0.\n",
      "  0.         -0.02523844  0.02468872  0.01829545  0.01866541 -0.01460709], Bias: 0.0\n",
      "Iteration 900: Weights: [ 0.         -0.04070292 -0.02444282  0.         -0.01223415  0.\n",
      " -0.02523844 -0.02454088  0.01457787  0.          0.01859083  0.04086745\n",
      "  0.         -0.02473819  0.          0.01859083  0.          0.03578057\n",
      "  0.          0.          0.02528902  0.          0.         -0.01490247\n",
      "  0.02459006 -0.02513759  0.02468872  0.          0.          0.\n",
      "  0.01457787  0.01866541  0.         -0.0184795  -0.01900478 -0.01496226\n",
      "  0.          0.          0.01814952  0.         -0.02503714  0.\n",
      " -0.02544136  0.01814952 -0.02473819  0.01196767  0.01889097 -0.02544136\n",
      "  0.01814952 -0.01900478 -0.01460709  0.04061956  0.          0.\n",
      " -0.01496226 -0.02444282  0.04095075  0.01889097 -0.01496226  0.\n",
      "  0.         -0.02014678 -0.02454088  0.          0.          0.01457787\n",
      " -0.03077264  0.          0.          0.01235723  0.          0.\n",
      " -0.01228323  0.03989707  0.          0.          0.          0.\n",
      "  0.01505239  0.         -0.02673554  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01866541\n",
      " -0.02513759  0.         -0.02523844  0.         -0.01460709 -0.02473819\n",
      "  0.          0.03182334  0.          0.02498707  0.01889097  0.01505239\n",
      " -0.02503714  0.         -0.01496226 -0.0184795  -0.02424786  0.02737683\n",
      " -0.01490247  0.          0.          0.0104408   0.01505239  0.\n",
      "  0.          0.02459006  0.0302817  -0.01894205  0.         -0.02544136\n",
      " -0.04046041  0.02539048  0.04111241  0.01859083 -0.03077264  0.\n",
      "  0.         -0.02523844  0.02468872  0.01829545  0.01866541 -0.01460709], Bias: 0.0\n",
      "Classifier (1, 2) predictions: [ 1. -1.  1.  1. -1.  1.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.\n",
      "  1. -1.  1.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.\n",
      "  1. -1.  0.  0. -1.  1.  0. -1.  1.  1. -1.  0.]\n",
      "Classifier (1, 3) predictions: [ 1.  0. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.\n",
      "  1.  0. -1.  1.  0. -1.  1.  1. -1.  1.  0. -1.  1.  0. -1.  1.  0. -1.\n",
      "  1.  0. -1.  0.  1. -1.  0.  1.  1.  1.  0. -1.]\n",
      "Classifier (2, 3) predictions: [-1.  1. -1.  0.  1. -1.  1.  1. -1.  0.  1. -1. -1.  1. -1.  0.  1. -1.\n",
      "  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1. -1.  1. -1.  0.  1. -1.\n",
      "  0.  1. -1.  0.  1. -1.  0.  1. -1.  0.  1. -1.]\n",
      "Votes: [[2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 1. 0.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [1. 2. 0.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]\n",
      " [0. 1. 2.]\n",
      " [1. 2. 0.]\n",
      " [1. 0. 2.]\n",
      " [0. 1. 2.]\n",
      " [1. 2. 0.]\n",
      " [2. 0. 1.]\n",
      " [2. 0. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 2.]]\n",
      "Final predictions: [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1\n",
      " 2 3 3 2 3 3 2 1 1 2 3]\n",
      "Predictions: [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1\n",
      " 2 3 3 2 3 3 2 1 1 2 3]\n",
      "Actual Labels: [1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1\n",
      " 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_list = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    # Remove numbers and special symbols\n",
    "    filtered_list = [w for w in filtered_list if w.isalnum() and not w.isdigit()]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_list = [lemmatizer.lemmatize(w, 'v') for w in filtered_list]\n",
    "    \n",
    "    return ' '.join(lemmatized_list)\n",
    "\n",
    "# Compute TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    vocabulary = list(set(word for document in corpus for word in document.split()))\n",
    "    vocabulary.sort()\n",
    "    tfidf_matrix = np.zeros((len(corpus), len(vocabulary)))\n",
    "\n",
    "    for i, document in enumerate(corpus):\n",
    "        word_counts = Counter(document.split())\n",
    "        for word, count in word_counts.items():\n",
    "            tf = count / len(document.split())\n",
    "            idf = log(len(corpus) / sum(1 for doc in corpus if word in doc.split()))\n",
    "            tfidf_matrix[i, vocabulary.index(word)] = tf * idf\n",
    "\n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Binary SVM Classifier\n",
    "class BinarySVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for iteration in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (2 * self.C * self.weights - np.dot(x_i, y[idx]))\n",
    "                    self.bias -= self.learning_rate * y[idx]\n",
    "\n",
    "            # Debugging: print weights and bias at every 100 iterations\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}: Weights: {self.weights}, Bias: {self.bias}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) - self.bias)\n",
    "\n",
    "# One-vs-One SVM Classifier\n",
    "class OneVsOneSVM:\n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        n_classes = len(unique_classes)\n",
    "        print(f\"Unique classes: {unique_classes}\")  # Debugging line\n",
    "        for i in range(n_classes):\n",
    "            for j in range(i + 1, n_classes):\n",
    "                class_i = unique_classes[i]\n",
    "                class_j = unique_classes[j]\n",
    "\n",
    "                # Filter the data for the two classes\n",
    "                idx = np.where((y == class_i) | (y == class_j))\n",
    "                X_filtered = X[idx]\n",
    "                y_filtered = y[idx]\n",
    "\n",
    "                # Convert class labels to +1 and -1\n",
    "                y_filtered = np.where(y_filtered == class_i, 1, -1)\n",
    "\n",
    "                # Train the binary classifier\n",
    "                clf = BinarySVM(C=self.C, learning_rate=self.learning_rate, n_iters=self.n_iters)\n",
    "                clf.fit(X_filtered, y_filtered)\n",
    "                self.classifiers.append((clf, (class_i, class_j)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = len(np.unique(y))\n",
    "        votes = np.zeros((X.shape[0], n_classes))\n",
    "\n",
    "        for clf, class_labels in self.classifiers:\n",
    "            predictions = clf.predict(X)\n",
    "            print(f\"Classifier {class_labels} predictions: {predictions}\")  # Debugging line\n",
    "            for idx, pred in enumerate(predictions):\n",
    "                if pred == 1:\n",
    "                    votes[idx, class_labels[0] - 1] += 1  # Adjust index for zero-based array\n",
    "                else:\n",
    "                    votes[idx, class_labels[1] - 1] += 1  # Adjust index for zero-based array\n",
    "\n",
    "        # Return the class with the most votes\n",
    "        final_predictions = np.argmax(votes, axis=1) + 1  # Adjust index to match class labels starting from 1\n",
    "        print(f\"Votes: {votes}\")  # Debugging line\n",
    "        print(f\"Final predictions: {final_predictions}\")  # Debugging line\n",
    "        return final_predictions\n",
    "\n",
    "# Sample dataset\n",
    "news_articles = [\n",
    "    \"government passes new law\",                     # Politics\n",
    "    \"football match ends in draw\",                    # Sports\n",
    "    \"new technology in smartphones\",                  # Technology\n",
    "    \"politician gives a speech\",                      # Politics\n",
    "    \"sports event attracts large crowd\",              # Sports\n",
    "    \"new tech gadgets released this year\",            # Technology\n",
    "    \"election results announced\",                     # Politics\n",
    "    \"soccer team wins championship\",                  # Sports\n",
    "    \"AI advancements in healthcare\",                  # Technology\n",
    "    \"local council meeting updates\",                  # Politics\n",
    "    \"basketball game highlights\",                     # Sports\n",
    "    \"innovations in artificial intelligence\",         # Technology\n",
    "    \"politician's new policy proposal\",               # Politics\n",
    "    \"swimming competition results\",                   # Sports\n",
    "    \"latest trends in smartphone design\",             # Technology\n",
    "    \"government budget allocation review\",            # Politics\n",
    "    \"volleyball tournament concludes\",                # Sports\n",
    "    \"breakthrough in renewable energy\",               # Technology\n",
    "    \"senator's speech on climate change\",             # Politics\n",
    "    \"baseball team training camp\",                    # Sports\n",
    "    \"tech company announces new software\",            # Technology\n",
    "    \"international relations summit\",                 # Politics\n",
    "    \"world cup qualifying matches\",                   # Sports\n",
    "    \"smart home devices market growth\",               # Technology\n",
    "    \"legislative bill discussion\",                    # Politics\n",
    "    \"rugby game results\",                            # Sports\n",
    "    \"technology in education sector\",                # Technology\n",
    "    \"mayoral election debate\",                        # Politics\n",
    "    \"national soccer league season start\",            # Sports\n",
    "    \"advancements in quantum computing\",              # Technology\n",
    "    \"press conference on new laws\",                   # Politics\n",
    "    \"hockey match final scores\",                      # Sports\n",
    "    \"virtual reality applications\",                   # Technology\n",
    "    \"parliamentary debate on economy\",                # Politics\n",
    "    \"college basketball championship\",                # Sports\n",
    "    \"latest trends in gadget development\",            # Technology\n",
    "    \"congressional committee meeting\",                # Politics\n",
    "    \"tennis tournament highlights\",                   # Sports\n",
    "    \"emerging technologies in fintech\",               # Technology\n",
    "    \"state of the union address\",                     # Politics\n",
    "    \"motorsports event results\",                      # Sports\n",
    "    \"new innovations in medical tech\",                # Technology\n",
    "    \"political rally speeches\",                       # Politics\n",
    "    \"community sports league updates\",                # Sports\n",
    "    \"tech industry conference news\",                  # Technology\n",
    "    \"government response to natural disaster\",        # Politics\n",
    "    \"annual sports awards ceremony\",                  # Sports\n",
    "    \"technological impacts on job market\"             # Technology\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3,  # Technology\n",
    "    1,  # Politics\n",
    "    2,  # Sports\n",
    "    3   # Technology\n",
    "]\n",
    "\n",
    "# Preprocess and convert to TF-IDF\n",
    "preprocessed_articles = [preprocess_text(article) for article in news_articles]\n",
    "X, vocabulary = compute_tfidf(preprocessed_articles)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize and train the One-vs-One SVM\n",
    "ovo_svm = OneVsOneSVM(C=1.0, learning_rate=0.001, n_iters=1000)\n",
    "ovo_svm.fit(X, y)\n",
    "\n",
    "# Test the classifier on the training data\n",
    "predictions = ovo_svm.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual Labels:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
