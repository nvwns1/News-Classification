{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    # lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove Stop Words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_list = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    # Remove numbers and special Symbols\n",
    "    # words like 100m 2m were not removed so using this\n",
    "    num = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "    num_filter = []\n",
    "    for i in range(0, len(filtered_list)):\n",
    "        for j in range(0, len(num)):\n",
    "            if num[j] in filtered_list[i]:\n",
    "                num_filter.append(filtered_list[i])\n",
    "                break\n",
    "\n",
    "    for filter in num_filter:\n",
    "        filtered_list.remove(filter)\n",
    "\n",
    "    filtered_list = [w for w in filtered_list if w.isalnum()]\n",
    "    filtered_list = [w for w in filtered_list if not w.isdigit()]\n",
    "\n",
    "    # Lematizing\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_list = [\n",
    "        wordnet_lemmatizer.lemmatize(w, wordnet.VERB) for w in filtered_list\n",
    "    ]\n",
    "    lemmatized_string = \" \".join(lemmatized_list)\n",
    "\n",
    "    return lemmatized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ArticleId                                               Text  Category  \\\n",
      "0       1833  worldcom launch defence lawyers defend former ...  business   \n",
      "1        154  german business confidence slide german busine...  business   \n",
      "2       1101  bbc poll indicate economic gloom citizens majo...  business   \n",
      "3       1976  lifestyle govern mobile choice faster better f...      tech   \n",
      "4        917  enron boss payout eighteen former enron direct...  business   \n",
      "\n",
      "   CategoryId  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           4  \n",
      "4           0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the BBC News dataset\n",
    "df = pd.read_csv(\"../dataset/BBC News Train.csv\")\n",
    "\n",
    "mapping = {\"business\": 0, \"entertainment\": 1, \"politics\": 2, \"sport\": 3, \"tech\": 4}\n",
    "df[\"CategoryId\"] = df[\"Category\"].map(mapping)\n",
    "df[\"Text\"] = df[\"Text\"].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "features = vectorizer.fit_transform(df[\"Text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = df.loc[:, \"CategoryId\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import MySVM\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class OneVsOneSVM:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : float, optional (default=1.0)\n",
    "    Penalty parameter C of the error term.\n",
    "\n",
    "    max_iter : int, optional (default=6000)\n",
    "\n",
    "    learning_rate : float, optional (default=0.00001)\n",
    "\n",
    "    Callable Functions\n",
    "    ------------------\n",
    "\n",
    "    fit(X,Y) : take data as X, target_labels as Y as input and trains the SVM model\n",
    "\n",
    "    score(X,Y) : take data as X, target_labels as Y as input and returns the accuracy of the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=1.0, max_iter=6000, learning_rate=0.00001):\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.C = C\n",
    "        self.svm_classifiers = {}\n",
    "\n",
    "    def generateClasswiseData(self, X, Y):\n",
    "        data = {}\n",
    "\n",
    "        no_of_classes = len(np.unique(Y))\n",
    "        no_of_samples = X.shape[0]\n",
    "\n",
    "        for i in range(no_of_classes):\n",
    "            data[i] = []\n",
    "\n",
    "        for i in range(no_of_samples):\n",
    "            data[Y[i]].append(X[i])\n",
    "\n",
    "        for k in range(no_of_classes):\n",
    "            data[k] = np.array(data[k])\n",
    "\n",
    "        return data\n",
    "\n",
    "    def getPairData(self, d1, d2):\n",
    "\n",
    "        l1 = d1.shape[0]\n",
    "        l2 = d2.shape[0]\n",
    "        data = np.zeros((l1 + l2, d1.shape[1]))\n",
    "        labels = np.zeros(l1 + l2)\n",
    "\n",
    "        data[:l1] = d1\n",
    "        data[l1:] = d2\n",
    "\n",
    "        labels[:l1] = 1\n",
    "        labels[l1:] = -1\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        global le\n",
    "        le = LabelEncoder()\n",
    "        le.fit(Y)\n",
    "        Y = le.transform(Y)\n",
    "\n",
    "        data = self.generateClasswiseData(X, Y)\n",
    "        svc = MySVM.SVC(self.C)\n",
    "        for i in range(len(data)):\n",
    "            self.svm_classifiers[i] = {}\n",
    "            for j in range(i + 1, len(np.unique(Y))):\n",
    "                x, y = self.getPairData(data[i], data[j])\n",
    "                wts, b, losses = svc.fit(\n",
    "                    x, y, learning_rate=self.learning_rate, max_itr=self.max_iter\n",
    "                )\n",
    "                self.svm_classifiers[i][j] = (wts, b)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        classes = len(self.svm_classifiers)\n",
    "        count = np.zeros(\n",
    "            classes,\n",
    "        )\n",
    "        for i in range(classes):\n",
    "            for j in range(i + 1, classes):\n",
    "                W = self.svm_classifiers[i][j][0]\n",
    "                b = self.svm_classifiers[i][j][1]\n",
    "                if (np.dot(W, X.T) + b) >= 0:\n",
    "                    count[i] += 1\n",
    "                else:\n",
    "                    count[j] += 1\n",
    "\n",
    "        index = np.argmax(count)\n",
    "        return le.inverse_transform([index])\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        count = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            if Y[i] == self.predict(X[i]):\n",
    "                count += 1\n",
    "\n",
    "        return count / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the OneVsOneSVM model\n",
    "ovo_SVM = OneVsOneSVM(C=10, max_iter=6000, learning_rate=0.00001)\n",
    "ovo_SVM.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business news predicted as: business\n",
      "Entertainment news predicted as: entertainment\n",
      "Politics news predicted as: politics\n",
      "Sports news predicted as: sports\n",
      "Tech news predicted as: tech\n"
     ]
    }
   ],
   "source": [
    "# Example custom news texts\n",
    "business_news = \"Global market trends indicate a significant rise in e-commerce investments. Major companies are expanding their online presence to capture growing digital consumer demand.\"\n",
    "entertainment_news = \"The latest blockbuster movie, directed by renowned filmmaker Jane Doe, has shattered box office records and received rave reviews from critics. Red carpet\"\n",
    "politics_news = \"In a recent speech, leader michael, down street, council tax, tory leader, election campaign, mr kennedy Leader Government the President outlined new policies and constitutions aimed at improving national healthcare and addressing income inequality in the country.\"\n",
    "sports_news = \"The local soccer team secured a dramatic victory in the championship final, with a last-minute goal clinching the title against their long-time rivals.\"\n",
    "tech_news = \"Tech giants Computer are unveiling their newest smart phones featuring advanced AI capabilities and enhanced security features that promise to revolutionize the industry.\"\n",
    "\n",
    "# Preprocess and vectorize the news texts\n",
    "business_vectorized = vectorizer.transform([preprocess_text(business_news)]).toarray()\n",
    "entertainment_vectorized = vectorizer.transform([preprocess_text(entertainment_news)]).toarray()\n",
    "politics_vectorized = vectorizer.transform([preprocess_text(politics_news)]).toarray()\n",
    "sports_vectorized = vectorizer.transform([preprocess_text(sports_news)]).toarray()\n",
    "tech_vectorized = vectorizer.transform([preprocess_text(tech_news)]).toarray()\n",
    "\n",
    "mapping = {0: \"business\", 1: \"entertainment\", 2: \"politics\", 3: \"sports\", 4: \"tech\"}\n",
    "\n",
    "# Predict the category\n",
    "predicted_business = ovo_SVM.predict(business_vectorized)[0]\n",
    "predicted_entertainment = ovo_SVM.predict(entertainment_vectorized)[0]\n",
    "predicted_politics = ovo_SVM.predict(politics_vectorized)[0]\n",
    "predicted_sports = ovo_SVM.predict(sports_vectorized)[0]\n",
    "predicted_tech = ovo_SVM.predict(tech_vectorized)[0]\n",
    "\n",
    "# Print the predicted categories\n",
    "print(f\"Business news predicted as: {mapping[predicted_business]}\")\n",
    "print(f\"Entertainment news predicted as: {mapping[predicted_entertainment]}\")\n",
    "print(f\"Politics news predicted as: {mapping[predicted_politics]}\")\n",
    "print(f\"Sports news predicted as: {mapping[predicted_sports]}\")\n",
    "print(f\"Tech news predicted as: {mapping[predicted_tech]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 99.46%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = ovo_SVM.score(X, y)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv(\"../dataset/BBC News Test.csv\")\n",
    "testing_data[\"Text\"] = testing_data[\"Text\"].apply(preprocess_text)\n",
    "X_test = vectorizer.transform(testing_data[\"Text\"]).toarray()\n",
    "y_test = testing_data.loc[:, \"Category\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ArticleId                                               Text\n",
      "0       1018  qpr keeper day heads for preston queens park r...\n",
      "1       1319  software watching while you work software that...\n",
      "2       1138  d arcy injury adds to ireland woe gordon d arc...\n",
      "3        459  india s reliance family feud heats up the ongo...\n",
      "4       1020  boro suffer morrison injury blow middlesbrough...\n"
     ]
    }
   ],
   "source": [
    "# Load the BBC News dataset\n",
    "df = pd.read_csv(\"../dataset/BBC News Test.csv\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to disk\n",
    "filename = \"OneVsOneSVM_model.pkl\"\n",
    "joblib.dump(ovo_SVM, filename)\n",
    "\n",
    "# save the vecorizer\n",
    "filename = \"vectorizer.pkl\"\n",
    "joblib.dump(vectorizer, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
